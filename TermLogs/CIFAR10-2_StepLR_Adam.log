PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\MultiModalClassifier\data\" --model_name 'squeezenetcustom' --learningratename 'StepLR' --optimizer 'Adam'      
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_squeezenetcustom_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
SqueezeNet (SqueezeNet)                  [128, 3, 224, 224]   [128, 490]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 96, 224, 224]  2,688                True
├─BatchNorm2d (bn1)                      [128, 96, 224, 224]  [128, 96, 224, 224]  192                  True
├─ReLU (relu)                            [128, 96, 224, 224]  [128, 96, 224, 224]  --                   --
├─MaxPool2d (maxpool1)                   [128, 96, 224, 224]  [128, 96, 112, 112]  --                   --
├─fire (fire2)                           [128, 96, 112, 112]  [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 96, 112, 112]  [128, 16, 112, 112]  1,552                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire3)                           [128, 128, 112, 112] [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 16, 112, 112]  2,064                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire4)                           [128, 128, 112, 112] [128, 256, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 32, 112, 112]  4,128                True
│    └─BatchNorm2d (bn1)                 [128, 32, 112, 112]  [128, 32, 112, 112]  64                   True
│    └─ReLU (relu1)                      [128, 32, 112, 112]  [128, 32, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 32, 112, 112]  [128, 128, 112, 112] 4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─Conv2d (conv3)                    [128, 32, 112, 112]  [128, 128, 112, 112] 36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─ReLU (relu2)                      [128, 256, 112, 112] [128, 256, 112, 112] --                   --
├─MaxPool2d (maxpool2)                   [128, 256, 112, 112] [128, 256, 56, 56]   --                   --
├─fire (fire5)                           [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 32, 56, 56]    8,224                True
│    └─BatchNorm2d (bn1)                 [128, 32, 56, 56]    [128, 32, 56, 56]    64                   True
│    └─ReLU (relu1)                      [128, 32, 56, 56]    [128, 32, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 32, 56, 56]    [128, 128, 56, 56]   4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─Conv2d (conv3)                    [128, 32, 56, 56]    [128, 128, 56, 56]   36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─ReLU (relu2)                      [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─fire (fire6)                           [128, 256, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 48, 56, 56]    12,336               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire7)                           [128, 384, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 48, 56, 56]    18,480               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire8)                           [128, 384, 56, 56]   [128, 512, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 64, 56, 56]    24,640               True
│    └─BatchNorm2d (bn1)                 [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    └─ReLU (relu1)                      [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 56, 56]    [128, 256, 56, 56]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 56, 56]    [128, 256, 56, 56]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─ReLU (relu2)                      [128, 512, 56, 56]   [128, 512, 56, 56]   --                   --
├─MaxPool2d (maxpool3)                   [128, 512, 56, 56]   [128, 512, 28, 28]   --                   --
├─fire (fire9)                           [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    └─Conv2d (conv1)                    [128, 512, 28, 28]   [128, 64, 28, 28]    32,832               True
│    └─BatchNorm2d (bn1)                 [128, 64, 28, 28]    [128, 64, 28, 28]    128                  True
│    └─ReLU (relu1)                      [128, 64, 28, 28]    [128, 64, 28, 28]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 28, 28]    [128, 256, 28, 28]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 28, 28]    [128, 256, 28, 28]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─ReLU (relu2)                      [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Conv2d (conv2)                         [128, 512, 28, 28]   [128, 10, 28, 28]    5,130                True
├─AvgPool2d (avg_pool)                   [128, 10, 28, 28]    [128, 10, 7, 7]      --                   --
========================================================================================================================
Total params: 734,986
Trainable params: 734,986
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 331.85
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 36693.51
Params size (MB): 2.94
Estimated Total Size (MB): 36773.52
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.658 ( 0.658)    Data  0.093 ( 0.093)    Loss 2.3319e+00 (2.3319e+00)    Acc@1  10.16 ( 10.16)   Acc@5  46.88 ( 46.88)
STAGE:2024-03-25 23:15:16 22012:37352 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 23:15:17 22012:37352 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 23:15:17 22012:37352 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-25 23:15:21 22012:37352 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 23:15:21 22012:37352 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 23:15:21 22012:37352 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.047 ( 0.135)    Data  0.027 ( 0.104)    Loss 1.5284e+00 (1.7222e+00)    Acc@1  40.62 ( 34.85)   Acc@5  87.50 ( 85.70)
Epoch: [0][201/313]     Time  0.048 ( 0.097)    Data  0.026 ( 0.069)    Loss 1.3943e+00 (1.5482e+00)    Acc@1  48.44 ( 41.96)   Acc@5  94.53 ( 89.63)
Epoch: [0][301/313]     Time  0.043 ( 0.081)    Data  0.023 ( 0.056)    Loss 1.1938e+00 (1.4508e+00)    Acc@1  56.25 ( 45.98)   Acc@5  94.53 ( 91.26)
train Loss: 1.4431 Acc: 0.4623
Epoch: [0][  1/313]     Time  0.052 ( 0.081)    Data  0.043 ( 0.055)    Loss 1.3290e+00 (1.4428e+00)    Acc@1  51.56 ( 46.24)   Acc@5  94.53 ( 91.37)
val Loss: 1.3218 Acc: 0.5262

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.118 ( 0.118)    Data  0.084 ( 0.084)    Loss 1.1505e+00 (1.1505e+00)    Acc@1  61.72 ( 61.72)   Acc@5  97.66 ( 97.66)
Epoch: [1][101/313]     Time  0.044 ( 0.061)    Data  0.025 ( 0.035)    Loss 1.0304e+00 (1.1141e+00)    Acc@1  62.50 ( 59.69)   Acc@5  97.66 ( 95.89)
Epoch: [1][201/313]     Time  0.045 ( 0.053)    Data  0.025 ( 0.030)    Loss 1.1618e+00 (1.0791e+00)    Acc@1  57.03 ( 60.98)   Acc@5  96.09 ( 96.19)
Epoch: [1][301/313]     Time  0.043 ( 0.050)    Data  0.024 ( 0.028)    Loss 1.0280e+00 (1.0451e+00)    Acc@1  60.16 ( 62.32)   Acc@5  96.88 ( 96.32)
train Loss: 1.0440 Acc: 0.6237
Epoch: [1][  1/313]     Time  0.041 ( 0.050)    Data  0.034 ( 0.028)    Loss 9.8590e-01 (1.0438e+00)    Acc@1  70.31 ( 62.40)   Acc@5  96.88 ( 96.34)
val Loss: 1.0660 Acc: 0.6239

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 8.7264e-01 (8.7264e-01)    Acc@1  63.28 ( 63.28)   Acc@5  95.31 ( 95.31)
Epoch: [2][101/313]     Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 6.8179e-01 (8.7228e-01)    Acc@1  75.00 ( 68.94)   Acc@5  99.22 ( 97.68)
Epoch: [2][201/313]     Time  0.045 ( 0.044)    Data  0.022 ( 0.024)    Loss 7.9539e-01 (8.6999e-01)    Acc@1  71.09 ( 69.12)   Acc@5  96.88 ( 97.61)
Epoch: [2][301/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 7.9018e-01 (8.6209e-01)    Acc@1  74.22 ( 69.33)   Acc@5  98.44 ( 97.63)
train Loss: 0.8597 Acc: 0.6938
Epoch: [2][  1/313]     Time  0.043 ( 0.044)    Data  0.037 ( 0.024)    Loss 1.2711e+00 (8.6103e-01)    Acc@1  64.06 ( 69.36)   Acc@5  96.09 ( 97.66)
val Loss: 1.0048 Acc: 0.6561

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 8.2873e-01 (8.2873e-01)    Acc@1  71.09 ( 71.09)   Acc@5  97.66 ( 97.66)
Epoch: [3][101/313]     Time  0.044 ( 0.045)    Data  0.024 ( 0.024)    Loss 8.0313e-01 (7.1595e-01)    Acc@1  71.09 ( 74.54)   Acc@5  96.88 ( 98.34)
Epoch: [3][201/313]     Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 8.6302e-01 (7.2953e-01)    Acc@1  70.31 ( 73.99)   Acc@5  98.44 ( 98.40)
Epoch: [3][301/313]     Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 7.1141e-01 (7.2444e-01)    Acc@1  75.78 ( 74.21)   Acc@5  97.66 ( 98.45)
train Loss: 0.7235 Acc: 0.7427
Epoch: [3][  1/313]     Time  0.043 ( 0.044)    Data  0.037 ( 0.024)    Loss 9.2933e-01 (7.2411e-01)    Acc@1  68.75 ( 74.25)   Acc@5  97.66 ( 98.44)
val Loss: 0.8439 Acc: 0.7036

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 6.5619e-01 (6.5619e-01)    Acc@1  75.78 ( 75.78)   Acc@5  99.22 ( 99.22)
Epoch: [4][101/313]     Time  0.044 ( 0.045)    Data  0.024 ( 0.024)    Loss 5.5601e-01 (5.9616e-01)    Acc@1  82.81 ( 79.03)   Acc@5  99.22 ( 98.91)
Epoch: [4][201/313]     Time  0.045 ( 0.044)    Data  0.026 ( 0.024)    Loss 5.9463e-01 (6.0860e-01)    Acc@1  76.56 ( 78.41)   Acc@5 100.00 ( 98.87)
Epoch: [4][301/313]     Time  0.048 ( 0.044)    Data  0.025 ( 0.024)    Loss 6.6472e-01 (6.1699e-01)    Acc@1  77.34 ( 78.08)   Acc@5  97.66 ( 98.79)
train Loss: 0.6163 Acc: 0.7812
Epoch: [4][  1/313]     Time  0.042 ( 0.044)    Data  0.035 ( 0.024)    Loss 8.6037e-01 (6.1710e-01)    Acc@1  70.31 ( 78.10)   Acc@5  99.22 ( 98.80)
val Loss: 0.9191 Acc: 0.7038

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 4.5358e-01 (4.5358e-01)    Acc@1  83.59 ( 83.59)   Acc@5 100.00 (100.00)
Epoch: [5][101/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 5.7501e-01 (4.9457e-01)    Acc@1  78.91 ( 82.64)   Acc@5 100.00 ( 99.32)
Epoch: [5][201/313]     Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 5.1183e-01 (5.2618e-01)    Acc@1  82.03 ( 81.44)   Acc@5 100.00 ( 99.23)
Epoch: [5][301/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.7992e-01 (5.3900e-01)    Acc@1  82.03 ( 80.98)   Acc@5 100.00 ( 99.19)
train Loss: 0.5413 Acc: 0.8091
Epoch: [5][  1/313]     Time  0.041 ( 0.044)    Data  0.035 ( 0.024)    Loss 9.3518e-01 (5.4260e-01)    Acc@1  65.62 ( 80.86)   Acc@5  98.44 ( 99.18)
val Loss: 0.8389 Acc: 0.7174

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.105 ( 0.105)    Data  0.072 ( 0.072)    Loss 5.3293e-01 (5.3293e-01)    Acc@1  80.47 ( 80.47)   Acc@5 100.00 (100.00)
Epoch: [6][101/313]     Time  0.043 ( 0.045)    Data  0.023 ( 0.024)    Loss 5.1675e-01 (4.4021e-01)    Acc@1  79.69 ( 84.59)   Acc@5  98.44 ( 99.50)
Epoch: [6][201/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 5.3662e-01 (4.4512e-01)    Acc@1  83.59 ( 84.35)   Acc@5  99.22 ( 99.48)
Epoch: [6][301/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.9048e-01 (4.5824e-01)    Acc@1  87.50 ( 83.90)   Acc@5  99.22 ( 99.45)
train Loss: 0.4585 Acc: 0.8391
Epoch: [6][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.2332e+00 (4.6093e-01)    Acc@1  60.94 ( 83.83)   Acc@5  96.88 ( 99.43)
val Loss: 0.9662 Acc: 0.6894

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 3.4532e-01 (3.4532e-01)    Acc@1  85.16 ( 85.16)   Acc@5 100.00 (100.00)
Epoch: [7][101/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 3.9968e-01 (3.4776e-01)    Acc@1  85.16 ( 87.93)   Acc@5  98.44 ( 99.70)
Epoch: [7][201/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.5194e-01 (3.7065e-01)    Acc@1  89.84 ( 87.15)   Acc@5 100.00 ( 99.64)
Epoch: [7][301/313]     Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.9148e-01 (3.8655e-01)    Acc@1  87.50 ( 86.54)   Acc@5  99.22 ( 99.58)
train Loss: 0.3873 Acc: 0.8649
Epoch: [7][  1/313]     Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 9.8848e-01 (3.8917e-01)    Acc@1  71.09 ( 86.45)   Acc@5  96.09 ( 99.56)
val Loss: 0.8479 Acc: 0.7331

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 3.9899e-01 (3.9899e-01)    Acc@1  85.16 ( 85.16)   Acc@5 100.00 (100.00)
Epoch: [8][101/313]     Time  0.041 ( 0.045)    Data  0.023 ( 0.025)    Loss 3.3754e-01 (2.9485e-01)    Acc@1  86.72 ( 89.48)   Acc@5 100.00 ( 99.80)
Epoch: [8][201/313]     Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 3.8848e-01 (3.1715e-01)    Acc@1  86.72 ( 88.60)   Acc@5  99.22 ( 99.78)
Epoch: [8][301/313]     Time  0.041 ( 0.045)    Data  0.023 ( 0.024)    Loss 3.4301e-01 (3.3678e-01)    Acc@1  87.50 ( 87.92)   Acc@5 100.00 ( 99.74)
train Loss: 0.3372 Acc: 0.8791
Epoch: [8][  1/313]     Time  0.042 ( 0.045)    Data  0.036 ( 0.024)    Loss 7.9422e-01 (3.3862e-01)    Acc@1  78.91 ( 87.88)   Acc@5  99.22 ( 99.74)
val Loss: 0.8246 Acc: 0.7457

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.111 ( 0.111)    Data  0.075 ( 0.075)    Loss 1.9083e-01 (1.9083e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [9][101/313]     Time  0.044 ( 0.047)    Data  0.025 ( 0.026)    Loss 1.9347e-01 (2.3824e-01)    Acc@1  93.75 ( 91.65)   Acc@5 100.00 ( 99.91)
Epoch: [9][201/313]     Time  0.047 ( 0.047)    Data  0.025 ( 0.026)    Loss 3.0204e-01 (2.5461e-01)    Acc@1  89.06 ( 90.91)   Acc@5 100.00 ( 99.89)
Epoch: [9][301/313]     Time  0.046 ( 0.046)    Data  0.025 ( 0.025)    Loss 3.7504e-01 (2.7538e-01)    Acc@1  86.72 ( 90.10)   Acc@5 100.00 ( 99.86)
train Loss: 0.2755 Acc: 0.9007
Epoch: [9][  1/313]     Time  0.044 ( 0.046)    Data  0.036 ( 0.025)    Loss 1.2580e+00 (2.7864e-01)    Acc@1  66.41 ( 89.99)   Acc@5  94.53 ( 99.84)
val Loss: 1.0441 Acc: 0.7134

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.131 ( 0.131)    Data  0.101 ( 0.101)    Loss 2.5744e-01 (2.5744e-01)    Acc@1  89.84 ( 89.84)   Acc@5 100.00 (100.00)
Epoch: [10][101/313]    Time  0.044 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.9526e-01 (2.0554e-01)    Acc@1  90.62 ( 92.74)   Acc@5 100.00 ( 99.92)
Epoch: [10][201/313]    Time  0.046 ( 0.045)    Data  0.024 ( 0.024)    Loss 2.0742e-01 (2.1882e-01)    Acc@1  92.97 ( 92.23)   Acc@5 100.00 ( 99.92)
Epoch: [10][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.0351e-01 (2.3291e-01)    Acc@1  92.97 ( 91.67)   Acc@5 100.00 ( 99.90)
train Loss: 0.2343 Acc: 0.9163
Epoch: [10][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 1.2015e+00 (2.3742e-01)    Acc@1  71.09 ( 91.56)   Acc@5  94.53 ( 99.89)
val Loss: 1.0623 Acc: 0.7221

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.114 ( 0.114)    Data  0.079 ( 0.079)    Loss 2.0210e-01 (2.0210e-01)    Acc@1  92.97 ( 92.97)   Acc@5 100.00 (100.00)
Epoch: [11][101/313]    Time  0.047 ( 0.048)    Data  0.024 ( 0.026)    Loss 1.2385e-01 (1.6709e-01)    Acc@1  96.09 ( 94.29)   Acc@5 100.00 ( 99.94)
Epoch: [11][201/313]    Time  0.059 ( 0.047)    Data  0.037 ( 0.026)    Loss 2.9666e-01 (1.7791e-01)    Acc@1  89.84 ( 93.71)   Acc@5 100.00 ( 99.96)
Epoch: [11][301/313]    Time  0.045 ( 0.046)    Data  0.025 ( 0.025)    Loss 2.1559e-01 (1.9470e-01)    Acc@1  89.06 ( 93.10)   Acc@5 100.00 ( 99.93)
train Loss: 0.1966 Acc: 0.9303
Epoch: [11][  1/313]    Time  0.044 ( 0.046)    Data  0.038 ( 0.025)    Loss 1.0855e+00 (1.9948e-01)    Acc@1  71.88 ( 92.96)   Acc@5  97.66 ( 99.92)
val Loss: 0.9362 Acc: 0.7530

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.108 ( 0.108)    Data  0.078 ( 0.078)    Loss 1.2973e-01 (1.2973e-01)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.044 ( 0.046)    Data  0.025 ( 0.026)    Loss 2.0372e-01 (1.4434e-01)    Acc@1  92.19 ( 94.93)   Acc@5 100.00 ( 99.98)
Epoch: [12][201/313]    Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.7379e-01 (1.5686e-01)    Acc@1  93.75 ( 94.41)   Acc@5 100.00 ( 99.97)
Epoch: [12][301/313]    Time  0.047 ( 0.045)    Data  0.026 ( 0.025)    Loss 2.3862e-01 (1.7286e-01)    Acc@1  91.41 ( 93.84)   Acc@5 100.00 ( 99.96)
train Loss: 0.1752 Acc: 0.9375
Epoch: [12][  1/313]    Time  0.074 ( 0.046)    Data  0.062 ( 0.025)    Loss 1.0960e+00 (1.7812e-01)    Acc@1  75.00 ( 93.69)   Acc@5  95.31 ( 99.95)
val Loss: 1.1034 Acc: 0.7261

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 2.1957e-01 (2.1957e-01)    Acc@1  91.41 ( 91.41)   Acc@5 100.00 (100.00)
Epoch: [13][101/313]    Time  0.046 ( 0.052)    Data  0.026 ( 0.028)    Loss 8.4600e-02 (1.3026e-01)    Acc@1  97.66 ( 95.51)   Acc@5 100.00 ( 99.99)
Epoch: [13][201/313]    Time  0.047 ( 0.049)    Data  0.025 ( 0.027)    Loss 2.1370e-01 (1.3727e-01)    Acc@1  91.41 ( 95.17)   Acc@5 100.00 ( 99.99)
Epoch: [13][301/313]    Time  0.045 ( 0.049)    Data  0.026 ( 0.027)    Loss 2.3019e-01 (1.4689e-01)    Acc@1  93.75 ( 94.80)   Acc@5 100.00 ( 99.99)
train Loss: 0.1486 Acc: 0.9472
Epoch: [13][  1/313]    Time  0.048 ( 0.049)    Data  0.040 ( 0.027)    Loss 6.4129e-01 (1.5022e-01)    Acc@1  81.25 ( 94.67)   Acc@5  99.22 ( 99.98)
val Loss: 0.9785 Acc: 0.7568

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.100 ( 0.100)    Data  0.072 ( 0.072)    Loss 1.2641e-01 (1.2641e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.045 ( 0.045)    Data  0.025 ( 0.024)    Loss 1.1616e-01 (1.2519e-01)    Acc@1  96.09 ( 95.59)   Acc@5 100.00 ( 99.99)
Epoch: [14][201/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.025)    Loss 9.6004e-02 (1.2620e-01)    Acc@1  96.09 ( 95.47)   Acc@5 100.00 (100.00)
Epoch: [14][301/313]    Time  0.044 ( 0.045)    Data  0.026 ( 0.025)    Loss 1.1060e-01 (1.3513e-01)    Acc@1  95.31 ( 95.16)   Acc@5 100.00 ( 99.99)
train Loss: 0.1349 Acc: 0.9517
Epoch: [14][  1/313]    Time  0.042 ( 0.045)    Data  0.036 ( 0.025)    Loss 1.2243e+00 (1.3841e-01)    Acc@1  71.88 ( 95.10)   Acc@5  96.88 ( 99.98)
val Loss: 1.0464 Acc: 0.7513

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.103 ( 0.103)    Data  0.071 ( 0.071)    Loss 2.0814e-01 (2.0814e-01)    Acc@1  92.97 ( 92.97)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.043 ( 0.047)    Data  0.024 ( 0.025)    Loss 7.7406e-02 (8.9888e-02)    Acc@1  97.66 ( 96.84)   Acc@5 100.00 (100.00)
Epoch: [15][201/313]    Time  0.045 ( 0.047)    Data  0.025 ( 0.026)    Loss 1.5911e-01 (9.4855e-02)    Acc@1  96.09 ( 96.67)   Acc@5 100.00 ( 99.99)
Epoch: [15][301/313]    Time  0.045 ( 0.048)    Data  0.024 ( 0.026)    Loss 1.2746e-01 (1.0753e-01)    Acc@1  96.09 ( 96.21)   Acc@5 100.00 ( 99.99)
train Loss: 0.1085 Acc: 0.9617
Epoch: [15][  1/313]    Time  0.045 ( 0.048)    Data  0.036 ( 0.026)    Loss 1.0779e+00 (1.1155e-01)    Acc@1  78.12 ( 96.12)   Acc@5  98.44 ( 99.99)
val Loss: 1.0705 Acc: 0.7561

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.108 ( 0.108)    Data  0.075 ( 0.075)    Loss 8.7437e-02 (8.7437e-02)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.045 ( 0.048)    Data  0.026 ( 0.027)    Loss 1.7082e-01 (9.6453e-02)    Acc@1  94.53 ( 96.57)   Acc@5 100.00 ( 99.99)
Epoch: [16][201/313]    Time  0.049 ( 0.053)    Data  0.026 ( 0.029)    Loss 9.7411e-02 (1.0489e-01)    Acc@1  96.09 ( 96.35)   Acc@5 100.00 ( 99.99)
Epoch: [16][301/313]    Time  0.056 ( 0.053)    Data  0.034 ( 0.030)    Loss 1.5514e-01 (1.1129e-01)    Acc@1  90.62 ( 96.06)   Acc@5 100.00 ( 99.99)
train Loss: 0.1121 Acc: 0.9603
Epoch: [16][  1/313]    Time  0.044 ( 0.053)    Data  0.037 ( 0.029)    Loss 1.1525e+00 (1.1543e-01)    Acc@1  75.00 ( 95.97)   Acc@5  97.66 ( 99.99)
val Loss: 1.1847 Acc: 0.7396

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.102 ( 0.102)    Data  0.070 ( 0.070)    Loss 6.5847e-02 (6.5847e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 2.6185e-02 (8.9777e-02)    Acc@1 100.00 ( 96.84)   Acc@5 100.00 (100.00)
Epoch: [17][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 8.5470e-02 (9.4603e-02)    Acc@1  95.31 ( 96.64)   Acc@5 100.00 (100.00)
Epoch: [17][301/313]    Time  0.044 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.0244e-01 (1.0011e-01)    Acc@1  95.31 ( 96.44)   Acc@5 100.00 ( 99.99)
train Loss: 0.1014 Acc: 0.9637
Epoch: [17][  1/313]    Time  0.043 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.1780e+00 (1.0482e-01)    Acc@1  67.19 ( 96.28)   Acc@5  99.22 ( 99.99)
val Loss: 1.2221 Acc: 0.7339

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.101 ( 0.101)    Data  0.069 ( 0.069)    Loss 7.4653e-02 (7.4653e-02)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 7.8921e-02 (8.6247e-02)    Acc@1  99.22 ( 96.77)   Acc@5 100.00 ( 99.98)
Epoch: [18][201/313]    Time  0.040 ( 0.044)    Data  0.022 ( 0.024)    Loss 8.6342e-02 (1.0223e-01)    Acc@1  96.88 ( 96.22)   Acc@5 100.00 ( 99.97)
Epoch: [18][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.0370e-01 (1.0887e-01)    Acc@1  97.66 ( 95.98)   Acc@5 100.00 ( 99.98)
train Loss: 0.1087 Acc: 0.9601
Epoch: [18][  1/313]    Time  0.045 ( 0.043)    Data  0.035 ( 0.023)    Loss 7.7264e-01 (1.1081e-01)    Acc@1  82.03 ( 95.97)   Acc@5  99.22 ( 99.98)
val Loss: 1.1110 Acc: 0.7563

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 1.2531e-01 (1.2531e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.046 ( 0.044)    Data  0.024 ( 0.024)    Loss 7.7300e-02 (8.1978e-02)    Acc@1  97.66 ( 97.06)   Acc@5 100.00 ( 99.99)
Epoch: [19][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 7.1110e-02 (8.2833e-02)    Acc@1  97.66 ( 97.08)   Acc@5 100.00 ( 99.99)
Epoch: [19][301/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.023)    Loss 7.8441e-02 (8.6235e-02)    Acc@1  98.44 ( 96.93)   Acc@5 100.00 ( 99.99)
train Loss: 0.0882 Acc: 0.9687
Epoch: [19][  1/313]    Time  0.041 ( 0.043)    Data  0.035 ( 0.023)    Loss 1.2156e+00 (9.1777e-02)    Acc@1  68.75 ( 96.78)   Acc@5  99.22 ( 99.99)
val Loss: 1.4890 Acc: 0.7154

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 6.4121e-02 (6.4121e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 8.5067e-02 (8.1315e-02)    Acc@1  97.66 ( 97.11)   Acc@5 100.00 ( 99.99)
Epoch: [20][201/313]    Time  0.040 ( 0.043)    Data  0.021 ( 0.024)    Loss 1.2778e-01 (8.4954e-02)    Acc@1  94.53 ( 96.97)   Acc@5 100.00 (100.00)
Epoch: [20][301/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 7.8609e-02 (8.5720e-02)    Acc@1  98.44 ( 96.99)   Acc@5 100.00 (100.00)
train Loss: 0.0871 Acc: 0.9696
Epoch: [20][  1/313]    Time  0.041 ( 0.043)    Data  0.035 ( 0.023)    Loss 1.1430e+00 (9.0441e-02)    Acc@1  76.56 ( 96.89)   Acc@5  98.44 ( 99.99)
val Loss: 1.2742 Acc: 0.7407

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 7.6103e-02 (7.6103e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.023)    Loss 7.1746e-02 (6.6845e-02)    Acc@1  96.88 ( 97.71)   Acc@5 100.00 (100.00)
Epoch: [21][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 6.6010e-02 (7.2396e-02)    Acc@1  97.66 ( 97.43)   Acc@5 100.00 (100.00)
Epoch: [21][301/313]    Time  0.043 ( 0.042)    Data  0.022 ( 0.023)    Loss 9.8550e-02 (8.0897e-02)    Acc@1  97.66 ( 97.16)   Acc@5 100.00 (100.00)
train Loss: 0.0822 Acc: 0.9713
Epoch: [21][  1/313]    Time  0.041 ( 0.042)    Data  0.034 ( 0.023)    Loss 1.5134e+00 (8.6788e-02)    Acc@1  71.09 ( 97.04)   Acc@5  96.88 ( 99.99)
val Loss: 1.3812 Acc: 0.7289

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 5.9736e-02 (5.9736e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.042 ( 0.043)    Data  0.022 ( 0.023)    Loss 1.0945e-01 (7.2240e-02)    Acc@1  96.88 ( 97.28)   Acc@5 100.00 (100.00)
Epoch: [22][201/313]    Time  0.045 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.9893e-02 (7.2188e-02)    Acc@1  99.22 ( 97.42)   Acc@5 100.00 (100.00)
Epoch: [22][301/313]    Time  0.046 ( 0.042)    Data  0.024 ( 0.023)    Loss 1.9571e-01 (7.2698e-02)    Acc@1  96.88 ( 97.40)   Acc@5 100.00 (100.00)
train Loss: 0.0736 Acc: 0.9737
Epoch: [22][  1/313]    Time  0.041 ( 0.042)    Data  0.034 ( 0.023)    Loss 1.5113e+00 (7.8165e-02)    Acc@1  74.22 ( 97.30)   Acc@5  97.66 ( 99.99)
val Loss: 1.3618 Acc: 0.7377

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.102 ( 0.102)    Data  0.070 ( 0.070)    Loss 4.6294e-02 (4.6294e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.7444e-02 (6.0969e-02)    Acc@1 100.00 ( 97.76)   Acc@5 100.00 ( 99.99)
Epoch: [23][201/313]    Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 7.1471e-02 (6.0312e-02)    Acc@1  96.88 ( 97.80)   Acc@5 100.00 (100.00)
Epoch: [23][301/313]    Time  0.041 ( 0.042)    Data  0.022 ( 0.023)    Loss 1.0111e-01 (7.0398e-02)    Acc@1  97.66 ( 97.47)   Acc@5 100.00 (100.00)
train Loss: 0.0707 Acc: 0.9747
Epoch: [23][  1/313]    Time  0.041 ( 0.042)    Data  0.034 ( 0.023)    Loss 1.0840e+00 (7.3974e-02)    Acc@1  70.31 ( 97.39)   Acc@5 100.00 (100.00)
val Loss: 1.3623 Acc: 0.7416

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.114 ( 0.114)    Data  0.080 ( 0.080)    Loss 1.4685e-01 (1.4685e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 5.2436e-02 (8.1617e-02)    Acc@1  96.88 ( 97.15)   Acc@5 100.00 (100.00)
Epoch: [24][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 7.9823e-02 (7.3976e-02)    Acc@1  96.88 ( 97.36)   Acc@5 100.00 (100.00)
Epoch: [24][301/313]    Time  0.045 ( 0.042)    Data  0.022 ( 0.023)    Loss 4.7119e-02 (7.4063e-02)    Acc@1  97.66 ( 97.34)   Acc@5 100.00 (100.00)
train Loss: 0.0739 Acc: 0.9735
Epoch: [24][  1/313]    Time  0.042 ( 0.042)    Data  0.036 ( 0.023)    Loss 1.3115e+00 (7.7811e-02)    Acc@1  78.91 ( 97.29)   Acc@5  97.66 ( 99.99)
val Loss: 1.2253 Acc: 0.7611

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.100 ( 0.100)    Data  0.069 ( 0.069)    Loss 3.6310e-02 (3.6310e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.023)    Loss 6.6953e-02 (6.1656e-02)    Acc@1  97.66 ( 97.79)   Acc@5 100.00 ( 99.99)
Epoch: [25][201/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 1.6259e-01 (7.0673e-02)    Acc@1  93.75 ( 97.51)   Acc@5 100.00 (100.00)
Epoch: [25][301/313]    Time  0.041 ( 0.042)    Data  0.023 ( 0.023)    Loss 7.5137e-02 (7.6212e-02)    Acc@1  97.66 ( 97.32)   Acc@5 100.00 ( 99.99)
train Loss: 0.0772 Acc: 0.9727
Epoch: [25][  1/313]    Time  0.041 ( 0.042)    Data  0.035 ( 0.023)    Loss 1.6534e+00 (8.2191e-02)    Acc@1  71.88 ( 97.19)   Acc@5  96.09 ( 99.98)
val Loss: 1.3168 Acc: 0.7463

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.101 ( 0.101)    Data  0.069 ( 0.069)    Loss 1.1207e-01 (1.1207e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.040 ( 0.044)    Data  0.022 ( 0.023)    Loss 1.2542e-02 (7.3983e-02)    Acc@1 100.00 ( 97.46)   Acc@5 100.00 (100.00)
Epoch: [26][201/313]    Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 7.5724e-02 (7.0896e-02)    Acc@1  96.88 ( 97.57)   Acc@5 100.00 (100.00)
Epoch: [26][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 3.9441e-02 (7.6399e-02)    Acc@1  98.44 ( 97.37)   Acc@5 100.00 (100.00)
train Loss: 0.0768 Acc: 0.9736
Epoch: [26][  1/313]    Time  0.043 ( 0.043)    Data  0.035 ( 0.023)    Loss 8.6155e-01 (7.9290e-02)    Acc@1  76.56 ( 97.29)   Acc@5 100.00 (100.00)
val Loss: 1.2959 Acc: 0.7569

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 2.5729e-02 (2.5729e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 4.7795e-02 (5.9925e-02)    Acc@1  98.44 ( 97.80)   Acc@5 100.00 (100.00)
Epoch: [27][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 5.8060e-02 (6.0058e-02)    Acc@1  97.66 ( 97.87)   Acc@5 100.00 (100.00)
Epoch: [27][301/313]    Time  0.044 ( 0.042)    Data  0.022 ( 0.023)    Loss 1.1280e-01 (6.5580e-02)    Acc@1  96.09 ( 97.63)   Acc@5 100.00 ( 99.99)
train Loss: 0.0658 Acc: 0.9763
Epoch: [27][  1/313]    Time  0.040 ( 0.042)    Data  0.033 ( 0.023)    Loss 1.0508e+00 (6.8933e-02)    Acc@1  75.78 ( 97.56)   Acc@5  98.44 ( 99.99)
val Loss: 1.2503 Acc: 0.7598

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 2.8919e-02 (2.8919e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.7207e-02 (4.7327e-02)    Acc@1  97.66 ( 98.45)   Acc@5 100.00 (100.00)
Epoch: [28][201/313]    Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 1.3563e-02 (4.5743e-02)    Acc@1 100.00 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [28][301/313]    Time  0.042 ( 0.042)    Data  0.022 ( 0.023)    Loss 2.7199e-02 (4.9789e-02)    Acc@1  98.44 ( 98.28)   Acc@5 100.00 (100.00)
train Loss: 0.0506 Acc: 0.9824
Epoch: [28][  1/313]    Time  0.042 ( 0.042)    Data  0.035 ( 0.023)    Loss 1.0471e+00 (5.3794e-02)    Acc@1  78.91 ( 98.18)   Acc@5  98.44 ( 99.99)
val Loss: 1.1977 Acc: 0.7591

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.099 ( 0.099)    Data  0.068 ( 0.068)    Loss 3.2372e-02 (3.2372e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 7.8050e-02 (4.9492e-02)    Acc@1  98.44 ( 98.25)   Acc@5 100.00 (100.00)
Epoch: [29][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 1.9891e-02 (5.7256e-02)    Acc@1  99.22 ( 97.97)   Acc@5 100.00 (100.00)
Epoch: [29][301/313]    Time  0.042 ( 0.042)    Data  0.022 ( 0.023)    Loss 1.1052e-01 (6.1191e-02)    Acc@1  96.88 ( 97.88)   Acc@5 100.00 (100.00)
train Loss: 0.0610 Acc: 0.9789
Epoch: [29][  1/313]    Time  0.040 ( 0.042)    Data  0.034 ( 0.023)    Loss 9.9613e-01 (6.4025e-02)    Acc@1  78.91 ( 97.83)   Acc@5  99.22 (100.00)
val Loss: 1.2293 Acc: 0.7689

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 2.0266e-02 (2.0266e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 7.0526e-03 (2.9555e-02)    Acc@1 100.00 ( 99.04)   Acc@5 100.00 ( 99.99)
Epoch: [30][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.9324e-02 (2.3905e-02)    Acc@1  99.22 ( 99.25)   Acc@5 100.00 (100.00)
Epoch: [30][301/313]    Time  0.045 ( 0.042)    Data  0.023 ( 0.023)    Loss 1.3030e-02 (2.0524e-02)    Acc@1  99.22 ( 99.36)   Acc@5 100.00 (100.00)
train Loss: 0.0202 Acc: 0.9937
Epoch: [30][  1/313]    Time  0.041 ( 0.042)    Data  0.035 ( 0.023)    Loss 1.1740e+00 (2.3858e-02)    Acc@1  75.00 ( 99.29)   Acc@5  97.66 ( 99.99)
val Loss: 1.0981 Acc: 0.7867

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.112 ( 0.112)    Data  0.081 ( 0.081)    Loss 8.3431e-03 (8.3431e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.7845e-03 (7.6354e-03)    Acc@1 100.00 ( 99.88)   Acc@5 100.00 (100.00)
Epoch: [31][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 1.3333e-03 (6.9275e-03)    Acc@1 100.00 ( 99.91)   Acc@5 100.00 (100.00)
Epoch: [31][301/313]    Time  0.042 ( 0.042)    Data  0.023 ( 0.023)    Loss 5.2323e-03 (6.6162e-03)    Acc@1 100.00 ( 99.91)   Acc@5 100.00 (100.00)
train Loss: 0.0066 Acc: 0.9991
Epoch: [31][  1/313]    Time  0.042 ( 0.042)    Data  0.035 ( 0.023)    Loss 9.6287e-01 (9.6263e-03)    Acc@1  79.69 ( 99.84)   Acc@5  99.22 (100.00)
val Loss: 1.0872 Acc: 0.7892

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 1.8112e-03 (1.8112e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 2.1657e-03 (4.2715e-03)    Acc@1 100.00 ( 99.97)   Acc@5 100.00 (100.00)
Epoch: [32][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.3976e-03 (4.2111e-03)    Acc@1 100.00 ( 99.97)   Acc@5 100.00 (100.00)
Epoch: [32][301/313]    Time  0.043 ( 0.042)    Data  0.023 ( 0.023)    Loss 1.7764e-03 (4.4451e-03)    Acc@1 100.00 ( 99.96)   Acc@5 100.00 (100.00)
train Loss: 0.0045 Acc: 0.9995
Epoch: [32][  1/313]    Time  0.042 ( 0.042)    Data  0.035 ( 0.023)    Loss 1.0019e+00 (7.7245e-03)    Acc@1  78.91 ( 99.88)   Acc@5  98.44 (100.00)
val Loss: 1.0965 Acc: 0.7891

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 2.5528e-03 (2.5528e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.042 ( 0.043)    Data  0.022 ( 0.023)    Loss 5.5822e-03 (3.6859e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [33][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 4.9645e-03 (3.5229e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [33][301/313]    Time  0.044 ( 0.042)    Data  0.024 ( 0.023)    Loss 1.4135e-03 (3.5834e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
train Loss: 0.0036 Acc: 0.9998
Epoch: [33][  1/313]    Time  0.041 ( 0.042)    Data  0.034 ( 0.023)    Loss 1.5036e+00 (8.3818e-03)    Acc@1  75.78 ( 99.90)   Acc@5  98.44 (100.00)
val Loss: 1.1062 Acc: 0.7913

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.102 ( 0.102)    Data  0.069 ( 0.069)    Loss 3.0818e-03 (3.0818e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.023)    Loss 6.7301e-03 (3.3427e-03)    Acc@1 100.00 ( 99.93)   Acc@5 100.00 (100.00)
Epoch: [34][201/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 6.3711e-04 (3.2432e-03)    Acc@1 100.00 ( 99.96)   Acc@5 100.00 (100.00)
Epoch: [34][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 7.8508e-04 (3.0722e-03)    Acc@1 100.00 ( 99.97)   Acc@5 100.00 (100.00)
train Loss: 0.0031 Acc: 0.9997
Epoch: [34][  1/313]    Time  0.041 ( 0.043)    Data  0.035 ( 0.023)    Loss 8.8741e-01 (5.9185e-03)    Acc@1  80.47 ( 99.91)   Acc@5  99.22 (100.00)
val Loss: 1.1059 Acc: 0.7925

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.099 ( 0.099)    Data  0.071 ( 0.071)    Loss 3.5773e-03 (3.5773e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.6330e-03 (2.0367e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.1072e-03 (2.0910e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.8490e-03 (2.0922e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0021 Acc: 1.0000
Epoch: [35][  1/313]    Time  0.041 ( 0.043)    Data  0.034 ( 0.023)    Loss 1.0695e+00 (5.4976e-03)    Acc@1  79.69 ( 99.93)   Acc@5  96.88 ( 99.99)
val Loss: 1.1155 Acc: 0.7926

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 1.1809e-03 (1.1809e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 5.9551e-04 (1.8208e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.0101e-03 (1.8564e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.041 ( 0.042)    Data  0.022 ( 0.023)    Loss 1.8229e-03 (1.8455e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0019 Acc: 0.9999
Epoch: [36][  1/313]    Time  0.041 ( 0.042)    Data  0.035 ( 0.023)    Loss 1.4261e+00 (6.3975e-03)    Acc@1  77.34 ( 99.92)   Acc@5  98.44 (100.00)
val Loss: 1.1281 Acc: 0.7910

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 2.0170e-03 (2.0170e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.042 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.9036e-04 (1.5739e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [37][201/313]    Time  0.041 ( 0.042)    Data  0.023 ( 0.023)    Loss 1.5331e-03 (1.4813e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][301/313]    Time  0.043 ( 0.042)    Data  0.023 ( 0.023)    Loss 1.0753e-03 (1.5495e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0016 Acc: 0.9999
Epoch: [37][  1/313]    Time  0.041 ( 0.042)    Data  0.035 ( 0.023)    Loss 8.1267e-01 (4.1632e-03)    Acc@1  82.81 ( 99.94)   Acc@5  99.22 (100.00)
val Loss: 1.1401 Acc: 0.7910

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 1.9293e-03 (1.9293e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.044 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.7404e-03 (1.5203e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 1.0302e-03 (1.4257e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [38][301/313]    Time  0.042 ( 0.042)    Data  0.022 ( 0.023)    Loss 2.4354e-03 (1.3654e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 0.9999
Epoch: [38][  1/313]    Time  0.041 ( 0.042)    Data  0.034 ( 0.023)    Loss 1.0548e+00 (4.7085e-03)    Acc@1  76.56 ( 99.92)   Acc@5  99.22 (100.00)
val Loss: 1.1381 Acc: 0.7908

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.098 ( 0.098)    Data  0.069 ( 0.069)    Loss 1.1867e-03 (1.1867e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.4341e-03 (1.0991e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][201/313]    Time  0.043 ( 0.042)    Data  0.023 ( 0.023)    Loss 1.6637e-03 (1.0646e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.041 ( 0.042)    Data  0.022 ( 0.023)    Loss 4.7408e-04 (1.0723e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0011 Acc: 1.0000
Epoch: [39][  1/313]    Time  0.041 ( 0.042)    Data  0.034 ( 0.023)    Loss 1.1033e+00 (4.5843e-03)    Acc@1  80.47 ( 99.94)   Acc@5  98.44 (100.00)
val Loss: 1.1516 Acc: 0.7903

Training complete in 11m 3s
Best val Acc: 0.792600
Test Loss: 0.223105

Test Accuracy of airplane: 84% (809/963)
Test Accuracy of automobile: 92% (918/995)
Test Accuracy of  bird: 68% (693/1013)
Test Accuracy of   cat: 63% (646/1014)
Test Accuracy of  deer: 74% (755/1013)
Test Accuracy of   dog: 67% (652/971)
Test Accuracy of  frog: 83% (847/1017)
Test Accuracy of horse: 80% (816/1012)
Test Accuracy of  ship: 89% (893/997)
Test Accuracy of truck: 89% (897/1005)

Test Accuracy (Overall): 79% (7926/10000)