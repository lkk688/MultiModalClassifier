PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\MultiModalClassifier\data\" --model_name 'squeezenetcustom' --learningratename 'StepLR' --optimizer 'SGD'       
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_squeezenetcustom_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
SqueezeNet (SqueezeNet)                  [128, 3, 224, 224]   [128, 490]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 96, 224, 224]  2,688                True
├─BatchNorm2d (bn1)                      [128, 96, 224, 224]  [128, 96, 224, 224]  192                  True
├─ReLU (relu)                            [128, 96, 224, 224]  [128, 96, 224, 224]  --                   --
├─MaxPool2d (maxpool1)                   [128, 96, 224, 224]  [128, 96, 112, 112]  --                   --
├─fire (fire2)                           [128, 96, 112, 112]  [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 96, 112, 112]  [128, 16, 112, 112]  1,552                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire3)                           [128, 128, 112, 112] [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 16, 112, 112]  2,064                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire4)                           [128, 128, 112, 112] [128, 256, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 32, 112, 112]  4,128                True
│    └─BatchNorm2d (bn1)                 [128, 32, 112, 112]  [128, 32, 112, 112]  64                   True
│    └─ReLU (relu1)                      [128, 32, 112, 112]  [128, 32, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 32, 112, 112]  [128, 128, 112, 112] 4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─Conv2d (conv3)                    [128, 32, 112, 112]  [128, 128, 112, 112] 36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─ReLU (relu2)                      [128, 256, 112, 112] [128, 256, 112, 112] --                   --
├─MaxPool2d (maxpool2)                   [128, 256, 112, 112] [128, 256, 56, 56]   --                   --
├─fire (fire5)                           [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 32, 56, 56]    8,224                True
│    └─BatchNorm2d (bn1)                 [128, 32, 56, 56]    [128, 32, 56, 56]    64                   True
│    └─ReLU (relu1)                      [128, 32, 56, 56]    [128, 32, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 32, 56, 56]    [128, 128, 56, 56]   4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─Conv2d (conv3)                    [128, 32, 56, 56]    [128, 128, 56, 56]   36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─ReLU (relu2)                      [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─fire (fire6)                           [128, 256, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 48, 56, 56]    12,336               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire7)                           [128, 384, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 48, 56, 56]    18,480               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire8)                           [128, 384, 56, 56]   [128, 512, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 64, 56, 56]    24,640               True
│    └─BatchNorm2d (bn1)                 [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    └─ReLU (relu1)                      [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 56, 56]    [128, 256, 56, 56]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 56, 56]    [128, 256, 56, 56]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─ReLU (relu2)                      [128, 512, 56, 56]   [128, 512, 56, 56]   --                   --
├─MaxPool2d (maxpool3)                   [128, 512, 56, 56]   [128, 512, 28, 28]   --                   --
├─fire (fire9)                           [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    └─Conv2d (conv1)                    [128, 512, 28, 28]   [128, 64, 28, 28]    32,832               True
│    └─BatchNorm2d (bn1)                 [128, 64, 28, 28]    [128, 64, 28, 28]    128                  True
│    └─ReLU (relu1)                      [128, 64, 28, 28]    [128, 64, 28, 28]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 28, 28]    [128, 256, 28, 28]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 28, 28]    [128, 256, 28, 28]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─ReLU (relu2)                      [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Conv2d (conv2)                         [128, 512, 28, 28]   [128, 10, 28, 28]    5,130                True
├─AvgPool2d (avg_pool)                   [128, 10, 28, 28]    [128, 10, 7, 7]      --                   --
========================================================================================================================
Total params: 734,986
Trainable params: 734,986
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 331.85
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 36693.51
Params size (MB): 2.94
Estimated Total Size (MB): 36773.52
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.514 ( 0.514)    Data  0.096 ( 0.096)    Loss 2.3355e+00 (2.3355e+00)    Acc@1  10.16 ( 10.16)   Acc@5  53.91 ( 53.91)
STAGE:2024-03-25 22:04:47 36428:18208 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:04:47 36428:18208 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:04:47 36428:18208 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-25 22:04:49 36428:18208 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:04:50 36428:18208 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:04:50 36428:18208 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.049 ( 0.097)    Data  0.030 ( 0.071)    Loss 1.5593e+00 (1.8891e+00)    Acc@1  44.53 ( 30.45)   Acc@5  90.62 ( 82.16)
Epoch: [0][201/313]     Time  0.044 ( 0.070)    Data  0.024 ( 0.048)    Loss 1.2415e+00 (1.6752e+00)    Acc@1  53.91 ( 38.33)   Acc@5  93.75 ( 87.11)
Epoch: [0][301/313]     Time  0.041 ( 0.061)    Data  0.022 ( 0.040)    Loss 1.1930e+00 (1.5562e+00)    Acc@1  59.38 ( 43.03)   Acc@5  94.53 ( 89.44)
train Loss: 1.5441 Acc: 0.4352
Epoch: [0][  1/313]     Time  0.042 ( 0.061)    Data  0.036 ( 0.039)    Loss 1.2805e+00 (1.5432e+00)    Acc@1  55.47 ( 43.56)   Acc@5  94.53 ( 89.64)
val Loss: 1.2960 Acc: 0.5356

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.107 ( 0.107)    Data  0.076 ( 0.076)    Loss 1.0444e+00 (1.0444e+00)    Acc@1  64.84 ( 64.84)   Acc@5  97.66 ( 97.66)
Epoch: [1][101/313]     Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.0440e+00 (1.1857e+00)    Acc@1  64.06 ( 57.08)   Acc@5  95.31 ( 95.06)
Epoch: [1][201/313]     Time  0.042 ( 0.043)    Data  0.022 ( 0.023)    Loss 9.7838e-01 (1.1270e+00)    Acc@1  64.06 ( 59.41)   Acc@5  96.09 ( 95.63)
Epoch: [1][301/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 8.9364e-01 (1.0883e+00)    Acc@1  64.06 ( 60.81)   Acc@5  98.44 ( 95.98)
train Loss: 1.0815 Acc: 0.6105
Epoch: [1][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.023)    Loss 9.9736e-01 (1.0812e+00)    Acc@1  70.31 ( 61.08)   Acc@5  97.66 ( 96.03)
val Loss: 1.1664 Acc: 0.6030

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.106 ( 0.106)    Data  0.075 ( 0.075)    Loss 8.6134e-01 (8.6134e-01)    Acc@1  67.19 ( 67.19)   Acc@5  98.44 ( 98.44)
Epoch: [2][101/313]     Time  0.041 ( 0.044)    Data  0.022 ( 0.023)    Loss 8.9303e-01 (9.0621e-01)    Acc@1  67.97 ( 67.95)   Acc@5 100.00 ( 97.39)
Epoch: [2][201/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.0035e+00 (8.9237e-01)    Acc@1  60.94 ( 68.42)   Acc@5  97.66 ( 97.54)
Epoch: [2][301/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 7.3977e-01 (8.7823e-01)    Acc@1  71.09 ( 69.00)   Acc@5 100.00 ( 97.57)
train Loss: 0.8748 Acc: 0.6913
Epoch: [2][  1/313]     Time  0.042 ( 0.043)    Data  0.034 ( 0.023)    Loss 1.1159e+00 (8.7556e-01)    Acc@1  62.50 ( 69.11)   Acc@5  95.31 ( 97.59)
val Loss: 1.0965 Acc: 0.6195

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 6.7823e-01 (6.7823e-01)    Acc@1  75.78 ( 75.78)   Acc@5  98.44 ( 98.44)
Epoch: [3][101/313]     Time  0.039 ( 0.044)    Data  0.021 ( 0.024)    Loss 6.9192e-01 (7.5187e-01)    Acc@1  75.00 ( 73.59)   Acc@5  97.66 ( 98.14)
Epoch: [3][201/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 6.8762e-01 (7.5720e-01)    Acc@1  77.34 ( 73.40)   Acc@5 100.00 ( 98.10)
Epoch: [3][301/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 6.4885e-01 (7.4722e-01)    Acc@1  77.34 ( 73.68)   Acc@5  97.66 ( 98.13)
train Loss: 0.7468 Acc: 0.7367
Epoch: [3][  1/313]     Time  0.115 ( 0.043)    Data  0.103 ( 0.024)    Loss 1.0400e+00 (7.4776e-01)    Acc@1  66.41 ( 73.65)   Acc@5  98.44 ( 98.14)
val Loss: 0.8902 Acc: 0.6911

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.128 ( 0.128)    Data  0.110 ( 0.110)    Loss 6.6225e-01 (6.6225e-01)    Acc@1  75.78 ( 75.78)   Acc@5 100.00 (100.00)
Epoch: [4][101/313]     Time  0.044 ( 0.044)    Data  0.026 ( 0.025)    Loss 5.4915e-01 (6.3168e-01)    Acc@1  83.59 ( 77.75)   Acc@5  99.22 ( 98.72)
Epoch: [4][201/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 8.0679e-01 (6.4864e-01)    Acc@1  72.66 ( 77.18)   Acc@5  96.09 ( 98.65)
Epoch: [4][301/313]     Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 6.7279e-01 (6.4668e-01)    Acc@1  75.00 ( 77.20)   Acc@5  98.44 ( 98.67)
train Loss: 0.6473 Acc: 0.7716
Epoch: [4][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 7.4964e-01 (6.4760e-01)    Acc@1  75.78 ( 77.15)   Acc@5  99.22 ( 98.69)
val Loss: 0.8191 Acc: 0.7155

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.125 ( 0.125)    Data  0.088 ( 0.088)    Loss 4.1316e-01 (4.1316e-01)    Acc@1  84.38 ( 84.38)   Acc@5 100.00 (100.00)
Epoch: [5][101/313]     Time  0.059 ( 0.052)    Data  0.035 ( 0.029)    Loss 5.1050e-01 (5.7173e-01)    Acc@1  80.47 ( 79.78)   Acc@5  99.22 ( 99.03)
Epoch: [5][201/313]     Time  0.063 ( 0.048)    Data  0.028 ( 0.026)    Loss 6.0770e-01 (5.7561e-01)    Acc@1  78.91 ( 79.98)   Acc@5  99.22 ( 98.94)
Epoch: [5][301/313]     Time  0.041 ( 0.046)    Data  0.022 ( 0.025)    Loss 5.6558e-01 (5.7589e-01)    Acc@1  80.47 ( 79.92)   Acc@5  98.44 ( 98.94)
train Loss: 0.5755 Acc: 0.7991
Epoch: [5][  1/313]     Time  0.042 ( 0.046)    Data  0.035 ( 0.025)    Loss 6.2622e-01 (5.7567e-01)    Acc@1  82.03 ( 79.92)   Acc@5  99.22 ( 98.94)
val Loss: 0.7797 Acc: 0.7430

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.103 ( 0.103)    Data  0.071 ( 0.071)    Loss 3.8092e-01 (3.8092e-01)    Acc@1  86.72 ( 86.72)   Acc@5  99.22 ( 99.22)
Epoch: [6][101/313]     Time  0.046 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.3867e-01 (4.9630e-01)    Acc@1  89.06 ( 82.50)   Acc@5 100.00 ( 99.23)
Epoch: [6][201/313]     Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 4.5536e-01 (5.0489e-01)    Acc@1  84.38 ( 82.12)   Acc@5 100.00 ( 99.27)
Epoch: [6][301/313]     Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 3.7095e-01 (5.1236e-01)    Acc@1  85.16 ( 81.88)   Acc@5  99.22 ( 99.22)
train Loss: 0.5123 Acc: 0.8190
Epoch: [6][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.023)    Loss 9.4687e-01 (5.1365e-01)    Acc@1  70.31 ( 81.86)   Acc@5  98.44 ( 99.22)
val Loss: 0.8761 Acc: 0.7126

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 5.2395e-01 (5.2395e-01)    Acc@1  79.69 ( 79.69)   Acc@5  98.44 ( 98.44)
Epoch: [7][101/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.1726e-01 (4.3364e-01)    Acc@1  85.94 ( 84.94)   Acc@5  99.22 ( 99.33)
Epoch: [7][201/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 5.1390e-01 (4.5586e-01)    Acc@1  80.47 ( 84.08)   Acc@5  99.22 ( 99.32)
Epoch: [7][301/313]     Time  0.041 ( 0.043)    Data  0.021 ( 0.023)    Loss 2.9913e-01 (4.6684e-01)    Acc@1  88.28 ( 83.72)   Acc@5 100.00 ( 99.34)
train Loss: 0.4665 Acc: 0.8371
Epoch: [7][  1/313]     Time  0.042 ( 0.043)    Data  0.036 ( 0.023)    Loss 8.8693e-01 (4.6786e-01)    Acc@1  71.09 ( 83.66)   Acc@5  99.22 ( 99.34)
val Loss: 0.9209 Acc: 0.7082

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.103 ( 0.103)    Data  0.071 ( 0.071)    Loss 3.5878e-01 (3.5878e-01)    Acc@1  88.28 ( 88.28)   Acc@5 100.00 (100.00)
Epoch: [8][101/313]     Time  0.041 ( 0.044)    Data  0.022 ( 0.023)    Loss 4.2752e-01 (3.7665e-01)    Acc@1  84.38 ( 86.78)   Acc@5 100.00 ( 99.62)
Epoch: [8][201/313]     Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 2.4646e-01 (4.0780e-01)    Acc@1  90.62 ( 85.71)   Acc@5 100.00 ( 99.48)
Epoch: [8][301/313]     Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 4.6750e-01 (4.1879e-01)    Acc@1  81.25 ( 85.32)   Acc@5  99.22 ( 99.47)
train Loss: 0.4190 Acc: 0.8533
Epoch: [8][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.023)    Loss 1.0476e+00 (4.2096e-01)    Acc@1  62.50 ( 85.26)   Acc@5  98.44 ( 99.46)
val Loss: 0.7847 Acc: 0.7426

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 3.5096e-01 (3.5096e-01)    Acc@1  87.50 ( 87.50)   Acc@5 100.00 (100.00)
Epoch: [9][101/313]     Time  0.041 ( 0.044)    Data  0.022 ( 0.023)    Loss 4.1389e-01 (3.5042e-01)    Acc@1  82.81 ( 87.83)   Acc@5 100.00 ( 99.64)
Epoch: [9][201/313]     Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 4.1409e-01 (3.6678e-01)    Acc@1  86.72 ( 87.18)   Acc@5 100.00 ( 99.65)
Epoch: [9][301/313]     Time  0.044 ( 0.043)    Data  0.023 ( 0.023)    Loss 4.3199e-01 (3.8005e-01)    Acc@1  85.16 ( 86.69)   Acc@5 100.00 ( 99.63)
train Loss: 0.3800 Acc: 0.8666
Epoch: [9][  1/313]     Time  0.049 ( 0.043)    Data  0.041 ( 0.023)    Loss 8.0861e-01 (3.8135e-01)    Acc@1  71.88 ( 86.62)   Acc@5  99.22 ( 99.64)
val Loss: 0.8562 Acc: 0.7387

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 3.4323e-01 (3.4323e-01)    Acc@1  90.62 ( 90.62)   Acc@5 100.00 (100.00)
Epoch: [10][101/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 4.7432e-01 (3.1520e-01)    Acc@1  81.25 ( 88.95)   Acc@5 100.00 ( 99.68)
Epoch: [10][201/313]    Time  0.046 ( 0.043)    Data  0.024 ( 0.023)    Loss 3.6712e-01 (3.3185e-01)    Acc@1  89.06 ( 88.39)   Acc@5 100.00 ( 99.69)
Epoch: [10][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.7916e-01 (3.4932e-01)    Acc@1  88.28 ( 87.72)   Acc@5  99.22 ( 99.66)
train Loss: 0.3515 Acc: 0.8767
Epoch: [10][  1/313]    Time  0.044 ( 0.043)    Data  0.035 ( 0.023)    Loss 7.8785e-01 (3.5288e-01)    Acc@1  74.22 ( 87.63)   Acc@5  98.44 ( 99.66)
val Loss: 0.7727 Acc: 0.7597

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.107 ( 0.107)    Data  0.075 ( 0.075)    Loss 2.6173e-01 (2.6173e-01)    Acc@1  92.19 ( 92.19)   Acc@5  99.22 ( 99.22)
Epoch: [11][101/313]    Time  0.046 ( 0.044)    Data  0.026 ( 0.023)    Loss 3.3643e-01 (2.8157e-01)    Acc@1  90.62 ( 90.01)   Acc@5 100.00 ( 99.78)
Epoch: [11][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 2.7000e-01 (3.0346e-01)    Acc@1  89.06 ( 89.25)   Acc@5 100.00 ( 99.75)
Epoch: [11][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.0853e-01 (3.1950e-01)    Acc@1  92.19 ( 88.61)   Acc@5 100.00 ( 99.72)
train Loss: 0.3219 Acc: 0.8850
Epoch: [11][  1/313]    Time  0.044 ( 0.043)    Data  0.037 ( 0.023)    Loss 8.4129e-01 (3.2357e-01)    Acc@1  75.00 ( 88.46)   Acc@5  98.44 ( 99.71)
val Loss: 0.7948 Acc: 0.7508

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.114 ( 0.114)    Data  0.085 ( 0.085)    Loss 3.2411e-01 (3.2411e-01)    Acc@1  87.50 ( 87.50)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.3914e-01 (2.6293e-01)    Acc@1  88.28 ( 90.68)   Acc@5 100.00 ( 99.83)
Epoch: [12][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 4.4208e-01 (2.8195e-01)    Acc@1  84.38 ( 90.12)   Acc@5 100.00 ( 99.79)
Epoch: [12][301/313]    Time  0.044 ( 0.043)    Data  0.022 ( 0.023)    Loss 4.3534e-01 (3.0061e-01)    Acc@1  86.72 ( 89.39)   Acc@5  99.22 ( 99.78)
train Loss: 0.3026 Acc: 0.8932
Epoch: [12][  1/313]    Time  0.044 ( 0.043)    Data  0.034 ( 0.023)    Loss 6.7106e-01 (3.0381e-01)    Acc@1  79.69 ( 89.29)   Acc@5  98.44 ( 99.76)
val Loss: 0.7412 Acc: 0.7694

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 2.0108e-01 (2.0108e-01)    Acc@1  91.41 ( 91.41)   Acc@5 100.00 (100.00)
Epoch: [13][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.5150e-01 (2.4885e-01)    Acc@1  96.88 ( 91.35)   Acc@5 100.00 ( 99.85)
Epoch: [13][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 3.4855e-01 (2.6345e-01)    Acc@1  87.50 ( 90.73)   Acc@5 100.00 ( 99.83)
Epoch: [13][301/313]    Time  0.045 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.2143e-01 (2.7671e-01)    Acc@1  89.06 ( 90.24)   Acc@5  99.22 ( 99.79)
train Loss: 0.2783 Acc: 0.9019
Epoch: [13][  1/313]    Time  0.051 ( 0.043)    Data  0.041 ( 0.024)    Loss 8.2881e-01 (2.8001e-01)    Acc@1  75.00 ( 90.14)   Acc@5  99.22 ( 99.78)
val Loss: 0.8701 Acc: 0.7454

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.102 ( 0.102)    Data  0.072 ( 0.072)    Loss 2.3321e-01 (2.3321e-01)    Acc@1  89.06 ( 89.06)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.045 ( 0.046)    Data  0.026 ( 0.025)    Loss 2.7551e-01 (2.3389e-01)    Acc@1  91.41 ( 91.87)   Acc@5  99.22 ( 99.87)
Epoch: [14][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.5697e-01 (2.4509e-01)    Acc@1  89.84 ( 91.39)   Acc@5 100.00 ( 99.84)
Epoch: [14][301/313]    Time  0.040 ( 0.044)    Data  0.022 ( 0.024)    Loss 3.6094e-01 (2.6266e-01)    Acc@1  88.28 ( 90.74)   Acc@5 100.00 ( 99.84)
train Loss: 0.2638 Acc: 0.9069
Epoch: [14][  1/313]    Time  0.042 ( 0.044)    Data  0.035 ( 0.024)    Loss 7.0956e-01 (2.6520e-01)    Acc@1  78.12 ( 90.64)   Acc@5  98.44 ( 99.83)
val Loss: 0.8695 Acc: 0.7560

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 1.9851e-01 (1.9851e-01)    Acc@1  92.97 ( 92.97)   Acc@5  99.22 ( 99.22)
Epoch: [15][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.2135e-01 (1.9093e-01)    Acc@1  94.53 ( 93.08)   Acc@5 100.00 ( 99.95)
Epoch: [15][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.2520e-01 (2.1443e-01)    Acc@1  88.28 ( 92.28)   Acc@5 100.00 ( 99.91)
Epoch: [15][301/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 3.2767e-01 (2.4061e-01)    Acc@1  88.28 ( 91.45)   Acc@5  99.22 ( 99.87)
train Loss: 0.2421 Acc: 0.9137
Epoch: [15][  1/313]    Time  0.045 ( 0.043)    Data  0.037 ( 0.023)    Loss 5.8866e-01 (2.4316e-01)    Acc@1  82.81 ( 91.34)   Acc@5  99.22 ( 99.86)
val Loss: 0.7476 Acc: 0.7759

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.102 ( 0.102)    Data  0.073 ( 0.073)    Loss 1.9034e-01 (1.9034e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.2198e-01 (1.8622e-01)    Acc@1  95.31 ( 93.44)   Acc@5 100.00 ( 99.91)
Epoch: [16][201/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.2381e-01 (2.1292e-01)    Acc@1  89.84 ( 92.42)   Acc@5 100.00 ( 99.88)
Epoch: [16][301/313]    Time  0.044 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.9403e-01 (2.2844e-01)    Acc@1  89.84 ( 91.82)   Acc@5 100.00 ( 99.89)
train Loss: 0.2300 Acc: 0.9176
Epoch: [16][  1/313]    Time  0.044 ( 0.043)    Data  0.037 ( 0.023)    Loss 8.4616e-01 (2.3200e-01)    Acc@1  78.91 ( 91.71)   Acc@5  98.44 ( 99.88)
val Loss: 0.7423 Acc: 0.7861

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 2.1568e-01 (2.1568e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.045 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.1258e-01 (1.7489e-01)    Acc@1  98.44 ( 93.96)   Acc@5 100.00 ( 99.95)
Epoch: [17][201/313]    Time  0.045 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.6354e-01 (1.9185e-01)    Acc@1  89.06 ( 93.18)   Acc@5 100.00 ( 99.96)
Epoch: [17][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.7329e-01 (2.0867e-01)    Acc@1  92.97 ( 92.57)   Acc@5 100.00 ( 99.93)
train Loss: 0.2118 Acc: 0.9243
Epoch: [17][  1/313]    Time  0.046 ( 0.043)    Data  0.037 ( 0.023)    Loss 8.3107e-01 (2.1380e-01)    Acc@1  74.22 ( 92.37)   Acc@5  96.88 ( 99.92)
val Loss: 0.7621 Acc: 0.7723

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 1.3406e-01 (1.3406e-01)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.044 ( 0.044)    Data  0.023 ( 0.023)    Loss 2.7725e-01 (1.7122e-01)    Acc@1  90.62 ( 94.01)   Acc@5 100.00 ( 99.94)
Epoch: [18][201/313]    Time  0.040 ( 0.044)    Data  0.022 ( 0.024)    Loss 2.0100e-01 (1.9120e-01)    Acc@1  91.41 ( 93.31)   Acc@5 100.00 ( 99.94)
Epoch: [18][301/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.6695e-01 (2.0602e-01)    Acc@1  89.84 ( 92.72)   Acc@5 100.00 ( 99.94)
train Loss: 0.2077 Acc: 0.9262
Epoch: [18][  1/313]    Time  0.043 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.1360e+00 (2.1065e-01)    Acc@1  65.62 ( 92.54)   Acc@5  96.88 ( 99.93)
val Loss: 1.1652 Acc: 0.7156

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.116 ( 0.116)    Data  0.087 ( 0.087)    Loss 2.7272e-01 (2.7272e-01)    Acc@1  90.62 ( 90.62)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.9583e-01 (1.7710e-01)    Acc@1  93.75 ( 93.82)   Acc@5 100.00 ( 99.95)
Epoch: [19][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 3.5922e-01 (1.8570e-01)    Acc@1  88.28 ( 93.44)   Acc@5 100.00 ( 99.93)
Epoch: [19][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.9280e-01 (1.9659e-01)    Acc@1  92.19 ( 92.95)   Acc@5  99.22 ( 99.94)
train Loss: 0.1982 Acc: 0.9291
Epoch: [19][  1/313]    Time  0.043 ( 0.043)    Data  0.035 ( 0.023)    Loss 5.8928e-01 (1.9949e-01)    Acc@1  84.38 ( 92.88)   Acc@5  98.44 ( 99.93)
val Loss: 0.9111 Acc: 0.7703

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 1.7757e-01 (1.7757e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 1.4534e-01 (1.5300e-01)    Acc@1  94.53 ( 94.66)   Acc@5 100.00 ( 99.98)
Epoch: [20][201/313]    Time  0.045 ( 0.043)    Data  0.024 ( 0.023)    Loss 1.7581e-01 (1.5809e-01)    Acc@1  94.53 ( 94.36)   Acc@5 100.00 ( 99.97)
Epoch: [20][301/313]    Time  0.040 ( 0.042)    Data  0.022 ( 0.023)    Loss 2.9963e-01 (1.7758e-01)    Acc@1  89.06 ( 93.69)   Acc@5 100.00 ( 99.95)
train Loss: 0.1781 Acc: 0.9368
Epoch: [20][  1/313]    Time  0.043 ( 0.043)    Data  0.037 ( 0.023)    Loss 9.0663e-01 (1.8044e-01)    Acc@1  78.91 ( 93.63)   Acc@5  99.22 ( 99.95)
val Loss: 0.8688 Acc: 0.7688

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 2.0017e-01 (2.0017e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.7248e-01 (1.4624e-01)    Acc@1  92.97 ( 94.80)   Acc@5 100.00 ( 99.99)
Epoch: [21][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 2.1827e-01 (1.5444e-01)    Acc@1  92.97 ( 94.49)   Acc@5 100.00 ( 99.98)
Epoch: [21][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.3805e-01 (1.7148e-01)    Acc@1  95.31 ( 93.84)   Acc@5 100.00 ( 99.96)
train Loss: 0.1734 Acc: 0.9378
Epoch: [21][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.023)    Loss 8.4437e-01 (1.7556e-01)    Acc@1  78.91 ( 93.73)   Acc@5  98.44 ( 99.96)
val Loss: 0.9113 Acc: 0.7507

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 1.5168e-01 (1.5168e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.7679e-01 (1.3272e-01)    Acc@1  92.19 ( 95.38)   Acc@5 100.00 ( 99.98)
Epoch: [22][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 2.4907e-01 (1.4616e-01)    Acc@1  89.84 ( 94.86)   Acc@5 100.00 ( 99.97)
Epoch: [22][301/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.4879e-01 (1.7142e-01)    Acc@1  89.84 ( 94.00)   Acc@5 100.00 ( 99.96)
train Loss: 0.1729 Acc: 0.9395
Epoch: [22][  1/313]    Time  0.044 ( 0.043)    Data  0.035 ( 0.023)    Loss 7.9827e-01 (1.7485e-01)    Acc@1  75.00 ( 93.89)   Acc@5  99.22 ( 99.96)
val Loss: 0.8775 Acc: 0.7621

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.105 ( 0.105)    Data  0.073 ( 0.073)    Loss 1.4148e-01 (1.4148e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.023)    Loss 8.8041e-02 (1.3600e-01)    Acc@1  98.44 ( 95.36)   Acc@5 100.00 ( 99.95)
Epoch: [23][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.3753e-01 (1.3752e-01)    Acc@1  96.09 ( 95.25)   Acc@5 100.00 ( 99.96)
Epoch: [23][301/313]    Time  0.045 ( 0.043)    Data  0.024 ( 0.023)    Loss 1.4200e-01 (1.5453e-01)    Acc@1  92.19 ( 94.63)   Acc@5 100.00 ( 99.94)
train Loss: 0.1563 Acc: 0.9457
Epoch: [23][  1/313]    Time  0.042 ( 0.043)    Data  0.034 ( 0.023)    Loss 9.4581e-01 (1.5882e-01)    Acc@1  75.78 ( 94.51)   Acc@5 100.00 ( 99.95)
val Loss: 0.9992 Acc: 0.7470

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.106 ( 0.106)    Data  0.075 ( 0.075)    Loss 1.7321e-01 (1.7321e-01)    Acc@1  91.41 ( 91.41)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.043 ( 0.044)    Data  0.022 ( 0.023)    Loss 1.0726e-01 (1.1556e-01)    Acc@1  96.09 ( 95.89)   Acc@5 100.00 ( 99.99)
Epoch: [24][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 1.5963e-01 (1.2903e-01)    Acc@1  94.53 ( 95.48)   Acc@5 100.00 ( 99.99)
Epoch: [24][301/313]    Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.9317e-01 (1.4663e-01)    Acc@1  89.06 ( 94.80)   Acc@5 100.00 ( 99.98)
train Loss: 0.1491 Acc: 0.9470
Epoch: [24][  1/313]    Time  0.044 ( 0.043)    Data  0.036 ( 0.023)    Loss 1.0388e+00 (1.5197e-01)    Acc@1  75.00 ( 94.64)   Acc@5  98.44 ( 99.98)
val Loss: 1.1590 Acc: 0.7145

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.105 ( 0.105)    Data  0.075 ( 0.075)    Loss 1.1202e-01 (1.1202e-01)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.023)    Loss 7.0796e-02 (1.2292e-01)    Acc@1  98.44 ( 95.80)   Acc@5 100.00 ( 99.98)
Epoch: [25][201/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.2829e-01 (1.2924e-01)    Acc@1  92.97 ( 95.44)   Acc@5 100.00 ( 99.98)
Epoch: [25][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.3475e-01 (1.4575e-01)    Acc@1  92.19 ( 94.75)   Acc@5 100.00 ( 99.98)
train Loss: 0.1474 Acc: 0.9469
Epoch: [25][  1/313]    Time  0.043 ( 0.043)    Data  0.034 ( 0.023)    Loss 7.7229e-01 (1.4938e-01)    Acc@1  78.91 ( 94.64)   Acc@5 100.00 ( 99.98)
val Loss: 0.9453 Acc: 0.7653

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 1.4818e-01 (1.4818e-01)    Acc@1  95.31 ( 95.31)   Acc@5  99.22 ( 99.22)
Epoch: [26][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 7.1335e-02 (1.3160e-01)    Acc@1  97.66 ( 95.47)   Acc@5 100.00 ( 99.98)
Epoch: [26][201/313]    Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.4896e-01 (1.3778e-01)    Acc@1  92.19 ( 95.11)   Acc@5 100.00 ( 99.98)
Epoch: [26][301/313]    Time  0.046 ( 0.043)    Data  0.022 ( 0.023)    Loss 2.3749e-01 (1.4925e-01)    Acc@1  89.06 ( 94.71)   Acc@5 100.00 ( 99.96)
train Loss: 0.1501 Acc: 0.9470
Epoch: [26][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.023)    Loss 1.0781e+00 (1.5302e-01)    Acc@1  70.31 ( 94.62)   Acc@5  97.66 ( 99.96)
val Loss: 0.8548 Acc: 0.7775

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 6.5715e-02 (6.5715e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.023)    Loss 6.0322e-02 (1.2259e-01)    Acc@1  98.44 ( 95.74)   Acc@5 100.00 ( 99.97)
Epoch: [27][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.6065e-01 (1.2739e-01)    Acc@1  91.41 ( 95.53)   Acc@5 100.00 ( 99.97)
Epoch: [27][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.6465e-01 (1.4253e-01)    Acc@1  90.62 ( 95.04)   Acc@5 100.00 ( 99.97)
train Loss: 0.1449 Acc: 0.9498
Epoch: [27][  1/313]    Time  0.043 ( 0.043)    Data  0.036 ( 0.023)    Loss 9.6591e-01 (1.4753e-01)    Acc@1  75.00 ( 94.92)   Acc@5  97.66 ( 99.96)
val Loss: 1.0094 Acc: 0.7437

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 1.2469e-01 (1.2469e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.023)    Loss 3.8516e-02 (1.2113e-01)    Acc@1  98.44 ( 95.69)   Acc@5 100.00 ( 99.98)
Epoch: [28][201/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.023)    Loss 2.2762e-01 (1.2719e-01)    Acc@1  94.53 ( 95.48)   Acc@5 100.00 ( 99.99)
Epoch: [28][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.023)    Loss 2.0998e-01 (1.4317e-01)    Acc@1  90.62 ( 94.85)   Acc@5 100.00 ( 99.98)
train Loss: 0.1441 Acc: 0.9480
Epoch: [28][  1/313]    Time  0.045 ( 0.043)    Data  0.039 ( 0.023)    Loss 1.1304e+00 (1.4726e-01)    Acc@1  73.44 ( 94.73)   Acc@5  96.88 ( 99.97)
val Loss: 1.1062 Acc: 0.7437

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.137 ( 0.137)    Data  0.103 ( 0.103)    Loss 1.5453e-01 (1.5453e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.042 ( 0.053)    Data  0.023 ( 0.030)    Loss 2.1501e-01 (1.2023e-01)    Acc@1  93.75 ( 95.70)   Acc@5 100.00 ( 99.98)
Epoch: [29][201/313]    Time  0.072 ( 0.054)    Data  0.049 ( 0.030)    Loss 1.3841e-01 (1.1853e-01)    Acc@1  95.31 ( 95.90)   Acc@5 100.00 ( 99.98)
Epoch: [29][301/313]    Time  0.044 ( 0.052)    Data  0.024 ( 0.029)    Loss 1.9278e-01 (1.2423e-01)    Acc@1  94.53 ( 95.68)   Acc@5 100.00 ( 99.98)
train Loss: 0.1273 Acc: 0.9556
Epoch: [29][  1/313]    Time  0.042 ( 0.052)    Data  0.036 ( 0.029)    Loss 9.5819e-01 (1.2998e-01)    Acc@1  78.12 ( 95.50)   Acc@5  96.88 ( 99.97)
val Loss: 1.2778 Acc: 0.7216

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.101 ( 0.101)    Data  0.073 ( 0.073)    Loss 1.2675e-01 (1.2675e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.0230e-02 (5.9290e-02)    Acc@1  99.22 ( 98.19)   Acc@5 100.00 (100.00)
Epoch: [30][201/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 3.9071e-02 (4.7361e-02)    Acc@1  99.22 ( 98.64)   Acc@5 100.00 (100.00)
Epoch: [30][301/313]    Time  0.049 ( 0.044)    Data  0.028 ( 0.024)    Loss 4.4131e-02 (4.1627e-02)    Acc@1  98.44 ( 98.84)   Acc@5 100.00 (100.00)
train Loss: 0.0411 Acc: 0.9886
Epoch: [30][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.025)    Loss 9.0673e-01 (4.3830e-02)    Acc@1  77.34 ( 98.79)   Acc@5  96.88 ( 99.99)
val Loss: 0.6677 Acc: 0.8272

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.094 ( 0.094)    Data  0.075 ( 0.075)    Loss 1.1589e-02 (1.1589e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.041 ( 0.044)    Data  0.022 ( 0.024)    Loss 1.7443e-02 (1.5160e-02)    Acc@1 100.00 ( 99.83)   Acc@5 100.00 (100.00)
Epoch: [31][201/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.024)    Loss 1.8126e-02 (1.5382e-02)    Acc@1 100.00 ( 99.79)   Acc@5 100.00 (100.00)
Epoch: [31][301/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.024)    Loss 2.8758e-02 (1.4660e-02)    Acc@1  99.22 ( 99.83)   Acc@5 100.00 (100.00)
train Loss: 0.0147 Acc: 0.9982
Epoch: [31][  1/313]    Time  0.043 ( 0.043)    Data  0.036 ( 0.024)    Loss 5.1326e-01 (1.6244e-02)    Acc@1  83.59 ( 99.77)   Acc@5  98.44 (100.00)
val Loss: 0.6700 Acc: 0.8302

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 5.7473e-03 (5.7473e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 7.5935e-03 (1.0074e-02)    Acc@1 100.00 ( 99.93)   Acc@5 100.00 (100.00)
Epoch: [32][201/313]    Time  0.045 ( 0.042)    Data  0.026 ( 0.023)    Loss 1.2994e-02 (1.0173e-02)    Acc@1 100.00 ( 99.93)   Acc@5 100.00 (100.00)
Epoch: [32][301/313]    Time  0.042 ( 0.042)    Data  0.023 ( 0.023)    Loss 8.8434e-03 (9.8814e-03)    Acc@1 100.00 ( 99.94)   Acc@5 100.00 (100.00)
train Loss: 0.0099 Acc: 0.9994
Epoch: [32][  1/313]    Time  0.040 ( 0.042)    Data  0.034 ( 0.023)    Loss 3.5617e-01 (1.0982e-02)    Acc@1  90.62 ( 99.91)   Acc@5 100.00 (100.00)
val Loss: 0.6789 Acc: 0.8313

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.102 ( 0.102)    Data  0.072 ( 0.072)    Loss 5.1339e-03 (5.1339e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.023)    Loss 7.7807e-03 (7.3564e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [33][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.7463e-03 (7.1349e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [33][301/313]    Time  0.040 ( 0.042)    Data  0.022 ( 0.023)    Loss 9.2547e-03 (7.2325e-03)    Acc@1 100.00 ( 99.97)   Acc@5 100.00 (100.00)
train Loss: 0.0073 Acc: 0.9997
Epoch: [33][  1/313]    Time  0.042 ( 0.042)    Data  0.035 ( 0.023)    Loss 6.1824e-01 (9.2729e-03)    Acc@1  82.81 ( 99.92)   Acc@5  99.22 (100.00)
val Loss: 0.6857 Acc: 0.8310

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.102 ( 0.102)    Data  0.072 ( 0.072)    Loss 1.2016e-02 (1.2016e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 4.2057e-03 (5.7297e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [34][201/313]    Time  0.041 ( 0.042)    Data  0.022 ( 0.023)    Loss 6.5523e-03 (5.8612e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [34][301/313]    Time  0.041 ( 0.042)    Data  0.022 ( 0.023)    Loss 3.7567e-03 (5.7524e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0058 Acc: 0.9999
Epoch: [34][  1/313]    Time  0.044 ( 0.042)    Data  0.038 ( 0.023)    Loss 7.6228e-01 (8.1664e-03)    Acc@1  82.03 ( 99.94)   Acc@5  99.22 (100.00)
val Loss: 0.6943 Acc: 0.8313

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 3.1112e-03 (3.1112e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.040 ( 0.043)    Data  0.021 ( 0.023)    Loss 7.4833e-03 (5.1922e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.045 ( 0.043)    Data  0.025 ( 0.023)    Loss 3.5682e-03 (5.1982e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.040 ( 0.043)    Data  0.022 ( 0.023)    Loss 1.0950e-02 (5.0316e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0050 Acc: 0.9999
Epoch: [35][  1/313]    Time  0.045 ( 0.043)    Data  0.036 ( 0.023)    Loss 5.5216e-01 (6.7433e-03)    Acc@1  86.72 ( 99.95)   Acc@5  99.22 (100.00)
val Loss: 0.6990 Acc: 0.8306

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.103 ( 0.103)    Data  0.074 ( 0.074)    Loss 2.6807e-03 (2.6807e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.041 ( 0.045)    Data  0.023 ( 0.025)    Loss 5.4717e-03 (4.5363e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.054 ( 0.045)    Data  0.028 ( 0.025)    Loss 3.8397e-03 (4.5688e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 5.5291e-03 (4.4724e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0045 Acc: 1.0000
Epoch: [36][  1/313]    Time  0.049 ( 0.046)    Data  0.041 ( 0.025)    Loss 6.4859e-01 (6.5497e-03)    Acc@1  83.59 ( 99.94)   Acc@5 100.00 (100.00)
val Loss: 0.7103 Acc: 0.8313

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.109 ( 0.109)    Data  0.078 ( 0.078)    Loss 4.0286e-03 (4.0286e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.045 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.8711e-03 (3.8151e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 3.2305e-03 (3.8695e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][301/313]    Time  0.044 ( 0.043)    Data  0.023 ( 0.023)    Loss 3.2408e-03 (3.8632e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0039 Acc: 1.0000
Epoch: [37][  1/313]    Time  0.049 ( 0.043)    Data  0.043 ( 0.023)    Loss 6.2933e-01 (5.8788e-03)    Acc@1  81.25 ( 99.94)   Acc@5  99.22 (100.00)
val Loss: 0.7086 Acc: 0.8348

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.099 ( 0.099)    Data  0.069 ( 0.069)    Loss 3.8953e-03 (3.8953e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.023)    Loss 1.7170e-03 (3.6798e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.042 ( 0.042)    Data  0.022 ( 0.023)    Loss 3.9229e-03 (3.6406e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][301/313]    Time  0.043 ( 0.042)    Data  0.023 ( 0.023)    Loss 3.6797e-03 (3.5977e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0036 Acc: 0.9999
Epoch: [38][  1/313]    Time  0.043 ( 0.042)    Data  0.036 ( 0.023)    Loss 5.3102e-01 (5.2936e-03)    Acc@1  85.94 ( 99.95)   Acc@5 100.00 (100.00)
val Loss: 0.7173 Acc: 0.8328

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.089 ( 0.089)    Data  0.070 ( 0.070)    Loss 3.2684e-03 (3.2684e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.040 ( 0.044)    Data  0.022 ( 0.024)    Loss 3.0152e-03 (3.1466e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][201/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 6.0281e-03 (3.1916e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.044 ( 0.043)    Data  0.022 ( 0.024)    Loss 2.0831e-03 (3.1601e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0032 Acc: 1.0000
Epoch: [39][  1/313]    Time  0.040 ( 0.043)    Data  0.033 ( 0.024)    Loss 7.1441e-01 (5.4402e-03)    Acc@1  83.59 ( 99.95)   Acc@5 100.00 (100.00)
val Loss: 0.7150 Acc: 0.8337

Training complete in 10m 48s
Best val Acc: 0.834800
Test Loss: 0.141725

Test Accuracy of airplane: 87% (883/1007)
Test Accuracy of automobile: 91% (894/975)
Test Accuracy of  bird: 78% (789/1001)
Test Accuracy of   cat: 66% (669/1006)
Test Accuracy of  deer: 81% (810/998)
Test Accuracy of   dog: 75% (766/1018)
Test Accuracy of  frog: 89% (901/1008)
Test Accuracy of horse: 84% (841/994)
Test Accuracy of  ship: 90% (882/978)
Test Accuracy of truck: 89% (913/1015)

Test Accuracy (Overall): 83% (8348/10000)