PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\MultiModalClassifier\data\" --model_name 'squeezenetcustom' --learningratename 'MultiStepLR' --optimizer 'SGD'  
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_squeezenetcustom_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
SqueezeNet (SqueezeNet)                  [128, 3, 224, 224]   [128, 490]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 96, 224, 224]  2,688                True
├─BatchNorm2d (bn1)                      [128, 96, 224, 224]  [128, 96, 224, 224]  192                  True
├─ReLU (relu)                            [128, 96, 224, 224]  [128, 96, 224, 224]  --                   --
├─MaxPool2d (maxpool1)                   [128, 96, 224, 224]  [128, 96, 112, 112]  --                   --
├─fire (fire2)                           [128, 96, 112, 112]  [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 96, 112, 112]  [128, 16, 112, 112]  1,552                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire3)                           [128, 128, 112, 112] [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 16, 112, 112]  2,064                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire4)                           [128, 128, 112, 112] [128, 256, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 32, 112, 112]  4,128                True
│    └─BatchNorm2d (bn1)                 [128, 32, 112, 112]  [128, 32, 112, 112]  64                   True
│    └─ReLU (relu1)                      [128, 32, 112, 112]  [128, 32, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 32, 112, 112]  [128, 128, 112, 112] 4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─Conv2d (conv3)                    [128, 32, 112, 112]  [128, 128, 112, 112] 36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─ReLU (relu2)                      [128, 256, 112, 112] [128, 256, 112, 112] --                   --
├─MaxPool2d (maxpool2)                   [128, 256, 112, 112] [128, 256, 56, 56]   --                   --
├─fire (fire5)                           [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 32, 56, 56]    8,224                True
│    └─BatchNorm2d (bn1)                 [128, 32, 56, 56]    [128, 32, 56, 56]    64                   True
│    └─ReLU (relu1)                      [128, 32, 56, 56]    [128, 32, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 32, 56, 56]    [128, 128, 56, 56]   4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─Conv2d (conv3)                    [128, 32, 56, 56]    [128, 128, 56, 56]   36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─ReLU (relu2)                      [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─fire (fire6)                           [128, 256, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 48, 56, 56]    12,336               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire7)                           [128, 384, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 48, 56, 56]    18,480               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire8)                           [128, 384, 56, 56]   [128, 512, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 64, 56, 56]    24,640               True
│    └─BatchNorm2d (bn1)                 [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    └─ReLU (relu1)                      [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 56, 56]    [128, 256, 56, 56]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 56, 56]    [128, 256, 56, 56]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─ReLU (relu2)                      [128, 512, 56, 56]   [128, 512, 56, 56]   --                   --
├─MaxPool2d (maxpool3)                   [128, 512, 56, 56]   [128, 512, 28, 28]   --                   --
├─fire (fire9)                           [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    └─Conv2d (conv1)                    [128, 512, 28, 28]   [128, 64, 28, 28]    32,832               True
│    └─BatchNorm2d (bn1)                 [128, 64, 28, 28]    [128, 64, 28, 28]    128                  True
│    └─ReLU (relu1)                      [128, 64, 28, 28]    [128, 64, 28, 28]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 28, 28]    [128, 256, 28, 28]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 28, 28]    [128, 256, 28, 28]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─ReLU (relu2)                      [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Conv2d (conv2)                         [128, 512, 28, 28]   [128, 10, 28, 28]    5,130                True
├─AvgPool2d (avg_pool)                   [128, 10, 28, 28]    [128, 10, 7, 7]      --                   --
========================================================================================================================
Total params: 734,986
Trainable params: 734,986
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 331.85
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 36693.51
Params size (MB): 2.94
Estimated Total Size (MB): 36773.52
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.431 ( 0.431)    Data  0.080 ( 0.080)    Loss 2.3146e+00 (2.3146e+00)    Acc@1  11.72 ( 11.72)   Acc@5  53.91 ( 53.91)
STAGE:2024-03-25 22:30:08 21424:21668 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:30:08 21424:21668 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:30:08 21424:21668 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-25 22:30:10 21424:21668 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:30:11 21424:21668 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:30:11 21424:21668 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.046 ( 0.094)    Data  0.024 ( 0.069)    Loss 1.7090e+00 (1.9032e+00)    Acc@1  30.47 ( 29.73)   Acc@5  89.06 ( 82.12)
Epoch: [0][201/313]     Time  0.046 ( 0.070)    Data  0.026 ( 0.048)    Loss 1.4189e+00 (1.6985e+00)    Acc@1  45.31 ( 37.06)   Acc@5  92.19 ( 86.83)
Epoch: [0][301/313]     Time  0.043 ( 0.061)    Data  0.025 ( 0.040)    Loss 1.1992e+00 (1.5820e+00)    Acc@1  56.25 ( 41.50)   Acc@5  96.09 ( 89.06)
train Loss: 1.5704 Acc: 0.4194
Epoch: [0][  1/313]     Time  0.041 ( 0.061)    Data  0.034 ( 0.039)    Loss 1.4244e+00 (1.5699e+00)    Acc@1  51.56 ( 41.97)   Acc@5  90.62 ( 89.27)
val Loss: 1.4335 Acc: 0.4862

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.104 ( 0.104)    Data  0.075 ( 0.075)    Loss 1.2017e+00 (1.2017e+00)    Acc@1  51.56 ( 51.56)   Acc@5  94.53 ( 94.53)
Epoch: [1][101/313]     Time  0.045 ( 0.046)    Data  0.025 ( 0.026)    Loss 1.1737e+00 (1.1687e+00)    Acc@1  55.47 ( 57.73)   Acc@5  94.53 ( 95.48)
Epoch: [1][201/313]     Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.1666e+00 (1.1230e+00)    Acc@1  58.59 ( 59.52)   Acc@5  96.88 ( 95.71)
Epoch: [1][301/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.0788e+00 (1.0951e+00)    Acc@1  59.38 ( 60.58)   Acc@5  96.88 ( 95.97)
train Loss: 1.0896 Acc: 0.6077
Epoch: [1][  1/313]     Time  0.041 ( 0.045)    Data  0.035 ( 0.025)    Loss 1.1510e+00 (1.0898e+00)    Acc@1  61.72 ( 60.77)   Acc@5  93.75 ( 95.99)
val Loss: 1.1518 Acc: 0.6044

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.103 ( 0.103)    Data  0.074 ( 0.074)    Loss 1.0723e+00 (1.0723e+00)    Acc@1  60.16 ( 60.16)   Acc@5  96.09 ( 96.09)
Epoch: [2][101/313]     Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 9.6365e-01 (9.2523e-01)    Acc@1  68.75 ( 66.94)   Acc@5  95.31 ( 97.06)
Epoch: [2][201/313]     Time  0.040 ( 0.043)    Data  0.022 ( 0.024)    Loss 9.5912e-01 (9.0396e-01)    Acc@1  68.75 ( 67.88)   Acc@5  96.09 ( 97.28)
Epoch: [2][301/313]     Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 8.4438e-01 (8.8667e-01)    Acc@1  67.97 ( 68.51)   Acc@5 100.00 ( 97.33)
train Loss: 0.8842 Acc: 0.6858
Epoch: [2][  1/313]     Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 7.2108e-01 (8.8368e-01)    Acc@1  71.09 ( 68.59)   Acc@5 100.00 ( 97.38)
val Loss: 0.9960 Acc: 0.6607

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 8.2223e-01 (8.2223e-01)    Acc@1  70.31 ( 70.31)   Acc@5  97.66 ( 97.66)
Epoch: [3][101/313]     Time  0.047 ( 0.048)    Data  0.024 ( 0.026)    Loss 8.6267e-01 (7.5710e-01)    Acc@1  67.97 ( 73.72)   Acc@5  98.44 ( 98.21)
Epoch: [3][201/313]     Time  0.042 ( 0.046)    Data  0.023 ( 0.025)    Loss 8.6990e-01 (7.4918e-01)    Acc@1  70.31 ( 73.92)   Acc@5  97.66 ( 98.25)
Epoch: [3][301/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 8.0184e-01 (7.4502e-01)    Acc@1  71.09 ( 73.89)   Acc@5  98.44 ( 98.27)
train Loss: 0.7424 Acc: 0.7395
Epoch: [3][  1/313]     Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 9.9025e-01 (7.4321e-01)    Acc@1  71.09 ( 73.94)   Acc@5  95.31 ( 98.28)
val Loss: 0.8978 Acc: 0.6905

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.107 ( 0.107)    Data  0.075 ( 0.075)    Loss 7.2867e-01 (7.2867e-01)    Acc@1  71.88 ( 71.88)   Acc@5  99.22 ( 99.22)
Epoch: [4][101/313]     Time  0.044 ( 0.047)    Data  0.025 ( 0.026)    Loss 6.0137e-01 (6.4507e-01)    Acc@1  79.69 ( 77.10)   Acc@5  96.88 ( 98.79)
Epoch: [4][201/313]     Time  0.046 ( 0.047)    Data  0.026 ( 0.027)    Loss 6.0580e-01 (6.5411e-01)    Acc@1  79.69 ( 76.71)   Acc@5 100.00 ( 98.81)
Epoch: [4][301/313]     Time  0.051 ( 0.047)    Data  0.028 ( 0.026)    Loss 6.9391e-01 (6.5075e-01)    Acc@1  77.34 ( 77.01)   Acc@5  97.66 ( 98.76)
train Loss: 0.6489 Acc: 0.7705
Epoch: [4][  1/313]     Time  0.044 ( 0.047)    Data  0.036 ( 0.026)    Loss 9.8448e-01 (6.4999e-01)    Acc@1  64.06 ( 77.01)   Acc@5  97.66 ( 98.76)
val Loss: 0.8945 Acc: 0.6990

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.103 ( 0.103)    Data  0.071 ( 0.071)    Loss 5.1031e-01 (5.1031e-01)    Acc@1  82.03 ( 82.03)   Acc@5  99.22 ( 99.22)
Epoch: [5][101/313]     Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 6.7441e-01 (5.6132e-01)    Acc@1  79.69 ( 80.23)   Acc@5  98.44 ( 99.07)
Epoch: [5][201/313]     Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 5.8062e-01 (5.7298e-01)    Acc@1  82.03 ( 79.77)   Acc@5 100.00 ( 99.02)
Epoch: [5][301/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.9291e-01 (5.7316e-01)    Acc@1  85.16 ( 79.82)   Acc@5  99.22 ( 99.04)
train Loss: 0.5743 Acc: 0.7981
Epoch: [5][  1/313]     Time  0.043 ( 0.043)    Data  0.036 ( 0.024)    Loss 7.8266e-01 (5.7500e-01)    Acc@1  73.44 ( 79.79)   Acc@5  98.44 ( 99.02)
val Loss: 0.8026 Acc: 0.7285

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 5.4226e-01 (5.4226e-01)    Acc@1  78.12 ( 78.12)   Acc@5  99.22 ( 99.22)
Epoch: [6][101/313]     Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 4.9246e-01 (4.7431e-01)    Acc@1  78.91 ( 83.50)   Acc@5  99.22 ( 99.35)
Epoch: [6][201/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 5.1160e-01 (4.9526e-01)    Acc@1  81.25 ( 82.81)   Acc@5  97.66 ( 99.28)
Epoch: [6][301/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 5.4193e-01 (5.0469e-01)    Acc@1  81.25 ( 82.40)   Acc@5 100.00 ( 99.21)
train Loss: 0.5057 Acc: 0.8236
Epoch: [6][  1/313]     Time  0.043 ( 0.043)    Data  0.035 ( 0.024)    Loss 7.3139e-01 (5.0639e-01)    Acc@1  75.00 ( 82.34)   Acc@5  98.44 ( 99.21)
val Loss: 0.8167 Acc: 0.7323

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 4.7613e-01 (4.7613e-01)    Acc@1  80.47 ( 80.47)   Acc@5  99.22 ( 99.22)
Epoch: [7][101/313]     Time  0.041 ( 0.044)    Data  0.022 ( 0.024)    Loss 4.4083e-01 (4.2231e-01)    Acc@1  85.94 ( 85.19)   Acc@5  99.22 ( 99.54)
Epoch: [7][201/313]     Time  0.041 ( 0.043)    Data  0.022 ( 0.024)    Loss 4.1978e-01 (4.4343e-01)    Acc@1  88.28 ( 84.61)   Acc@5 100.00 ( 99.46)
Epoch: [7][301/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 5.5655e-01 (4.6252e-01)    Acc@1  81.25 ( 83.94)   Acc@5  99.22 ( 99.40)
train Loss: 0.4639 Acc: 0.8387
Epoch: [7][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 6.3785e-01 (4.6444e-01)    Acc@1  78.12 ( 83.85)   Acc@5  97.66 ( 99.37)
val Loss: 0.7129 Acc: 0.7627

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 4.7146e-01 (4.7146e-01)    Acc@1  80.47 ( 80.47)   Acc@5  99.22 ( 99.22)
Epoch: [8][101/313]     Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 4.2555e-01 (3.9266e-01)    Acc@1  88.28 ( 86.15)   Acc@5  99.22 ( 99.59)
Epoch: [8][201/313]     Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 2.8748e-01 (3.9862e-01)    Acc@1  89.84 ( 85.92)   Acc@5 100.00 ( 99.58)
Epoch: [8][301/313]     Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 4.9062e-01 (4.1183e-01)    Acc@1  84.38 ( 85.50)   Acc@5  98.44 ( 99.55)
train Loss: 0.4150 Acc: 0.8541
Epoch: [8][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 6.8275e-01 (4.1587e-01)    Acc@1  75.78 ( 85.37)   Acc@5 100.00 ( 99.54)
val Loss: 0.8440 Acc: 0.7152

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 3.9666e-01 (3.9666e-01)    Acc@1  82.03 ( 82.03)   Acc@5 100.00 (100.00)
Epoch: [9][101/313]     Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.4536e-01 (3.5234e-01)    Acc@1  92.97 ( 87.80)   Acc@5  99.22 ( 99.69)
Epoch: [9][201/313]     Time  0.044 ( 0.043)    Data  0.024 ( 0.024)    Loss 3.4386e-01 (3.7677e-01)    Acc@1  88.28 ( 86.84)   Acc@5  98.44 ( 99.64)
Epoch: [9][301/313]     Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 4.2694e-01 (3.8673e-01)    Acc@1  85.16 ( 86.37)   Acc@5 100.00 ( 99.62)
train Loss: 0.3863 Acc: 0.8640
Epoch: [9][  1/313]     Time  0.043 ( 0.043)    Data  0.036 ( 0.024)    Loss 5.6339e-01 (3.8690e-01)    Acc@1  78.12 ( 86.38)   Acc@5  98.44 ( 99.62)
val Loss: 0.7158 Acc: 0.7738

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 2.8265e-01 (2.8265e-01)    Acc@1  90.62 ( 90.62)   Acc@5 100.00 (100.00)
Epoch: [10][101/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.0802e-01 (3.0139e-01)    Acc@1  85.16 ( 89.61)   Acc@5  99.22 ( 99.71)
Epoch: [10][201/313]    Time  0.043 ( 0.043)    Data  0.025 ( 0.024)    Loss 4.2946e-01 (3.3443e-01)    Acc@1  82.03 ( 88.29)   Acc@5 100.00 ( 99.67)
Epoch: [10][301/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 3.8443e-01 (3.4993e-01)    Acc@1  89.84 ( 87.65)   Acc@5  99.22 ( 99.66)
train Loss: 0.3503 Acc: 0.8763
Epoch: [10][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 7.7905e-01 (3.5165e-01)    Acc@1  74.22 ( 87.59)   Acc@5  99.22 ( 99.67)
val Loss: 0.7797 Acc: 0.7554

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 2.4357e-01 (2.4357e-01)    Acc@1  91.41 ( 91.41)   Acc@5 100.00 (100.00)
Epoch: [11][101/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.024)    Loss 2.7346e-01 (2.7639e-01)    Acc@1  89.06 ( 90.10)   Acc@5  99.22 ( 99.79)
Epoch: [11][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.9197e-01 (2.9300e-01)    Acc@1  89.84 ( 89.51)   Acc@5 100.00 ( 99.77)
Epoch: [11][301/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.6473e-01 (3.1049e-01)    Acc@1  87.50 ( 88.94)   Acc@5 100.00 ( 99.73)
train Loss: 0.3119 Acc: 0.8889
Epoch: [11][  1/313]    Time  0.043 ( 0.043)    Data  0.037 ( 0.024)    Loss 7.2664e-01 (3.1320e-01)    Acc@1  78.91 ( 88.85)   Acc@5  98.44 ( 99.71)
val Loss: 0.8784 Acc: 0.7341

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.113 ( 0.113)    Data  0.083 ( 0.083)    Loss 2.5348e-01 (2.5348e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.024)    Loss 1.7558e-01 (2.5022e-01)    Acc@1  92.97 ( 91.42)   Acc@5 100.00 ( 99.91)
Epoch: [12][201/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 3.3962e-01 (2.7383e-01)    Acc@1  88.28 ( 90.40)   Acc@5 100.00 ( 99.86)
Epoch: [12][301/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 3.3305e-01 (2.9611e-01)    Acc@1  87.50 ( 89.59)   Acc@5 100.00 ( 99.80)
train Loss: 0.2992 Acc: 0.8949
Epoch: [12][  1/313]    Time  0.044 ( 0.044)    Data  0.036 ( 0.024)    Loss 9.0626e-01 (3.0111e-01)    Acc@1  74.22 ( 89.44)   Acc@5  97.66 ( 99.79)
val Loss: 0.9112 Acc: 0.7269

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 2.5063e-01 (2.5063e-01)    Acc@1  92.97 ( 92.97)   Acc@5 100.00 (100.00)
Epoch: [13][101/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.024)    Loss 2.9175e-01 (2.2082e-01)    Acc@1  89.84 ( 92.26)   Acc@5 100.00 ( 99.90)
Epoch: [13][201/313]    Time  0.045 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.8571e-01 (2.4608e-01)    Acc@1  93.75 ( 91.32)   Acc@5 100.00 ( 99.87)
Epoch: [13][301/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 4.2311e-01 (2.7074e-01)    Acc@1  82.81 ( 90.34)   Acc@5  98.44 ( 99.83)
train Loss: 0.2723 Acc: 0.9027
Epoch: [13][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 7.1806e-01 (2.7372e-01)    Acc@1  78.12 ( 90.23)   Acc@5  98.44 ( 99.82)
val Loss: 0.8897 Acc: 0.7450

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 3.3664e-01 (3.3664e-01)    Acc@1  85.94 ( 85.94)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.7988e-01 (2.1740e-01)    Acc@1  95.31 ( 92.19)   Acc@5 100.00 ( 99.92)
Epoch: [14][201/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 3.1241e-01 (2.3230e-01)    Acc@1  88.28 ( 91.74)   Acc@5 100.00 ( 99.89)
Epoch: [14][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.4113e-01 (2.4800e-01)    Acc@1  87.50 ( 91.15)   Acc@5 100.00 ( 99.88)
train Loss: 0.2495 Acc: 0.9112
Epoch: [14][  1/313]    Time  0.043 ( 0.043)    Data  0.036 ( 0.024)    Loss 8.4552e-01 (2.5142e-01)    Acc@1  80.47 ( 91.09)   Acc@5  96.88 ( 99.87)
val Loss: 0.8745 Acc: 0.7520

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 2.6044e-01 (2.6044e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.1245e-01 (1.9652e-01)    Acc@1  94.53 ( 93.02)   Acc@5 100.00 ( 99.92)
Epoch: [15][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 4.4768e-01 (2.2129e-01)    Acc@1  84.38 ( 92.12)   Acc@5 100.00 ( 99.92)
Epoch: [15][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.5231e-01 (2.4160e-01)    Acc@1  94.53 ( 91.45)   Acc@5 100.00 ( 99.89)
train Loss: 0.2431 Acc: 0.9138
Epoch: [15][  1/313]    Time  0.047 ( 0.043)    Data  0.041 ( 0.024)    Loss 1.1157e+00 (2.4592e-01)    Acc@1  68.75 ( 91.31)   Acc@5  97.66 ( 99.89)
val Loss: 1.0375 Acc: 0.7169

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.106 ( 0.106)    Data  0.073 ( 0.073)    Loss 1.6448e-01 (1.6448e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.5409e-01 (1.8934e-01)    Acc@1  96.88 ( 93.34)   Acc@5 100.00 ( 99.95)
Epoch: [16][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.2700e-01 (1.9855e-01)    Acc@1  95.31 ( 93.03)   Acc@5 100.00 ( 99.93)
Epoch: [16][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 5.2077e-01 (2.1691e-01)    Acc@1  85.16 ( 92.32)   Acc@5  98.44 ( 99.90)
train Loss: 0.2197 Acc: 0.9222
Epoch: [16][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 1.0836e+00 (2.2249e-01)    Acc@1  70.31 ( 92.15)   Acc@5  99.22 ( 99.89)
val Loss: 1.1069 Acc: 0.7098

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.110 ( 0.110)    Data  0.078 ( 0.078)    Loss 1.6819e-01 (1.6819e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.4987e-01 (1.7188e-01)    Acc@1  95.31 ( 94.02)   Acc@5 100.00 ( 99.97)
Epoch: [17][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 2.4033e-01 (1.8947e-01)    Acc@1  92.19 ( 93.35)   Acc@5 100.00 ( 99.96)
Epoch: [17][301/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 3.1005e-01 (2.0309e-01)    Acc@1  91.41 ( 92.84)   Acc@5  98.44 ( 99.92)
train Loss: 0.2037 Acc: 0.9282
Epoch: [17][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.3944e+00 (2.0750e-01)    Acc@1  67.19 ( 92.74)   Acc@5  98.44 ( 99.92)
val Loss: 1.2346 Acc: 0.7141

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 2.4224e-01 (2.4224e-01)    Acc@1  89.84 ( 89.84)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.024)    Loss 1.6807e-01 (1.6384e-01)    Acc@1  92.97 ( 94.33)   Acc@5 100.00 ( 99.95)
Epoch: [18][201/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 2.5758e-01 (1.7716e-01)    Acc@1  91.41 ( 93.76)   Acc@5 100.00 ( 99.93)
Epoch: [18][301/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.7451e-01 (1.9298e-01)    Acc@1  88.28 ( 93.19)   Acc@5 100.00 ( 99.93)
train Loss: 0.1942 Acc: 0.9313
Epoch: [18][  1/313]    Time  0.044 ( 0.044)    Data  0.036 ( 0.024)    Loss 6.4299e-01 (1.9559e-01)    Acc@1  78.91 ( 93.08)   Acc@5 100.00 ( 99.93)
val Loss: 0.7575 Acc: 0.7873

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.102 ( 0.102)    Data  0.074 ( 0.074)    Loss 1.8267e-01 (1.8267e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.049 ( 0.048)    Data  0.029 ( 0.026)    Loss 2.0543e-01 (1.5469e-01)    Acc@1  90.62 ( 94.55)   Acc@5 100.00 ( 99.98)
Epoch: [19][201/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.6924e-01 (1.6986e-01)    Acc@1  93.75 ( 93.99)   Acc@5 100.00 ( 99.97)
Epoch: [19][301/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.025)    Loss 2.3847e-01 (1.8838e-01)    Acc@1  88.28 ( 93.31)   Acc@5 100.00 ( 99.95)
train Loss: 0.1899 Acc: 0.9325
Epoch: [19][  1/313]    Time  0.042 ( 0.044)    Data  0.036 ( 0.025)    Loss 9.6274e-01 (1.9232e-01)    Acc@1  78.91 ( 93.21)   Acc@5  97.66 ( 99.94)
val Loss: 1.1316 Acc: 0.7304

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 8.8038e-02 (8.8038e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.041 ( 0.044)    Data  0.022 ( 0.024)    Loss 9.3863e-02 (1.3988e-01)    Acc@1  96.88 ( 95.08)   Acc@5 100.00 ( 99.99)
Epoch: [20][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 2.5792e-01 (1.5405e-01)    Acc@1  91.41 ( 94.57)   Acc@5 100.00 ( 99.97)
Epoch: [20][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.8799e-01 (1.7201e-01)    Acc@1  93.75 ( 93.92)   Acc@5 100.00 ( 99.95)
train Loss: 0.1758 Acc: 0.9377
Epoch: [20][  1/313]    Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 1.3411e+00 (1.7948e-01)    Acc@1  67.19 ( 93.68)   Acc@5  97.66 ( 99.94)
val Loss: 1.1486 Acc: 0.7020

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 2.3979e-01 (2.3979e-01)    Acc@1  90.62 ( 90.62)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.3281e-01 (1.6002e-01)    Acc@1  96.88 ( 94.45)   Acc@5 100.00 ( 99.98)
Epoch: [21][201/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.4193e-01 (1.5290e-01)    Acc@1  96.09 ( 94.64)   Acc@5 100.00 ( 99.98)
Epoch: [21][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 2.2781e-01 (1.6498e-01)    Acc@1  89.84 ( 94.18)   Acc@5 100.00 ( 99.98)
train Loss: 0.1673 Acc: 0.9409
Epoch: [21][  1/313]    Time  0.041 ( 0.043)    Data  0.034 ( 0.024)    Loss 1.2166e+00 (1.7067e-01)    Acc@1  71.88 ( 94.01)   Acc@5  97.66 ( 99.98)
val Loss: 1.0637 Acc: 0.7459

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 1.3834e-01 (1.3834e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.3817e-01 (1.5174e-01)    Acc@1  95.31 ( 94.80)   Acc@5 100.00 ( 99.97)
Epoch: [22][201/313]    Time  0.047 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.7749e-01 (1.6455e-01)    Acc@1  96.09 ( 94.19)   Acc@5  99.22 ( 99.97)
Epoch: [22][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.9192e-01 (1.7245e-01)    Acc@1  92.19 ( 93.87)   Acc@5 100.00 ( 99.96)
train Loss: 0.1725 Acc: 0.9385
Epoch: [22][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.2564e+00 (1.7597e-01)    Acc@1  66.41 ( 93.76)   Acc@5  99.22 ( 99.96)
val Loss: 1.3601 Acc: 0.6862

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.115 ( 0.115)    Data  0.085 ( 0.085)    Loss 1.2195e-01 (1.2195e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 7.4930e-02 (1.1607e-01)    Acc@1  97.66 ( 95.95)   Acc@5 100.00 ( 99.98)
Epoch: [23][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.0397e-01 (1.3259e-01)    Acc@1  92.19 ( 95.37)   Acc@5  99.22 ( 99.97)
Epoch: [23][301/313]    Time  0.044 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.4390e-01 (1.4861e-01)    Acc@1  94.53 ( 94.79)   Acc@5 100.00 ( 99.96)
train Loss: 0.1496 Acc: 0.9474
Epoch: [23][  1/313]    Time  0.044 ( 0.043)    Data  0.034 ( 0.024)    Loss 9.5498e-01 (1.5220e-01)    Acc@1  69.53 ( 94.66)   Acc@5  99.22 ( 99.96)
val Loss: 1.1416 Acc: 0.7376

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 1.1273e-01 (1.1273e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.4422e-01 (1.1183e-01)    Acc@1  94.53 ( 96.07)   Acc@5 100.00 ( 99.98)
Epoch: [24][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.2018e-01 (1.2808e-01)    Acc@1  96.88 ( 95.48)   Acc@5 100.00 ( 99.98)
Epoch: [24][301/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.1493e-01 (1.4160e-01)    Acc@1  92.97 ( 95.06)   Acc@5 100.00 ( 99.97)
train Loss: 0.1420 Acc: 0.9505
Epoch: [24][  1/313]    Time  0.044 ( 0.043)    Data  0.036 ( 0.024)    Loss 6.7856e-01 (1.4371e-01)    Acc@1  81.25 ( 95.00)   Acc@5  98.44 ( 99.97)
val Loss: 0.8238 Acc: 0.7861

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 1.1478e-01 (1.1478e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.6120e-01 (1.1723e-01)    Acc@1  92.97 ( 95.92)   Acc@5  99.22 ( 99.98)
Epoch: [25][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 9.1989e-02 (1.2844e-01)    Acc@1  96.88 ( 95.49)   Acc@5 100.00 ( 99.98)
Epoch: [25][301/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 2.8575e-01 (1.3943e-01)    Acc@1  92.19 ( 95.08)   Acc@5 100.00 ( 99.98)
train Loss: 0.1412 Acc: 0.9501
Epoch: [25][  1/313]    Time  0.045 ( 0.043)    Data  0.037 ( 0.024)    Loss 8.9273e-01 (1.4360e-01)    Acc@1  78.12 ( 94.96)   Acc@5  96.88 ( 99.97)
val Loss: 0.8945 Acc: 0.7676

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 1.7887e-01 (1.7887e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.041 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.1601e-01 (1.4679e-01)    Acc@1  96.09 ( 94.84)   Acc@5 100.00 ( 99.98)
Epoch: [26][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.1487e-01 (1.3733e-01)    Acc@1  91.41 ( 95.30)   Acc@5 100.00 ( 99.99)
Epoch: [26][301/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.024)    Loss 1.2277e-01 (1.4369e-01)    Acc@1  95.31 ( 94.98)   Acc@5 100.00 ( 99.99)
train Loss: 0.1453 Acc: 0.9490
Epoch: [26][  1/313]    Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 9.6633e-01 (1.4796e-01)    Acc@1  76.56 ( 94.84)   Acc@5 100.00 ( 99.99)
val Loss: 0.9721 Acc: 0.7522

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.100 ( 0.100)    Data  0.071 ( 0.071)    Loss 1.1888e-01 (1.1888e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.2887e-01 (1.0796e-01)    Acc@1  92.97 ( 96.23)   Acc@5 100.00 (100.00)
Epoch: [27][201/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.8901e-01 (1.2025e-01)    Acc@1  92.19 ( 95.70)   Acc@5 100.00 ( 99.99)
Epoch: [27][301/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.5310e-01 (1.3316e-01)    Acc@1  92.97 ( 95.23)   Acc@5 100.00 ( 99.98)
train Loss: 0.1347 Acc: 0.9516
Epoch: [27][  1/313]    Time  0.042 ( 0.044)    Data  0.035 ( 0.024)    Loss 1.1216e+00 (1.3790e-01)    Acc@1  72.66 ( 95.08)   Acc@5  98.44 ( 99.98)
val Loss: 1.1442 Acc: 0.7370

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 1.4503e-01 (1.4503e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 1.6904e-01 (1.1163e-01)    Acc@1  96.09 ( 95.99)   Acc@5 100.00 ( 99.98)
Epoch: [28][201/313]    Time  0.041 ( 0.044)    Data  0.022 ( 0.024)    Loss 1.9337e-01 (1.2633e-01)    Acc@1  95.31 ( 95.46)   Acc@5 100.00 ( 99.98)
Epoch: [28][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.7706e-01 (1.4101e-01)    Acc@1  94.53 ( 94.94)   Acc@5 100.00 ( 99.97)
train Loss: 0.1414 Acc: 0.9492
Epoch: [28][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 8.2695e-01 (1.4358e-01)    Acc@1  75.78 ( 94.86)   Acc@5  98.44 ( 99.97)
val Loss: 0.9861 Acc: 0.7486

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 1.1418e-01 (1.1418e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 9.0573e-02 (1.0436e-01)    Acc@1  96.09 ( 96.24)   Acc@5 100.00 ( 99.98)
Epoch: [29][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 4.4886e-02 (9.7749e-02)    Acc@1  98.44 ( 96.57)   Acc@5 100.00 ( 99.99)
Epoch: [29][301/313]    Time  0.041 ( 0.043)    Data  0.022 ( 0.024)    Loss 1.5981e-01 (1.1958e-01)    Acc@1  93.75 ( 95.79)   Acc@5 100.00 ( 99.98)
train Loss: 0.1215 Acc: 0.9572
Epoch: [29][  1/313]    Time  0.044 ( 0.043)    Data  0.037 ( 0.024)    Loss 1.3714e+00 (1.2549e-01)    Acc@1  71.09 ( 95.64)   Acc@5  96.09 ( 99.97)
val Loss: 1.1325 Acc: 0.7413

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 1.2185e-01 (1.2185e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.041 ( 0.045)    Data  0.023 ( 0.025)    Loss 6.6731e-02 (7.8436e-02)    Acc@1  96.09 ( 97.33)   Acc@5 100.00 ( 99.99)
Epoch: [30][201/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.1865e-02 (6.1702e-02)    Acc@1 100.00 ( 98.04)   Acc@5 100.00 ( 99.99)
Epoch: [30][301/313]    Time  0.044 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.2044e-02 (5.2850e-02)    Acc@1 100.00 ( 98.38)   Acc@5 100.00 ( 99.99)
train Loss: 0.0519 Acc: 0.9842
Epoch: [30][  1/313]    Time  0.045 ( 0.043)    Data  0.037 ( 0.024)    Loss 7.6478e-01 (5.4191e-02)    Acc@1  83.59 ( 98.37)   Acc@5  98.44 ( 99.99)
val Loss: 0.6964 Acc: 0.8193

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 2.8120e-02 (2.8120e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.8069e-02 (1.8728e-02)    Acc@1 100.00 ( 99.79)   Acc@5 100.00 (100.00)
Epoch: [31][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.9970e-02 (1.7831e-02)    Acc@1  99.22 ( 99.79)   Acc@5 100.00 (100.00)
Epoch: [31][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.3220e-02 (1.7180e-02)    Acc@1  99.22 ( 99.78)   Acc@5 100.00 (100.00)
train Loss: 0.0170 Acc: 0.9979
Epoch: [31][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 3.6943e-01 (1.8129e-02)    Acc@1  85.94 ( 99.74)   Acc@5  99.22 (100.00)
val Loss: 0.7053 Acc: 0.8197

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 1.2654e-02 (1.2654e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.045 ( 0.047)    Data  0.025 ( 0.026)    Loss 7.5043e-03 (1.0868e-02)    Acc@1 100.00 ( 99.96)   Acc@5 100.00 (100.00)
Epoch: [32][201/313]    Time  0.050 ( 0.049)    Data  0.029 ( 0.027)    Loss 1.2503e-02 (1.0969e-02)    Acc@1 100.00 ( 99.95)   Acc@5 100.00 (100.00)
Epoch: [32][301/313]    Time  0.043 ( 0.049)    Data  0.024 ( 0.027)    Loss 1.2379e-02 (1.0763e-02)    Acc@1 100.00 ( 99.95)   Acc@5 100.00 (100.00)
train Loss: 0.0107 Acc: 0.9995
Epoch: [32][  1/313]    Time  0.064 ( 0.049)    Data  0.041 ( 0.027)    Loss 4.8972e-01 (1.2277e-02)    Acc@1  85.16 ( 99.90)   Acc@5 100.00 (100.00)
val Loss: 0.7126 Acc: 0.8200

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.093 ( 0.093)    Data  0.073 ( 0.073)    Loss 5.1003e-03 (5.1003e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.025)    Loss 6.4631e-03 (8.1064e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [33][201/313]    Time  0.044 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.5070e-02 (8.3995e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [33][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 7.1830e-03 (8.0583e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0081 Acc: 0.9999
Epoch: [33][  1/313]    Time  0.043 ( 0.043)    Data  0.037 ( 0.024)    Loss 5.1005e-01 (9.6739e-03)    Acc@1  89.84 ( 99.95)   Acc@5  97.66 ( 99.99)
val Loss: 0.7222 Acc: 0.8208

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 7.2610e-03 (7.2610e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.043 ( 0.045)    Data  0.023 ( 0.024)    Loss 3.2890e-03 (6.7387e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][201/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 4.2355e-03 (6.4174e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][301/313]    Time  0.048 ( 0.044)    Data  0.025 ( 0.024)    Loss 3.7251e-03 (6.5226e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0065 Acc: 0.9999
Epoch: [34][  1/313]    Time  0.044 ( 0.044)    Data  0.036 ( 0.024)    Loss 5.9185e-01 (8.3938e-03)    Acc@1  87.50 ( 99.95)   Acc@5  99.22 (100.00)
val Loss: 0.7318 Acc: 0.8221

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.105 ( 0.105)    Data  0.073 ( 0.073)    Loss 4.0222e-03 (4.0222e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 3.4730e-03 (5.8599e-03)    Acc@1 100.00 ( 99.97)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.6989e-03 (5.8965e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 6.5872e-03 (5.6673e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0057 Acc: 0.9999
Epoch: [35][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 1.1046e+00 (9.1981e-03)    Acc@1  77.34 ( 99.91)   Acc@5  98.44 (100.00)
val Loss: 0.7348 Acc: 0.8219

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 5.8845e-03 (5.8845e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 6.4330e-03 (4.7228e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.0897e-03 (4.7724e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 4.3461e-03 (4.8868e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0049 Acc: 0.9999
Epoch: [36][  1/313]    Time  0.046 ( 0.044)    Data  0.040 ( 0.024)    Loss 7.1375e-01 (7.1465e-03)    Acc@1  78.91 ( 99.93)   Acc@5  98.44 (100.00)
val Loss: 0.7453 Acc: 0.8217

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.094 ( 0.094)    Data  0.074 ( 0.074)    Loss 1.3780e-02 (1.3780e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.045 ( 0.050)    Data  0.025 ( 0.030)    Loss 1.4392e-03 (4.3397e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [37][201/313]    Time  0.047 ( 0.048)    Data  0.024 ( 0.027)    Loss 5.4874e-03 (4.2186e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [37][301/313]    Time  0.044 ( 0.048)    Data  0.025 ( 0.027)    Loss 5.5129e-03 (4.2535e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0043 Acc: 1.0000
Epoch: [37][  1/313]    Time  0.044 ( 0.048)    Data  0.037 ( 0.027)    Loss 5.1919e-01 (5.9219e-03)    Acc@1  89.84 ( 99.96)   Acc@5  99.22 (100.00)
val Loss: 0.7498 Acc: 0.8216

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.103 ( 0.103)    Data  0.074 ( 0.074)    Loss 2.7525e-03 (2.7525e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.043 ( 0.046)    Data  0.025 ( 0.025)    Loss 4.0061e-03 (3.9525e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.042 ( 0.046)    Data  0.023 ( 0.025)    Loss 3.5926e-03 (3.9761e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [38][301/313]    Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.7771e-03 (4.0369e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0041 Acc: 1.0000
Epoch: [38][  1/313]    Time  0.046 ( 0.045)    Data  0.037 ( 0.025)    Loss 6.2128e-01 (6.0792e-03)    Acc@1  81.25 ( 99.94)   Acc@5 100.00 (100.00)
val Loss: 0.7502 Acc: 0.8223

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.108 ( 0.108)    Data  0.076 ( 0.076)    Loss 2.6031e-03 (2.6031e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.025)    Loss 3.8395e-03 (3.3615e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][201/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 5.8438e-03 (3.4050e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.2191e-03 (3.4286e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0035 Acc: 1.0000
Epoch: [39][  1/313]    Time  0.044 ( 0.045)    Data  0.038 ( 0.025)    Loss 1.0438e+00 (6.7862e-03)    Acc@1  78.12 ( 99.93)   Acc@5  98.44 (100.00)
val Loss: 0.7573 Acc: 0.8218

Training complete in 10m 57s
Best val Acc: 0.822300
Test Loss: 0.150034

Test Accuracy of airplane: 83% (830/996)
Test Accuracy of automobile: 91% (931/1022)
Test Accuracy of  bird: 74% (769/1029)
Test Accuracy of   cat: 66% (668/1001)
Test Accuracy of  deer: 80% (819/1019)
Test Accuracy of   dog: 72% (719/996)
Test Accuracy of  frog: 86% (814/943)
Test Accuracy of horse: 84% (854/1005)
Test Accuracy of  ship: 92% (914/993)
Test Accuracy of truck: 90% (905/996)

Test Accuracy (Overall): 82% (8223/10000)