PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\MultiModalClassifier\data\" --model_name 'squeezenetcustom' --learningratename 'ExponentialLR' --optimizer 'SGD'

2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_squeezenetcustom_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
SqueezeNet (SqueezeNet)                  [128, 3, 224, 224]   [128, 490]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 96, 224, 224]  2,688                True
├─BatchNorm2d (bn1)                      [128, 96, 224, 224]  [128, 96, 224, 224]  192                  True
├─ReLU (relu)                            [128, 96, 224, 224]  [128, 96, 224, 224]  --                   --
├─MaxPool2d (maxpool1)                   [128, 96, 224, 224]  [128, 96, 112, 112]  --                   --
├─fire (fire2)                           [128, 96, 112, 112]  [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 96, 112, 112]  [128, 16, 112, 112]  1,552                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire3)                           [128, 128, 112, 112] [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 16, 112, 112]  2,064                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire4)                           [128, 128, 112, 112] [128, 256, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 32, 112, 112]  4,128                True
│    └─BatchNorm2d (bn1)                 [128, 32, 112, 112]  [128, 32, 112, 112]  64                   True
│    └─ReLU (relu1)                      [128, 32, 112, 112]  [128, 32, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 32, 112, 112]  [128, 128, 112, 112] 4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─Conv2d (conv3)                    [128, 32, 112, 112]  [128, 128, 112, 112] 36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─ReLU (relu2)                      [128, 256, 112, 112] [128, 256, 112, 112] --                   --
├─MaxPool2d (maxpool2)                   [128, 256, 112, 112] [128, 256, 56, 56]   --                   --
├─fire (fire5)                           [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 32, 56, 56]    8,224                True
│    └─BatchNorm2d (bn1)                 [128, 32, 56, 56]    [128, 32, 56, 56]    64                   True
│    └─ReLU (relu1)                      [128, 32, 56, 56]    [128, 32, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 32, 56, 56]    [128, 128, 56, 56]   4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─Conv2d (conv3)                    [128, 32, 56, 56]    [128, 128, 56, 56]   36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─ReLU (relu2)                      [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─fire (fire6)                           [128, 256, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 48, 56, 56]    12,336               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire7)                           [128, 384, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 48, 56, 56]    18,480               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire8)                           [128, 384, 56, 56]   [128, 512, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 64, 56, 56]    24,640               True
│    └─BatchNorm2d (bn1)                 [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    └─ReLU (relu1)                      [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 56, 56]    [128, 256, 56, 56]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 56, 56]    [128, 256, 56, 56]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─ReLU (relu2)                      [128, 512, 56, 56]   [128, 512, 56, 56]   --                   --
├─MaxPool2d (maxpool3)                   [128, 512, 56, 56]   [128, 512, 28, 28]   --                   --
├─fire (fire9)                           [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    └─Conv2d (conv1)                    [128, 512, 28, 28]   [128, 64, 28, 28]    32,832               True
│    └─BatchNorm2d (bn1)                 [128, 64, 28, 28]    [128, 64, 28, 28]    128                  True
│    └─ReLU (relu1)                      [128, 64, 28, 28]    [128, 64, 28, 28]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 28, 28]    [128, 256, 28, 28]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 28, 28]    [128, 256, 28, 28]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─ReLU (relu2)                      [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Conv2d (conv2)                         [128, 512, 28, 28]   [128, 10, 28, 28]    5,130                True
├─AvgPool2d (avg_pool)                   [128, 10, 28, 28]    [128, 10, 7, 7]      --                   --
========================================================================================================================
Total params: 734,986
Trainable params: 734,986
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 331.85
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 36693.51
Params size (MB): 2.94
Estimated Total Size (MB): 36773.52
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.469 ( 0.469)    Data  0.086 ( 0.086)    Loss 2.3569e+00 (2.3569e+00)    Acc@1  10.16 ( 10.16)   Acc@5  48.44 ( 48.44)
STAGE:2024-03-25 22:17:58 30528:15112 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:17:58 30528:15112 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:17:58 30528:15112 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-25 22:18:00 30528:15112 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:18:01 30528:15112 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:18:01 30528:15112 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.046 ( 0.095)    Data  0.026 ( 0.070)    Loss 1.5880e+00 (1.9035e+00)    Acc@1  33.59 ( 30.11)   Acc@5  94.53 ( 82.81)
Epoch: [0][201/313]     Time  0.044 ( 0.069)    Data  0.025 ( 0.047)    Loss 1.3214e+00 (1.7053e+00)    Acc@1  46.09 ( 37.54)   Acc@5  92.97 ( 87.11)
Epoch: [0][301/313]     Time  0.044 ( 0.061)    Data  0.024 ( 0.040)    Loss 1.3144e+00 (1.5836e+00)    Acc@1  51.56 ( 42.08)   Acc@5  94.53 ( 89.20)
train Loss: 1.5720 Acc: 0.4251
Epoch: [0][  1/313]     Time  0.044 ( 0.061)    Data  0.038 ( 0.039)    Loss 1.4604e+00 (1.5716e+00)    Acc@1  46.09 ( 42.52)   Acc@5  95.31 ( 89.42)
val Loss: 1.4999 Acc: 0.4886

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.110 ( 0.110)    Data  0.077 ( 0.077)    Loss 1.2484e+00 (1.2484e+00)    Acc@1  53.91 ( 53.91)   Acc@5  95.31 ( 95.31)
Epoch: [1][101/313]     Time  0.043 ( 0.046)    Data  0.023 ( 0.025)    Loss 1.2158e+00 (1.1959e+00)    Acc@1  56.25 ( 56.68)   Acc@5  91.41 ( 94.96)
Epoch: [1][201/313]     Time  0.045 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.1132e+00 (1.1462e+00)    Acc@1  64.06 ( 58.56)   Acc@5  96.88 ( 95.45)
Epoch: [1][301/313]     Time  0.046 ( 0.044)    Data  0.025 ( 0.025)    Loss 8.2357e-01 (1.1111e+00)    Acc@1  68.75 ( 60.17)   Acc@5  99.22 ( 95.75)
train Loss: 1.1066 Acc: 0.6034
Epoch: [1][  1/313]     Time  0.044 ( 0.044)    Data  0.038 ( 0.025)    Loss 1.0678e+00 (1.1065e+00)    Acc@1  59.38 ( 60.34)   Acc@5  96.88 ( 95.78)
val Loss: 1.2891 Acc: 0.5566

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.106 ( 0.106)    Data  0.076 ( 0.076)    Loss 1.0182e+00 (1.0182e+00)    Acc@1  61.72 ( 61.72)   Acc@5  97.66 ( 97.66)
Epoch: [2][101/313]     Time  0.044 ( 0.046)    Data  0.026 ( 0.025)    Loss 7.3150e-01 (9.2399e-01)    Acc@1  73.44 ( 67.23)   Acc@5  99.22 ( 96.94)
Epoch: [2][201/313]     Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 7.0478e-01 (9.0850e-01)    Acc@1  75.00 ( 67.90)   Acc@5  99.22 ( 97.19)
Epoch: [2][301/313]     Time  0.047 ( 0.045)    Data  0.024 ( 0.025)    Loss 7.9343e-01 (8.8947e-01)    Acc@1  69.53 ( 68.54)   Acc@5  96.88 ( 97.41)
train Loss: 0.8883 Acc: 0.6858
Epoch: [2][  1/313]     Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 9.5769e-01 (8.8852e-01)    Acc@1  60.94 ( 68.55)   Acc@5  99.22 ( 97.43)
val Loss: 1.0209 Acc: 0.6389

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 7.6096e-01 (7.6096e-01)    Acc@1  67.97 ( 67.97)   Acc@5  97.66 ( 97.66)
Epoch: [3][101/313]     Time  0.042 ( 0.046)    Data  0.024 ( 0.025)    Loss 7.6244e-01 (7.2853e-01)    Acc@1  74.22 ( 74.39)   Acc@5  99.22 ( 98.61)
Epoch: [3][201/313]     Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 6.2146e-01 (7.3931e-01)    Acc@1  77.34 ( 73.97)   Acc@5  98.44 ( 98.43)
Epoch: [3][301/313]     Time  0.044 ( 0.045)    Data  0.024 ( 0.025)    Loss 8.2575e-01 (7.3610e-01)    Acc@1  71.09 ( 74.19)   Acc@5  99.22 ( 98.41)
train Loss: 0.7333 Acc: 0.7427
Epoch: [3][  1/313]     Time  0.045 ( 0.045)    Data  0.038 ( 0.025)    Loss 9.3009e-01 (7.3397e-01)    Acc@1  67.97 ( 74.25)   Acc@5  96.88 ( 98.42)
val Loss: 0.8179 Acc: 0.7124

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.106 ( 0.106)    Data  0.073 ( 0.073)    Loss 6.3208e-01 (6.3208e-01)    Acc@1  78.12 ( 78.12)   Acc@5  98.44 ( 98.44)
Epoch: [4][101/313]     Time  0.044 ( 0.046)    Data  0.025 ( 0.025)    Loss 7.3624e-01 (6.2108e-01)    Acc@1  75.00 ( 77.97)   Acc@5  98.44 ( 98.82)
Epoch: [4][201/313]     Time  0.042 ( 0.045)    Data  0.024 ( 0.024)    Loss 8.1549e-01 (6.1827e-01)    Acc@1  71.88 ( 78.02)   Acc@5  97.66 ( 98.91)
Epoch: [4][301/313]     Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 6.9977e-01 (6.2303e-01)    Acc@1  75.78 ( 78.01)   Acc@5  96.88 ( 98.83)
train Loss: 0.6225 Acc: 0.7801
Epoch: [4][  1/313]     Time  0.045 ( 0.044)    Data  0.038 ( 0.024)    Loss 7.6852e-01 (6.2300e-01)    Acc@1  71.09 ( 77.98)   Acc@5  98.44 ( 98.83)
val Loss: 0.8072 Acc: 0.7222

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.104 ( 0.104)    Data  0.074 ( 0.074)    Loss 5.9300e-01 (5.9300e-01)    Acc@1  78.91 ( 78.91)   Acc@5 100.00 (100.00)
Epoch: [5][101/313]     Time  0.043 ( 0.046)    Data  0.023 ( 0.025)    Loss 4.7676e-01 (5.1258e-01)    Acc@1  84.38 ( 82.47)   Acc@5  99.22 ( 99.27)
Epoch: [5][201/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 5.4145e-01 (5.2376e-01)    Acc@1  77.34 ( 81.87)   Acc@5  99.22 ( 99.19)
Epoch: [5][301/313]     Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 4.4159e-01 (5.2807e-01)    Acc@1  85.94 ( 81.55)   Acc@5 100.00 ( 99.19)
train Loss: 0.5303 Acc: 0.8144
Epoch: [5][  1/313]     Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 8.6452e-01 (5.3133e-01)    Acc@1  71.09 ( 81.41)   Acc@5  96.88 ( 99.19)
val Loss: 0.7696 Acc: 0.7409

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 4.3541e-01 (4.3541e-01)    Acc@1  88.28 ( 88.28)   Acc@5 100.00 (100.00)
Epoch: [6][101/313]     Time  0.050 ( 0.046)    Data  0.026 ( 0.025)    Loss 4.2418e-01 (4.4268e-01)    Acc@1  83.59 ( 84.56)   Acc@5 100.00 ( 99.60)
Epoch: [6][201/313]     Time  0.044 ( 0.046)    Data  0.025 ( 0.026)    Loss 6.5585e-01 (4.5567e-01)    Acc@1  77.34 ( 83.92)   Acc@5  99.22 ( 99.49)
Epoch: [6][301/313]     Time  0.051 ( 0.047)    Data  0.028 ( 0.026)    Loss 5.3286e-01 (4.6026e-01)    Acc@1  79.69 ( 83.85)   Acc@5  97.66 ( 99.41)
train Loss: 0.4619 Acc: 0.8380
Epoch: [6][  1/313]     Time  0.043 ( 0.047)    Data  0.036 ( 0.026)    Loss 7.6256e-01 (4.6281e-01)    Acc@1  71.88 ( 83.76)   Acc@5  97.66 ( 99.40)
val Loss: 0.7821 Acc: 0.7412

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 3.6001e-01 (3.6001e-01)    Acc@1  87.50 ( 87.50)   Acc@5 100.00 (100.00)
Epoch: [7][101/313]     Time  0.041 ( 0.044)    Data  0.023 ( 0.025)    Loss 4.1976e-01 (3.5795e-01)    Acc@1  85.16 ( 87.48)   Acc@5 100.00 ( 99.73)
Epoch: [7][201/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.5583e-01 (3.7146e-01)    Acc@1  85.94 ( 86.85)   Acc@5 100.00 ( 99.68)
Epoch: [7][301/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.9705e-01 (3.7922e-01)    Acc@1  86.72 ( 86.66)   Acc@5 100.00 ( 99.63)
train Loss: 0.3798 Acc: 0.8664
Epoch: [7][  1/313]     Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 8.6434e-01 (3.8138e-01)    Acc@1  71.88 ( 86.59)   Acc@5  98.44 ( 99.64)
val Loss: 0.8074 Acc: 0.7400

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.101 ( 0.101)    Data  0.073 ( 0.073)    Loss 3.1955e-01 (3.1955e-01)    Acc@1  91.41 ( 91.41)   Acc@5 100.00 (100.00)
Epoch: [8][101/313]     Time  0.043 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.2257e-01 (2.8742e-01)    Acc@1  92.97 ( 90.23)   Acc@5  99.22 ( 99.79)
Epoch: [8][201/313]     Time  0.047 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.2104e-01 (2.9689e-01)    Acc@1  87.50 ( 89.65)   Acc@5  99.22 ( 99.75)
Epoch: [8][301/313]     Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.1334e-01 (3.1201e-01)    Acc@1  88.28 ( 89.06)   Acc@5 100.00 ( 99.72)
train Loss: 0.3130 Acc: 0.8903
Epoch: [8][  1/313]     Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 8.9244e-01 (3.1486e-01)    Acc@1  78.12 ( 89.00)   Acc@5  98.44 ( 99.72)
val Loss: 0.8246 Acc: 0.7594

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 2.5427e-01 (2.5427e-01)    Acc@1  91.41 ( 91.41)   Acc@5  99.22 ( 99.22)
Epoch: [9][101/313]     Time  0.042 ( 0.045)    Data  0.024 ( 0.024)    Loss 1.4370e-01 (2.2496e-01)    Acc@1  95.31 ( 92.50)   Acc@5 100.00 ( 99.86)
Epoch: [9][201/313]     Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.4210e-01 (2.3552e-01)    Acc@1  87.50 ( 91.94)   Acc@5 100.00 ( 99.88)
Epoch: [9][301/313]     Time  0.043 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.2605e-01 (2.4923e-01)    Acc@1  90.62 ( 91.39)   Acc@5 100.00 ( 99.85)
train Loss: 0.2511 Acc: 0.9131
Epoch: [9][  1/313]     Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 7.9867e-01 (2.5288e-01)    Acc@1  75.78 ( 91.26)   Acc@5  97.66 ( 99.84)
val Loss: 0.9456 Acc: 0.7259

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 1.6738e-01 (1.6738e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [10][101/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.6846e-01 (1.7591e-01)    Acc@1  94.53 ( 94.10)   Acc@5 100.00 ( 99.92)
Epoch: [10][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.025)    Loss 3.3784e-01 (1.7717e-01)    Acc@1  87.50 ( 93.89)   Acc@5  99.22 ( 99.93)
Epoch: [10][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.4933e-01 (1.9008e-01)    Acc@1  91.41 ( 93.41)   Acc@5 100.00 ( 99.92)
train Loss: 0.1918 Acc: 0.9334
Epoch: [10][  1/313]    Time  0.046 ( 0.044)    Data  0.036 ( 0.024)    Loss 7.6104e-01 (1.9363e-01)    Acc@1  75.78 ( 93.29)   Acc@5  99.22 ( 99.92)
val Loss: 0.8195 Acc: 0.7666

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.104 ( 0.104)    Data  0.072 ( 0.072)    Loss 1.5315e-01 (1.5315e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [11][101/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.026)    Loss 8.4920e-02 (1.2705e-01)    Acc@1  98.44 ( 95.95)   Acc@5 100.00 ( 99.98)
Epoch: [11][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.7330e-01 (1.3284e-01)    Acc@1  93.75 ( 95.57)   Acc@5 100.00 ( 99.96)
Epoch: [11][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.025)    Loss 1.9778e-01 (1.3849e-01)    Acc@1  92.97 ( 95.37)   Acc@5 100.00 ( 99.96)
train Loss: 0.1394 Acc: 0.9534
Epoch: [11][  1/313]    Time  0.042 ( 0.044)    Data  0.035 ( 0.025)    Loss 1.1148e+00 (1.4254e-01)    Acc@1  73.44 ( 95.27)   Acc@5  96.88 ( 99.95)
val Loss: 0.7859 Acc: 0.7834

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 1.0612e-01 (1.0612e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 6.5618e-02 (9.4085e-02)    Acc@1  97.66 ( 97.01)   Acc@5 100.00 ( 99.98)
Epoch: [12][201/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.3929e-01 (8.7604e-02)    Acc@1  94.53 ( 97.24)   Acc@5 100.00 ( 99.99)
Epoch: [12][301/313]    Time  0.046 ( 0.044)    Data  0.023 ( 0.024)    Loss 8.8795e-02 (9.3210e-02)    Acc@1  96.09 ( 97.04)   Acc@5 100.00 ( 99.98)
train Loss: 0.0939 Acc: 0.9702
Epoch: [12][  1/313]    Time  0.043 ( 0.044)    Data  0.037 ( 0.024)    Loss 8.7802e-01 (9.6420e-02)    Acc@1  76.56 ( 96.95)   Acc@5  99.22 ( 99.98)
val Loss: 1.0981 Acc: 0.7415

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.099 ( 0.099)    Data  0.071 ( 0.071)    Loss 8.2820e-02 (8.2820e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [13][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 4.8416e-02 (6.6253e-02)    Acc@1  99.22 ( 97.97)   Acc@5 100.00 (100.00)
Epoch: [13][201/313]    Time  0.045 ( 0.043)    Data  0.024 ( 0.024)    Loss 6.1990e-02 (5.9523e-02)    Acc@1  96.88 ( 98.24)   Acc@5 100.00 (100.00)
Epoch: [13][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 7.5120e-02 (5.9774e-02)    Acc@1  96.88 ( 98.20)   Acc@5 100.00 ( 99.99)
train Loss: 0.0602 Acc: 0.9817
Epoch: [13][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.0333e+00 (6.3349e-02)    Acc@1  78.91 ( 98.11)   Acc@5  97.66 ( 99.99)
val Loss: 0.9902 Acc: 0.7673

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 4.0054e-02 (4.0054e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.1715e-02 (4.1914e-02)    Acc@1 100.00 ( 98.86)   Acc@5 100.00 ( 99.99)
Epoch: [14][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.1478e-02 (3.6316e-02)    Acc@1  99.22 ( 99.08)   Acc@5 100.00 ( 99.99)
Epoch: [14][301/313]    Time  0.043 ( 0.043)    Data  0.025 ( 0.024)    Loss 3.1846e-02 (3.5261e-02)    Acc@1  99.22 ( 99.09)   Acc@5 100.00 ( 99.99)
train Loss: 0.0352 Acc: 0.9909
Epoch: [14][  1/313]    Time  0.045 ( 0.043)    Data  0.037 ( 0.024)    Loss 8.7822e-01 (3.7898e-02)    Acc@1  77.34 ( 99.03)   Acc@5 100.00 (100.00)
val Loss: 0.8434 Acc: 0.7983

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 1.8920e-02 (1.8920e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.4823e-02 (1.7774e-02)    Acc@1 100.00 ( 99.67)   Acc@5 100.00 (100.00)
Epoch: [15][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 9.4901e-03 (1.6577e-02)    Acc@1 100.00 ( 99.70)   Acc@5 100.00 (100.00)
Epoch: [15][301/313]    Time  0.046 ( 0.043)    Data  0.025 ( 0.024)    Loss 1.8152e-02 (1.7114e-02)    Acc@1 100.00 ( 99.69)   Acc@5 100.00 (100.00)
train Loss: 0.0170 Acc: 0.9969
Epoch: [15][  1/313]    Time  0.043 ( 0.043)    Data  0.037 ( 0.024)    Loss 9.3668e-01 (1.9965e-02)    Acc@1  81.25 ( 99.63)   Acc@5  96.09 ( 99.99)
val Loss: 0.8801 Acc: 0.8006

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.103 ( 0.103)    Data  0.071 ( 0.071)    Loss 8.1508e-03 (8.1508e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.5711e-02 (9.3855e-03)    Acc@1  99.22 ( 99.89)   Acc@5 100.00 (100.00)
Epoch: [16][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 5.3850e-03 (8.8325e-03)    Acc@1 100.00 ( 99.91)   Acc@5 100.00 (100.00)
Epoch: [16][301/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.6675e-02 (8.2576e-03)    Acc@1  99.22 ( 99.93)   Acc@5 100.00 (100.00)
train Loss: 0.0084 Acc: 0.9993
Epoch: [16][  1/313]    Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 7.9828e-01 (1.0895e-02)    Acc@1  79.69 ( 99.86)   Acc@5 100.00 (100.00)
val Loss: 0.8594 Acc: 0.8011

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 3.5052e-03 (3.5052e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 5.8215e-03 (4.8600e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [17][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.4670e-03 (5.0841e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [17][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.9704e-03 (4.8264e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
train Loss: 0.0048 Acc: 0.9999
Epoch: [17][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.0486e+00 (8.1539e-03)    Acc@1  74.22 ( 99.90)   Acc@5  98.44 (100.00)
val Loss: 0.8836 Acc: 0.8029

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.098 ( 0.098)    Data  0.069 ( 0.069)    Loss 1.2784e-03 (1.2784e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.9192e-03 (3.6989e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [18][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 8.6619e-03 (3.5900e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [18][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 4.3152e-03 (3.6804e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
train Loss: 0.0037 Acc: 0.9998
Epoch: [18][  1/313]    Time  0.045 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.0519e+00 (7.0649e-03)    Acc@1  78.12 ( 99.91)   Acc@5  98.44 (100.00)
val Loss: 0.8892 Acc: 0.8052

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.099 ( 0.099)    Data  0.071 ( 0.071)    Loss 2.4311e-03 (2.4311e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.042 ( 0.047)    Data  0.024 ( 0.026)    Loss 4.1787e-03 (2.9148e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [19][201/313]    Time  0.045 ( 0.045)    Data  0.027 ( 0.025)    Loss 5.7877e-03 (2.7559e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [19][301/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.2339e-03 (2.7261e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0027 Acc: 1.0000
Epoch: [19][  1/313]    Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 1.1037e+00 (6.2349e-03)    Acc@1  78.91 ( 99.93)   Acc@5  96.88 ( 99.99)
val Loss: 0.8941 Acc: 0.8043

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 1.4880e-03 (1.4880e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.041 ( 0.047)    Data  0.022 ( 0.026)    Loss 1.3928e-03 (2.5626e-03)    Acc@1 100.00 ( 99.98)   Acc@5 100.00 (100.00)
Epoch: [20][201/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.4035e-03 (2.5673e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [20][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.9372e-03 (2.5289e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0025 Acc: 0.9999
Epoch: [20][  1/313]    Time  0.041 ( 0.044)    Data  0.034 ( 0.024)    Loss 9.1500e-01 (5.4172e-03)    Acc@1  83.59 ( 99.94)   Acc@5  99.22 (100.00)
val Loss: 0.8869 Acc: 0.8047

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 4.3685e-03 (4.3685e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.042 ( 0.044)    Data  0.022 ( 0.024)    Loss 1.2877e-03 (2.0233e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [21][201/313]    Time  0.042 ( 0.043)    Data  0.023 ( 0.024)    Loss 3.1633e-03 (1.9866e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [21][301/313]    Time  0.046 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.1752e-03 (2.0342e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0021 Acc: 1.0000
Epoch: [21][  1/313]    Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 6.5211e-01 (4.1274e-03)    Acc@1  82.81 ( 99.95)   Acc@5 100.00 (100.00)
val Loss: 0.8890 Acc: 0.8057

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.101 ( 0.101)    Data  0.071 ( 0.071)    Loss 1.5188e-03 (1.5188e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.5594e-03 (1.7553e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [22][201/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.4504e-03 (1.8526e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [22][301/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.2871e-03 (1.8274e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0018 Acc: 1.0000
Epoch: [22][  1/313]    Time  0.041 ( 0.043)    Data  0.034 ( 0.024)    Loss 9.0279e-01 (4.7039e-03)    Acc@1  78.91 ( 99.93)   Acc@5  98.44 (100.00)
val Loss: 0.8941 Acc: 0.8059

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 2.0412e-03 (2.0412e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.6563e-03 (1.9825e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [23][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.0287e-03 (2.0243e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
Epoch: [23][301/313]    Time  0.045 ( 0.043)    Data  0.026 ( 0.024)    Loss 1.2240e-03 (1.9287e-03)    Acc@1 100.00 ( 99.99)   Acc@5 100.00 (100.00)
train Loss: 0.0019 Acc: 1.0000
Epoch: [23][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 1.1425e+00 (5.5696e-03)    Acc@1  81.25 ( 99.94)   Acc@5  97.66 ( 99.99)
val Loss: 0.8917 Acc: 0.8059

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 1.0164e-03 (1.0164e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 9.5625e-04 (1.5907e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [24][201/313]    Time  0.045 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.0461e-03 (1.5759e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [24][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.0786e-03 (1.5993e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0016 Acc: 1.0000
Epoch: [24][  1/313]    Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 1.0039e+00 (4.7958e-03)    Acc@1  79.69 ( 99.94)   Acc@5  96.88 ( 99.99)
val Loss: 0.8972 Acc: 0.8047

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.102 ( 0.102)    Data  0.070 ( 0.070)    Loss 8.7970e-04 (8.7970e-04)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.4068e-03 (1.6404e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [25][201/313]    Time  0.041 ( 0.045)    Data  0.023 ( 0.024)    Loss 1.1547e-03 (1.5617e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [25][301/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 8.3048e-04 (1.5526e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0016 Acc: 1.0000
Epoch: [25][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 9.6310e-01 (4.6175e-03)    Acc@1  73.44 ( 99.92)   Acc@5  98.44 (100.00)
val Loss: 0.9015 Acc: 0.8056

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.098 ( 0.098)    Data  0.069 ( 0.069)    Loss 1.7520e-03 (1.7520e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 9.5501e-04 (1.4784e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [26][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.0324e-03 (1.5101e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [26][301/313]    Time  0.044 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.3655e-03 (1.5389e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0015 Acc: 1.0000
Epoch: [26][  1/313]    Time  0.040 ( 0.043)    Data  0.034 ( 0.024)    Loss 1.3327e+00 (5.7906e-03)    Acc@1  78.91 ( 99.93)   Acc@5  96.88 ( 99.99)
val Loss: 0.9005 Acc: 0.8067

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.099 ( 0.099)    Data  0.070 ( 0.070)    Loss 1.4520e-03 (1.4520e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.1685e-03 (1.4332e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [27][201/313]    Time  0.044 ( 0.043)    Data  0.024 ( 0.024)    Loss 2.8459e-03 (1.4084e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [27][301/313]    Time  0.046 ( 0.043)    Data  0.025 ( 0.024)    Loss 1.4792e-03 (1.4479e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0015 Acc: 1.0000
Epoch: [27][  1/313]    Time  0.042 ( 0.043)    Data  0.035 ( 0.024)    Loss 7.6218e-01 (3.8767e-03)    Acc@1  87.50 ( 99.96)   Acc@5  97.66 ( 99.99)
val Loss: 0.8992 Acc: 0.8059

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 1.0834e-03 (1.0834e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.7681e-03 (1.6288e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [28][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.0185e-03 (1.5809e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [28][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 8.2387e-04 (1.5076e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0015 Acc: 1.0000
Epoch: [28][  1/313]    Time  0.041 ( 0.043)    Data  0.034 ( 0.024)    Loss 1.0399e+00 (4.8134e-03)    Acc@1  80.47 ( 99.94)   Acc@5  99.22 (100.00)
val Loss: 0.9008 Acc: 0.8062

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.101 ( 0.101)    Data  0.069 ( 0.069)    Loss 1.0982e-03 (1.0982e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.2619e-03 (1.4708e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [29][201/313]    Time  0.042 ( 0.043)    Data  0.024 ( 0.024)    Loss 9.1736e-04 (1.4048e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [29][301/313]    Time  0.043 ( 0.043)    Data  0.023 ( 0.024)    Loss 5.3104e-03 (1.4287e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0014 Acc: 1.0000
Epoch: [29][  1/313]    Time  0.042 ( 0.043)    Data  0.036 ( 0.024)    Loss 7.8300e-01 (3.9186e-03)    Acc@1  80.47 ( 99.94)   Acc@5 100.00 (100.00)
val Loss: 0.9008 Acc: 0.8073

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 2.7907e-03 (2.7907e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 1.6725e-03 (1.3920e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [30][201/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 7.6447e-04 (1.4003e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [30][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.0325e-03 (1.4076e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0014 Acc: 1.0000
Epoch: [30][  1/313]    Time  0.043 ( 0.043)    Data  0.037 ( 0.024)    Loss 1.4308e+00 (5.9864e-03)    Acc@1  75.78 ( 99.92)   Acc@5  97.66 ( 99.99)
val Loss: 0.9036 Acc: 0.8056

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.101 ( 0.101)    Data  0.070 ( 0.070)    Loss 1.8074e-03 (1.8074e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 7.7435e-04 (1.5095e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [31][201/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 1.0594e-03 (1.4402e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [31][301/313]    Time  0.043 ( 0.043)    Data  0.024 ( 0.024)    Loss 3.0950e-03 (1.4516e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0015 Acc: 1.0000
Epoch: [31][  1/313]    Time  0.043 ( 0.043)    Data  0.036 ( 0.024)    Loss 1.3904e+00 (5.8821e-03)    Acc@1  73.44 ( 99.92)   Acc@5  97.66 ( 99.99)
val Loss: 0.9031 Acc: 0.8054

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.100 ( 0.100)    Data  0.070 ( 0.070)    Loss 8.7529e-04 (8.7529e-04)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.2955e-03 (1.3994e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [32][201/313]    Time  0.044 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.5707e-03 (1.3548e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [32][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.8651e-03 (1.3855e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0014 Acc: 1.0000
Epoch: [32][  1/313]    Time  0.047 ( 0.043)    Data  0.041 ( 0.024)    Loss 8.3879e-01 (4.0547e-03)    Acc@1  82.81 ( 99.95)   Acc@5  99.22 (100.00)
val Loss: 0.9052 Acc: 0.8059

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.104 ( 0.104)    Data  0.082 ( 0.082)    Loss 1.2260e-03 (1.2260e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.045 ( 0.046)    Data  0.025 ( 0.026)    Loss 1.7769e-03 (1.3466e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][201/313]    Time  0.047 ( 0.049)    Data  0.027 ( 0.028)    Loss 7.1547e-04 (1.4183e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][301/313]    Time  0.042 ( 0.049)    Data  0.023 ( 0.028)    Loss 9.2793e-04 (1.4053e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0014 Acc: 1.0000
Epoch: [33][  1/313]    Time  0.043 ( 0.049)    Data  0.035 ( 0.028)    Loss 8.0675e-01 (3.9712e-03)    Acc@1  78.91 ( 99.93)   Acc@5  99.22 (100.00)
val Loss: 0.9038 Acc: 0.8058

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.101 ( 0.101)    Data  0.072 ( 0.072)    Loss 1.4377e-03 (1.4377e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.042 ( 0.046)    Data  0.023 ( 0.025)    Loss 8.5854e-04 (1.3542e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][201/313]    Time  0.041 ( 0.044)    Data  0.023 ( 0.025)    Loss 7.7289e-04 (1.3083e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [34][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 6.3694e-04 (1.3126e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 1.0000
Epoch: [34][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 6.9695e-01 (3.5414e-03)    Acc@1  85.94 ( 99.96)   Acc@5  99.22 (100.00)
val Loss: 0.9038 Acc: 0.8060

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 2.1659e-03 (2.1659e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.024)    Loss 7.8053e-04 (1.2290e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.1797e-03 (1.3487e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.025)    Loss 1.2225e-03 (1.3550e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 1.0000
Epoch: [35][  1/313]    Time  0.048 ( 0.044)    Data  0.042 ( 0.025)    Loss 1.2625e+00 (5.3655e-03)    Acc@1  75.78 ( 99.92)   Acc@5  95.31 ( 99.99)
val Loss: 0.8999 Acc: 0.8063

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.149 ( 0.149)    Data  0.116 ( 0.116)    Loss 3.3959e-03 (3.3959e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.3070e-03 (1.2988e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.025)    Loss 1.1876e-03 (1.2889e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.024)    Loss 1.1710e-03 (1.2914e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 1.0000
Epoch: [36][  1/313]    Time  0.044 ( 0.044)    Data  0.035 ( 0.024)    Loss 7.0247e-01 (3.5230e-03)    Acc@1  82.81 ( 99.95)   Acc@5  99.22 (100.00)
val Loss: 0.8988 Acc: 0.8063

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.103 ( 0.103)    Data  0.071 ( 0.071)    Loss 1.3790e-03 (1.3790e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.048 ( 0.044)    Data  0.027 ( 0.024)    Loss 1.6124e-03 (1.3472e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.3366e-03 (1.3033e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [37][301/313]    Time  0.041 ( 0.043)    Data  0.023 ( 0.024)    Loss 1.0547e-03 (1.2597e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 1.0000
Epoch: [37][  1/313]    Time  0.048 ( 0.043)    Data  0.038 ( 0.024)    Loss 1.4055e+00 (5.7429e-03)    Acc@1  73.44 ( 99.92)   Acc@5  99.22 (100.00)
val Loss: 0.9007 Acc: 0.8054

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.102 ( 0.102)    Data  0.071 ( 0.071)    Loss 1.1307e-03 (1.1307e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.043 ( 0.047)    Data  0.024 ( 0.025)    Loss 1.9358e-03 (1.2563e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.3998e-03 (1.2978e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [38][301/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.3444e-03 (1.3499e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 1.0000
Epoch: [38][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.025)    Loss 8.0690e-01 (3.9146e-03)    Acc@1  80.47 ( 99.94)   Acc@5  98.44 (100.00)
val Loss: 0.9026 Acc: 0.8072

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.123 ( 0.123)    Data  0.093 ( 0.093)    Loss 8.2167e-04 (8.2167e-04)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 9.0389e-04 (1.2495e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][201/313]    Time  0.045 ( 0.044)    Data  0.025 ( 0.024)    Loss 3.5758e-03 (1.2806e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.024)    Loss 1.0414e-03 (1.2622e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
train Loss: 0.0013 Acc: 1.0000
Epoch: [39][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 9.1623e-01 (4.1917e-03)    Acc@1  83.59 ( 99.95)   Acc@5  99.22 (100.00)
val Loss: 0.9050 Acc: 0.8049

Training complete in 10m 58s
Best val Acc: 0.807300
Test Loss: 0.180151

Test Accuracy of airplane: 83% (859/1030)
Test Accuracy of automobile: 89% (922/1028)
Test Accuracy of  bird: 69% (686/982)
Test Accuracy of   cat: 67% (649/957)
Test Accuracy of  deer: 77% (776/1001)
Test Accuracy of   dog: 70% (692/982)
Test Accuracy of  frog: 83% (842/1007)
Test Accuracy of horse: 83% (862/1033)
Test Accuracy of  ship: 92% (891/968)
Test Accuracy of truck: 88% (894/1012)

Test Accuracy (Overall): 80% (8073/10000)