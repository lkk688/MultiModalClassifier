PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\MultiModalClassifier\data\" --model_name 'squeezenetcustom' --learningratename 'OneCycleLR' --optimizer 'SGD'
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_squeezenetcustom_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
SqueezeNet (SqueezeNet)                  [128, 3, 224, 224]   [128, 490]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 96, 224, 224]  2,688                True
├─BatchNorm2d (bn1)                      [128, 96, 224, 224]  [128, 96, 224, 224]  192                  True
├─ReLU (relu)                            [128, 96, 224, 224]  [128, 96, 224, 224]  --                   --
├─MaxPool2d (maxpool1)                   [128, 96, 224, 224]  [128, 96, 112, 112]  --                   --
├─fire (fire2)                           [128, 96, 112, 112]  [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 96, 112, 112]  [128, 16, 112, 112]  1,552                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire3)                           [128, 128, 112, 112] [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 16, 112, 112]  2,064                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire4)                           [128, 128, 112, 112] [128, 256, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 32, 112, 112]  4,128                True
│    └─BatchNorm2d (bn1)                 [128, 32, 112, 112]  [128, 32, 112, 112]  64                   True
│    └─ReLU (relu1)                      [128, 32, 112, 112]  [128, 32, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 32, 112, 112]  [128, 128, 112, 112] 4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─Conv2d (conv3)                    [128, 32, 112, 112]  [128, 128, 112, 112] 36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─ReLU (relu2)                      [128, 256, 112, 112] [128, 256, 112, 112] --                   --
├─MaxPool2d (maxpool2)                   [128, 256, 112, 112] [128, 256, 56, 56]   --                   --
├─fire (fire5)                           [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 32, 56, 56]    8,224                True
│    └─BatchNorm2d (bn1)                 [128, 32, 56, 56]    [128, 32, 56, 56]    64                   True
│    └─ReLU (relu1)                      [128, 32, 56, 56]    [128, 32, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 32, 56, 56]    [128, 128, 56, 56]   4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─Conv2d (conv3)                    [128, 32, 56, 56]    [128, 128, 56, 56]   36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─ReLU (relu2)                      [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─fire (fire6)                           [128, 256, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 48, 56, 56]    12,336               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire7)                           [128, 384, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 48, 56, 56]    18,480               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire8)                           [128, 384, 56, 56]   [128, 512, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 64, 56, 56]    24,640               True
│    └─BatchNorm2d (bn1)                 [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    └─ReLU (relu1)                      [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 56, 56]    [128, 256, 56, 56]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 56, 56]    [128, 256, 56, 56]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─ReLU (relu2)                      [128, 512, 56, 56]   [128, 512, 56, 56]   --                   --
├─MaxPool2d (maxpool3)                   [128, 512, 56, 56]   [128, 512, 28, 28]   --                   --
├─fire (fire9)                           [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    └─Conv2d (conv1)                    [128, 512, 28, 28]   [128, 64, 28, 28]    32,832               True
│    └─BatchNorm2d (bn1)                 [128, 64, 28, 28]    [128, 64, 28, 28]    128                  True
│    └─ReLU (relu1)                      [128, 64, 28, 28]    [128, 64, 28, 28]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 28, 28]    [128, 256, 28, 28]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 28, 28]    [128, 256, 28, 28]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─ReLU (relu2)                      [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Conv2d (conv2)                         [128, 512, 28, 28]   [128, 10, 28, 28]    5,130                True
├─AvgPool2d (avg_pool)                   [128, 10, 28, 28]    [128, 10, 7, 7]      --                   --
========================================================================================================================
Total params: 734,986
Trainable params: 734,986
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 331.85
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 36693.51
Params size (MB): 2.94
Estimated Total Size (MB): 36773.52
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.441 ( 0.441)    Data  0.083 ( 0.083)    Loss 2.3121e+00 (2.3121e+00)    Acc@1  15.62 ( 15.62)   Acc@5  54.69 ( 54.69)
STAGE:2024-03-25 22:42:09 5052:22652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:42:09 5052:22652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:42:09 5052:22652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
STAGE:2024-03-25 22:42:11 5052:22652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 22:42:11 5052:22652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 22:42:11 5052:22652 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing
Epoch: [0][101/313]     Time  0.050 ( 0.096)    Data  0.028 ( 0.071)    Loss 1.7742e+00 (2.0978e+00)    Acc@1  32.81 ( 20.92)   Acc@5  86.72 ( 71.21)
Epoch: [0][201/313]     Time  0.043 ( 0.070)    Data  0.024 ( 0.048)    Loss 1.6777e+00 (1.9077e+00)    Acc@1  38.28 ( 27.17)   Acc@5  85.94 ( 79.32)
Epoch: [0][301/313]     Time  0.045 ( 0.062)    Data  0.026 ( 0.040)    Loss 1.5004e+00 (1.7889e+00)    Acc@1  42.19 ( 32.07)   Acc@5  91.41 ( 83.09)
train Loss: 1.7775 Acc: 0.3256
Epoch: [0][  1/313]     Time  0.045 ( 0.061)    Data  0.038 ( 0.040)    Loss 1.7437e+00 (1.7774e+00)    Acc@1  38.28 ( 32.58)   Acc@5  89.84 ( 83.47)
val Loss: 1.5305 Acc: 0.4319

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.108 ( 0.108)    Data  0.076 ( 0.076)    Loss 1.4480e+00 (1.4480e+00)    Acc@1  53.91 ( 53.91)   Acc@5  90.62 ( 90.62)
Epoch: [1][101/313]     Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.4689e+00 (1.4224e+00)    Acc@1  46.09 ( 47.35)   Acc@5  93.75 ( 92.71)
Epoch: [1][201/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 1.1542e+00 (1.3933e+00)    Acc@1  53.12 ( 48.92)   Acc@5  96.88 ( 92.91)
Epoch: [1][301/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 1.1861e+00 (1.3523e+00)    Acc@1  54.69 ( 50.38)   Acc@5  96.09 ( 93.39)
train Loss: 1.3493 Acc: 0.5050
Epoch: [1][  1/313]     Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 1.2093e+00 (1.3489e+00)    Acc@1  56.25 ( 50.52)   Acc@5  92.19 ( 93.43)
val Loss: 1.2641 Acc: 0.5364

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.104 ( 0.104)    Data  0.074 ( 0.074)    Loss 1.2954e+00 (1.2954e+00)    Acc@1  53.12 ( 53.12)   Acc@5  93.75 ( 93.75)
Epoch: [2][101/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.3205e+00 (1.1933e+00)    Acc@1  50.00 ( 56.27)   Acc@5  94.53 ( 95.06)
Epoch: [2][201/313]     Time  0.100 ( 0.046)    Data  0.058 ( 0.025)    Loss 1.2417e+00 (1.1781e+00)    Acc@1  55.47 ( 57.18)   Acc@5  94.53 ( 95.23)
Epoch: [2][301/313]     Time  0.043 ( 0.047)    Data  0.024 ( 0.026)    Loss 1.0960e+00 (1.1716e+00)    Acc@1  63.28 ( 57.78)   Acc@5  94.53 ( 95.29)
train Loss: 1.1691 Acc: 0.5782
Epoch: [2][  1/313]     Time  0.047 ( 0.047)    Data  0.037 ( 0.026)    Loss 1.2292e+00 (1.1693e+00)    Acc@1  57.81 ( 57.82)   Acc@5  95.31 ( 95.33)
val Loss: 1.1570 Acc: 0.5822

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.104 ( 0.104)    Data  0.075 ( 0.075)    Loss 1.2424e+00 (1.2424e+00)    Acc@1  53.12 ( 53.12)   Acc@5  96.88 ( 96.88)
Epoch: [3][101/313]     Time  0.043 ( 0.047)    Data  0.025 ( 0.026)    Loss 1.1125e+00 (1.0483e+00)    Acc@1  62.50 ( 61.98)   Acc@5  96.09 ( 96.44)
Epoch: [3][201/313]     Time  0.042 ( 0.047)    Data  0.023 ( 0.026)    Loss 1.2491e+00 (1.0442e+00)    Acc@1  52.34 ( 62.32)   Acc@5  95.31 ( 96.47)
Epoch: [3][301/313]     Time  0.046 ( 0.047)    Data  0.025 ( 0.026)    Loss 9.3574e-01 (1.0330e+00)    Acc@1  65.62 ( 62.74)   Acc@5  98.44 ( 96.48)
train Loss: 1.0326 Acc: 0.6280
Epoch: [3][  1/313]     Time  0.045 ( 0.047)    Data  0.038 ( 0.026)    Loss 9.2978e-01 (1.0323e+00)    Acc@1  65.62 ( 62.81)   Acc@5  96.88 ( 96.46)
val Loss: 1.0212 Acc: 0.6317

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.106 ( 0.106)    Data  0.073 ( 0.073)    Loss 9.2273e-01 (9.2273e-01)    Acc@1  68.75 ( 68.75)   Acc@5  96.88 ( 96.88)
Epoch: [4][101/313]     Time  0.045 ( 0.045)    Data  0.025 ( 0.025)    Loss 9.0060e-01 (9.3666e-01)    Acc@1  66.41 ( 66.72)   Acc@5  96.88 ( 97.19)
Epoch: [4][201/313]     Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.1142e+00 (9.3107e-01)    Acc@1  61.72 ( 66.87)   Acc@5  93.75 ( 97.21)
Epoch: [4][301/313]     Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.0184e+00 (9.3162e-01)    Acc@1  63.28 ( 66.73)   Acc@5  95.31 ( 97.22)
train Loss: 0.9315 Acc: 0.6673
Epoch: [4][  1/313]     Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 9.7531e-01 (9.3166e-01)    Acc@1  65.62 ( 66.72)   Acc@5  96.09 ( 97.20)
val Loss: 1.0882 Acc: 0.6189

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.112 ( 0.112)    Data  0.081 ( 0.081)    Loss 7.7183e-01 (7.7183e-01)    Acc@1  76.56 ( 76.56)   Acc@5  98.44 ( 98.44)
Epoch: [5][101/313]     Time  0.046 ( 0.048)    Data  0.025 ( 0.026)    Loss 8.3319e-01 (8.5305e-01)    Acc@1  67.97 ( 69.76)   Acc@5  98.44 ( 97.65)
Epoch: [5][201/313]     Time  0.042 ( 0.046)    Data  0.023 ( 0.025)    Loss 8.4998e-01 (8.5770e-01)    Acc@1  67.19 ( 69.42)   Acc@5  96.09 ( 97.58)
Epoch: [5][301/313]     Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 9.2152e-01 (8.5003e-01)    Acc@1  69.53 ( 69.74)   Acc@5  98.44 ( 97.65)
train Loss: 0.8486 Acc: 0.6978
Epoch: [5][  1/313]     Time  0.050 ( 0.046)    Data  0.040 ( 0.025)    Loss 7.5133e-01 (8.4825e-01)    Acc@1  71.09 ( 69.79)   Acc@5  99.22 ( 97.65)
val Loss: 0.9462 Acc: 0.6648

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 8.4352e-01 (8.4352e-01)    Acc@1  67.97 ( 67.97)   Acc@5  98.44 ( 98.44)
Epoch: [6][101/313]     Time  0.042 ( 0.046)    Data  0.022 ( 0.025)    Loss 6.5843e-01 (7.6871e-01)    Acc@1  78.91 ( 73.05)   Acc@5  97.66 ( 98.19)
Epoch: [6][201/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 8.3078e-01 (7.7138e-01)    Acc@1  70.31 ( 72.72)   Acc@5  99.22 ( 98.18)
Epoch: [6][301/313]     Time  0.044 ( 0.046)    Data  0.025 ( 0.025)    Loss 7.1556e-01 (7.7216e-01)    Acc@1  71.88 ( 72.57)   Acc@5  96.88 ( 98.18)
train Loss: 0.7723 Acc: 0.7255
Epoch: [6][  1/313]     Time  0.042 ( 0.045)    Data  0.036 ( 0.025)    Loss 1.1254e+00 (7.7340e-01)    Acc@1  63.28 ( 72.52)   Acc@5  96.88 ( 98.17)
val Loss: 1.0275 Acc: 0.6478

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.164 ( 0.164)    Data  0.132 ( 0.132)    Loss 6.6456e-01 (6.6456e-01)    Acc@1  75.78 ( 75.78)   Acc@5  99.22 ( 99.22)
Epoch: [7][101/313]     Time  0.043 ( 0.048)    Data  0.024 ( 0.027)    Loss 7.0772e-01 (6.9671e-01)    Acc@1  75.78 ( 75.21)   Acc@5  99.22 ( 98.60)
Epoch: [7][201/313]     Time  0.043 ( 0.047)    Data  0.024 ( 0.026)    Loss 6.8358e-01 (7.0297e-01)    Acc@1  75.00 ( 75.10)   Acc@5  99.22 ( 98.57)
Epoch: [7][301/313]     Time  0.042 ( 0.046)    Data  0.023 ( 0.026)    Loss 7.9252e-01 (7.0812e-01)    Acc@1  72.66 ( 74.94)   Acc@5  97.66 ( 98.54)
train Loss: 0.7077 Acc: 0.7492
Epoch: [7][  1/313]     Time  0.044 ( 0.046)    Data  0.037 ( 0.026)    Loss 9.1048e-01 (7.0835e-01)    Acc@1  68.75 ( 74.90)   Acc@5  97.66 ( 98.55)
val Loss: 0.9044 Acc: 0.6864

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.104 ( 0.104)    Data  0.072 ( 0.072)    Loss 6.5391e-01 (6.5391e-01)    Acc@1  79.69 ( 79.69)   Acc@5  98.44 ( 98.44)
Epoch: [8][101/313]     Time  0.046 ( 0.050)    Data  0.026 ( 0.027)    Loss 5.3392e-01 (6.2562e-01)    Acc@1  82.03 ( 77.77)   Acc@5  99.22 ( 98.87)
Epoch: [8][201/313]     Time  0.044 ( 0.048)    Data  0.024 ( 0.026)    Loss 5.8993e-01 (6.2839e-01)    Acc@1  77.34 ( 77.65)   Acc@5  97.66 ( 98.87)
Epoch: [8][301/313]     Time  0.045 ( 0.049)    Data  0.024 ( 0.027)    Loss 5.3108e-01 (6.3939e-01)    Acc@1  80.47 ( 77.28)   Acc@5  99.22 ( 98.85)
train Loss: 0.6404 Acc: 0.7727
Epoch: [8][  1/313]     Time  0.049 ( 0.048)    Data  0.042 ( 0.027)    Loss 9.6948e-01 (6.4146e-01)    Acc@1  66.41 ( 77.23)   Acc@5  97.66 ( 98.84)
val Loss: 0.9092 Acc: 0.6833

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.129 ( 0.129)    Data  0.094 ( 0.094)    Loss 6.2185e-01 (6.2185e-01)    Acc@1  78.91 ( 78.91)   Acc@5 100.00 (100.00)
Epoch: [9][101/313]     Time  0.045 ( 0.065)    Data  0.025 ( 0.037)    Loss 5.8836e-01 (5.7183e-01)    Acc@1  81.25 ( 79.72)   Acc@5  99.22 ( 99.18)
Epoch: [9][201/313]     Time  0.050 ( 0.056)    Data  0.029 ( 0.032)    Loss 6.6363e-01 (5.8192e-01)    Acc@1  77.34 ( 79.16)   Acc@5  99.22 ( 99.10)
Epoch: [9][301/313]     Time  0.046 ( 0.053)    Data  0.025 ( 0.030)    Loss 6.3086e-01 (5.9462e-01)    Acc@1  78.12 ( 78.86)   Acc@5  98.44 ( 99.04)
train Loss: 0.5941 Acc: 0.7891
Epoch: [9][  1/313]     Time  0.048 ( 0.053)    Data  0.038 ( 0.030)    Loss 9.7717e-01 (5.9536e-01)    Acc@1  68.75 ( 78.88)   Acc@5  96.88 ( 99.03)
val Loss: 0.8933 Acc: 0.6988

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.109 ( 0.109)    Data  0.076 ( 0.076)    Loss 5.5794e-01 (5.5794e-01)    Acc@1  82.81 ( 82.81)   Acc@5  99.22 ( 99.22)
Epoch: [10][101/313]    Time  0.049 ( 0.049)    Data  0.030 ( 0.027)    Loss 4.9116e-01 (5.0639e-01)    Acc@1  82.03 ( 82.24)   Acc@5  98.44 ( 99.26)
Epoch: [10][201/313]    Time  0.045 ( 0.048)    Data  0.025 ( 0.026)    Loss 5.0874e-01 (5.1894e-01)    Acc@1  81.25 ( 81.70)   Acc@5 100.00 ( 99.21)
Epoch: [10][301/313]    Time  0.063 ( 0.048)    Data  0.029 ( 0.026)    Loss 5.8256e-01 (5.3122e-01)    Acc@1  80.47 ( 81.21)   Acc@5  98.44 ( 99.18)
train Loss: 0.5309 Acc: 0.8122
Epoch: [10][  1/313]    Time  0.049 ( 0.048)    Data  0.042 ( 0.027)    Loss 7.0693e-01 (5.3144e-01)    Acc@1  75.78 ( 81.21)   Acc@5  99.22 ( 99.18)
val Loss: 0.8728 Acc: 0.7095

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.113 ( 0.113)    Data  0.078 ( 0.078)    Loss 4.5478e-01 (4.5478e-01)    Acc@1  80.47 ( 80.47)   Acc@5  99.22 ( 99.22)
Epoch: [11][101/313]    Time  0.046 ( 0.048)    Data  0.026 ( 0.026)    Loss 5.0557e-01 (4.5628e-01)    Acc@1  85.94 ( 84.14)   Acc@5  99.22 ( 99.44)
Epoch: [11][201/313]    Time  0.045 ( 0.051)    Data  0.024 ( 0.028)    Loss 5.4479e-01 (4.5929e-01)    Acc@1  82.81 ( 84.07)   Acc@5  99.22 ( 99.48)
Epoch: [11][301/313]    Time  0.047 ( 0.049)    Data  0.026 ( 0.027)    Loss 4.8851e-01 (4.7987e-01)    Acc@1  82.81 ( 83.34)   Acc@5 100.00 ( 99.42)
train Loss: 0.4806 Acc: 0.8331
Epoch: [11][  1/313]    Time  0.049 ( 0.049)    Data  0.042 ( 0.027)    Loss 8.2905e-01 (4.8175e-01)    Acc@1  67.19 ( 83.25)   Acc@5  98.44 ( 99.42)
val Loss: 0.9949 Acc: 0.6851

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.117 ( 0.117)    Data  0.082 ( 0.082)    Loss 4.1768e-01 (4.1768e-01)    Acc@1  88.28 ( 88.28)   Acc@5 100.00 (100.00)
Epoch: [12][101/313]    Time  0.044 ( 0.048)    Data  0.024 ( 0.026)    Loss 2.7732e-01 (4.1804e-01)    Acc@1  89.06 ( 85.43)   Acc@5 100.00 ( 99.43)
Epoch: [12][201/313]    Time  0.044 ( 0.047)    Data  0.024 ( 0.026)    Loss 4.8965e-01 (4.2106e-01)    Acc@1  85.16 ( 85.33)   Acc@5  99.22 ( 99.48)
Epoch: [12][301/313]    Time  0.046 ( 0.047)    Data  0.026 ( 0.026)    Loss 4.6570e-01 (4.3040e-01)    Acc@1  84.38 ( 84.90)   Acc@5 100.00 ( 99.49)
train Loss: 0.4333 Acc: 0.8480
Epoch: [12][  1/313]    Time  0.045 ( 0.047)    Data  0.037 ( 0.026)    Loss 1.2375e+00 (4.3589e-01)    Acc@1  68.75 ( 84.75)   Acc@5  95.31 ( 99.47)
val Loss: 0.9358 Acc: 0.7001

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.105 ( 0.105)    Data  0.073 ( 0.073)    Loss 2.9142e-01 (2.9142e-01)    Acc@1  92.19 ( 92.19)   Acc@5  99.22 ( 99.22)
Epoch: [13][101/313]    Time  0.044 ( 0.049)    Data  0.024 ( 0.027)    Loss 3.8996e-01 (3.6836e-01)    Acc@1  88.28 ( 87.35)   Acc@5 100.00 ( 99.68)
Epoch: [13][201/313]    Time  0.044 ( 0.047)    Data  0.025 ( 0.027)    Loss 4.9107e-01 (3.8657e-01)    Acc@1  82.03 ( 86.54)   Acc@5 100.00 ( 99.65)
Epoch: [13][301/313]    Time  0.042 ( 0.047)    Data  0.024 ( 0.026)    Loss 3.1815e-01 (3.9525e-01)    Acc@1  88.28 ( 86.07)   Acc@5 100.00 ( 99.62)
train Loss: 0.3980 Acc: 0.8601
Epoch: [13][  1/313]    Time  0.045 ( 0.046)    Data  0.038 ( 0.026)    Loss 8.9997e-01 (3.9964e-01)    Acc@1  72.66 ( 85.97)   Acc@5  97.66 ( 99.59)
val Loss: 0.9583 Acc: 0.6991

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.102 ( 0.102)    Data  0.073 ( 0.073)    Loss 3.5554e-01 (3.5554e-01)    Acc@1  87.50 ( 87.50)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.044 ( 0.046)    Data  0.025 ( 0.025)    Loss 2.9159e-01 (3.4105e-01)    Acc@1  88.28 ( 88.09)   Acc@5  99.22 ( 99.69)
Epoch: [14][201/313]    Time  0.043 ( 0.047)    Data  0.024 ( 0.026)    Loss 3.9835e-01 (3.3807e-01)    Acc@1  88.28 ( 88.32)   Acc@5  99.22 ( 99.69)
Epoch: [14][301/313]    Time  0.044 ( 0.046)    Data  0.023 ( 0.025)    Loss 4.4180e-01 (3.4965e-01)    Acc@1  83.59 ( 87.76)   Acc@5  99.22 ( 99.69)
train Loss: 0.3525 Acc: 0.8765
Epoch: [14][  1/313]    Time  0.048 ( 0.046)    Data  0.038 ( 0.025)    Loss 7.3020e-01 (3.5372e-01)    Acc@1  75.78 ( 87.61)   Acc@5  97.66 ( 99.67)
val Loss: 0.8765 Acc: 0.7219

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 2.0860e-01 (2.0860e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 3.2868e-01 (2.9694e-01)    Acc@1  86.72 ( 89.65)   Acc@5 100.00 ( 99.84)
Epoch: [15][201/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 3.2267e-01 (3.0079e-01)    Acc@1  87.50 ( 89.49)   Acc@5 100.00 ( 99.80)
Epoch: [15][301/313]    Time  0.051 ( 0.045)    Data  0.028 ( 0.025)    Loss 3.8138e-01 (3.1624e-01)    Acc@1  87.50 ( 88.95)   Acc@5  99.22 ( 99.77)
train Loss: 0.3174 Acc: 0.8890
Epoch: [15][  1/313]    Time  0.045 ( 0.045)    Data  0.037 ( 0.025)    Loss 6.3254e-01 (3.1838e-01)    Acc@1  77.34 ( 88.86)   Acc@5 100.00 ( 99.76)
val Loss: 0.9740 Acc: 0.7128

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.100 ( 0.100)    Data  0.079 ( 0.079)    Loss 1.5110e-01 (1.5110e-01)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.026)    Loss 2.1697e-01 (2.4907e-01)    Acc@1  93.75 ( 91.36)   Acc@5 100.00 ( 99.85)
Epoch: [16][201/313]    Time  0.047 ( 0.045)    Data  0.026 ( 0.025)    Loss 3.2820e-01 (2.5267e-01)    Acc@1  85.94 ( 91.12)   Acc@5 100.00 ( 99.83)
Epoch: [16][301/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.9077e-01 (2.6991e-01)    Acc@1  88.28 ( 90.38)   Acc@5 100.00 ( 99.81)
train Loss: 0.2740 Acc: 0.9025
Epoch: [16][  1/313]    Time  0.045 ( 0.045)    Data  0.038 ( 0.025)    Loss 9.5331e-01 (2.7613e-01)    Acc@1  72.66 ( 90.19)   Acc@5  99.22 ( 99.80)
val Loss: 1.0547 Acc: 0.7126

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.115 ( 0.115)    Data  0.081 ( 0.081)    Loss 3.5000e-01 (3.5000e-01)    Acc@1  89.84 ( 89.84)   Acc@5  99.22 ( 99.22)
Epoch: [17][101/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 2.4632e-01 (2.5448e-01)    Acc@1  90.62 ( 91.30)   Acc@5 100.00 ( 99.86)
Epoch: [17][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.9110e-01 (2.5072e-01)    Acc@1  90.62 ( 91.36)   Acc@5  99.22 ( 99.86)
Epoch: [17][301/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.0807e-01 (2.5816e-01)    Acc@1  93.75 ( 90.98)   Acc@5 100.00 ( 99.87)
train Loss: 0.2594 Acc: 0.9093
Epoch: [17][  1/313]    Time  0.047 ( 0.045)    Data  0.038 ( 0.025)    Loss 9.9700e-01 (2.6176e-01)    Acc@1  71.88 ( 90.87)   Acc@5  96.09 ( 99.85)
val Loss: 1.0721 Acc: 0.7129

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.108 ( 0.108)    Data  0.075 ( 0.075)    Loss 1.4380e-01 (1.4380e-01)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [18][101/313]    Time  0.069 ( 0.062)    Data  0.042 ( 0.035)    Loss 2.6901e-01 (2.1283e-01)    Acc@1  90.62 ( 92.82)   Acc@5 100.00 ( 99.94)
Epoch: [18][201/313]    Time  0.044 ( 0.054)    Data  0.025 ( 0.031)    Loss 3.1160e-01 (2.1865e-01)    Acc@1  89.06 ( 92.45)   Acc@5  99.22 ( 99.90)
Epoch: [18][301/313]    Time  0.090 ( 0.052)    Data  0.063 ( 0.029)    Loss 2.9193e-01 (2.3248e-01)    Acc@1  88.28 ( 91.94)   Acc@5 100.00 ( 99.87)
train Loss: 0.2346 Acc: 0.9185
Epoch: [18][  1/313]    Time  0.048 ( 0.052)    Data  0.041 ( 0.029)    Loss 1.2391e+00 (2.3783e-01)    Acc@1  65.62 ( 91.77)   Acc@5  97.66 ( 99.87)
val Loss: 1.0666 Acc: 0.7149

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 2.1347e-01 (2.1347e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.052 ( 0.048)    Data  0.028 ( 0.027)    Loss 2.3573e-01 (1.8896e-01)    Acc@1  89.06 ( 93.32)   Acc@5 100.00 ( 99.97)
Epoch: [19][201/313]    Time  0.042 ( 0.048)    Data  0.023 ( 0.027)    Loss 1.8443e-01 (1.8480e-01)    Acc@1  92.97 ( 93.57)   Acc@5 100.00 ( 99.94)
Epoch: [19][301/313]    Time  0.111 ( 0.050)    Data  0.074 ( 0.028)    Loss 1.7885e-01 (1.9615e-01)    Acc@1  92.97 ( 93.13)   Acc@5 100.00 ( 99.94)
train Loss: 0.1970 Acc: 0.9310
Epoch: [19][  1/313]    Time  0.059 ( 0.052)    Data  0.051 ( 0.029)    Loss 9.1919e-01 (1.9930e-01)    Acc@1  73.44 ( 93.04)   Acc@5  98.44 ( 99.93)
val Loss: 1.0146 Acc: 0.7253

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.110 ( 0.110)    Data  0.079 ( 0.079)    Loss 2.6688e-01 (2.6688e-01)    Acc@1  92.97 ( 92.97)   Acc@5  97.66 ( 97.66)
Epoch: [20][101/313]    Time  0.045 ( 0.053)    Data  0.026 ( 0.030)    Loss 1.0705e-01 (1.7131e-01)    Acc@1  96.88 ( 94.22)   Acc@5 100.00 ( 99.95)
Epoch: [20][201/313]    Time  0.045 ( 0.051)    Data  0.026 ( 0.029)    Loss 2.8721e-01 (1.7125e-01)    Acc@1  89.06 ( 94.23)   Acc@5  99.22 ( 99.93)
Epoch: [20][301/313]    Time  0.048 ( 0.050)    Data  0.026 ( 0.028)    Loss 2.4395e-01 (1.7917e-01)    Acc@1  92.19 ( 93.83)   Acc@5 100.00 ( 99.94)
train Loss: 0.1798 Acc: 0.9378
Epoch: [20][  1/313]    Time  0.049 ( 0.050)    Data  0.042 ( 0.029)    Loss 1.5964e+00 (1.8433e-01)    Acc@1  64.06 ( 93.68)   Acc@5  96.09 ( 99.93)
val Loss: 1.1688 Acc: 0.7084

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.142 ( 0.142)    Data  0.108 ( 0.108)    Loss 1.7566e-01 (1.7566e-01)    Acc@1  92.97 ( 92.97)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.047 ( 0.051)    Data  0.026 ( 0.029)    Loss 8.7936e-02 (1.3465e-01)    Acc@1  96.88 ( 95.45)   Acc@5 100.00 ( 99.98)
Epoch: [21][201/313]    Time  0.044 ( 0.051)    Data  0.024 ( 0.028)    Loss 1.2995e-01 (1.4319e-01)    Acc@1  94.53 ( 95.15)   Acc@5 100.00 ( 99.97)
Epoch: [21][301/313]    Time  0.048 ( 0.049)    Data  0.025 ( 0.027)    Loss 1.7872e-01 (1.5745e-01)    Acc@1  93.75 ( 94.58)   Acc@5 100.00 ( 99.96)
train Loss: 0.1592 Acc: 0.9451
Epoch: [21][  1/313]    Time  0.045 ( 0.048)    Data  0.038 ( 0.027)    Loss 8.8845e-01 (1.6148e-01)    Acc@1  73.44 ( 94.44)   Acc@5  99.22 ( 99.96)
val Loss: 1.0967 Acc: 0.7232

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.122 ( 0.122)    Data  0.076 ( 0.076)    Loss 1.4889e-01 (1.4889e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.045 ( 0.049)    Data  0.025 ( 0.027)    Loss 1.0311e-01 (1.3020e-01)    Acc@1  96.88 ( 95.77)   Acc@5 100.00 ( 99.99)
Epoch: [22][201/313]    Time  0.043 ( 0.047)    Data  0.024 ( 0.026)    Loss 1.2079e-01 (1.2946e-01)    Acc@1  95.31 ( 95.66)   Acc@5 100.00 ( 99.97)
Epoch: [22][301/313]    Time  0.046 ( 0.047)    Data  0.024 ( 0.025)    Loss 1.8703e-01 (1.3774e-01)    Acc@1  92.97 ( 95.29)   Acc@5 100.00 ( 99.97)
train Loss: 0.1384 Acc: 0.9526
Epoch: [22][  1/313]    Time  0.049 ( 0.047)    Data  0.041 ( 0.025)    Loss 1.0074e+00 (1.4117e-01)    Acc@1  71.88 ( 95.18)   Acc@5  96.88 ( 99.96)
val Loss: 1.1574 Acc: 0.7109

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.091 ( 0.091)    Data  0.072 ( 0.072)    Loss 1.3834e-01 (1.3834e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.049 ( 0.046)    Data  0.026 ( 0.026)    Loss 1.3111e-01 (1.1873e-01)    Acc@1  95.31 ( 96.19)   Acc@5 100.00 ( 99.99)
Epoch: [23][201/313]    Time  0.045 ( 0.053)    Data  0.026 ( 0.030)    Loss 8.0744e-02 (1.1364e-01)    Acc@1  96.88 ( 96.31)   Acc@5 100.00 (100.00)
Epoch: [23][301/313]    Time  0.044 ( 0.050)    Data  0.025 ( 0.029)    Loss 2.1697e-01 (1.1972e-01)    Acc@1  92.97 ( 95.99)   Acc@5 100.00 ( 99.99)
train Loss: 0.1213 Acc: 0.9591
Epoch: [23][  1/313]    Time  0.045 ( 0.050)    Data  0.037 ( 0.029)    Loss 1.1579e+00 (1.2456e-01)    Acc@1  71.88 ( 95.83)   Acc@5  97.66 ( 99.98)
val Loss: 1.1626 Acc: 0.7187

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.114 ( 0.114)    Data  0.076 ( 0.076)    Loss 9.1296e-02 (9.1296e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.044 ( 0.051)    Data  0.024 ( 0.029)    Loss 1.1484e-01 (1.1177e-01)    Acc@1  97.66 ( 95.99)   Acc@5 100.00 ( 99.98)
Epoch: [24][201/313]    Time  0.045 ( 0.052)    Data  0.025 ( 0.029)    Loss 8.6965e-02 (1.1271e-01)    Acc@1  97.66 ( 96.02)   Acc@5 100.00 ( 99.98)
Epoch: [24][301/313]    Time  0.043 ( 0.049)    Data  0.025 ( 0.028)    Loss 9.8317e-02 (1.2066e-01)    Acc@1  97.66 ( 95.76)   Acc@5 100.00 ( 99.99)
train Loss: 0.1206 Acc: 0.9576
Epoch: [24][  1/313]    Time  0.053 ( 0.049)    Data  0.043 ( 0.028)    Loss 1.0141e+00 (1.2347e-01)    Acc@1  73.44 ( 95.69)   Acc@5 100.00 ( 99.99)
val Loss: 1.1596 Acc: 0.7200

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.105 ( 0.105)    Data  0.075 ( 0.075)    Loss 8.0520e-02 (8.0520e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.054 ( 0.046)    Data  0.032 ( 0.025)    Loss 3.0043e-02 (8.7601e-02)    Acc@1  99.22 ( 97.11)   Acc@5 100.00 ( 99.98)
Epoch: [25][201/313]    Time  0.045 ( 0.049)    Data  0.025 ( 0.028)    Loss 7.0927e-02 (9.5027e-02)    Acc@1  96.88 ( 96.84)   Acc@5 100.00 ( 99.98)
Epoch: [25][301/313]    Time  0.043 ( 0.049)    Data  0.024 ( 0.028)    Loss 1.2820e-01 (1.0020e-01)    Acc@1  96.09 ( 96.65)   Acc@5 100.00 ( 99.98)
train Loss: 0.1006 Acc: 0.9663
Epoch: [25][  1/313]    Time  0.052 ( 0.049)    Data  0.045 ( 0.028)    Loss 1.0182e+00 (1.0351e-01)    Acc@1  77.34 ( 96.56)   Acc@5  97.66 ( 99.97)
val Loss: 1.2391 Acc: 0.7184

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.108 ( 0.108)    Data  0.075 ( 0.075)    Loss 3.1362e-02 (3.1362e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.045 ( 0.048)    Data  0.026 ( 0.026)    Loss 9.3344e-02 (8.6112e-02)    Acc@1  97.66 ( 97.18)   Acc@5 100.00 ( 99.98)
Epoch: [26][201/313]    Time  0.044 ( 0.047)    Data  0.025 ( 0.026)    Loss 8.4680e-02 (9.0936e-02)    Acc@1  96.88 ( 96.99)   Acc@5 100.00 ( 99.99)
Epoch: [26][301/313]    Time  0.048 ( 0.046)    Data  0.027 ( 0.025)    Loss 6.2711e-02 (9.1856e-02)    Acc@1  98.44 ( 96.92)   Acc@5 100.00 ( 99.99)
train Loss: 0.0919 Acc: 0.9694
Epoch: [26][  1/313]    Time  0.044 ( 0.046)    Data  0.038 ( 0.026)    Loss 1.1151e+00 (9.5196e-02)    Acc@1  75.78 ( 96.87)   Acc@5  96.88 ( 99.98)
val Loss: 1.2546 Acc: 0.7253

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.092 ( 0.092)    Data  0.073 ( 0.073)    Loss 3.8957e-02 (3.8957e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.044 ( 0.055)    Data  0.025 ( 0.031)    Loss 7.8291e-02 (7.6419e-02)    Acc@1  97.66 ( 97.46)   Acc@5 100.00 (100.00)
Epoch: [27][201/313]    Time  0.047 ( 0.051)    Data  0.026 ( 0.029)    Loss 1.1886e-01 (8.3851e-02)    Acc@1  96.09 ( 97.17)   Acc@5 100.00 (100.00)
Epoch: [27][301/313]    Time  0.043 ( 0.050)    Data  0.025 ( 0.028)    Loss 1.4160e-01 (8.9594e-02)    Acc@1  94.53 ( 96.98)   Acc@5 100.00 ( 99.99)
train Loss: 0.0898 Acc: 0.9698
Epoch: [27][  1/313]    Time  0.045 ( 0.049)    Data  0.037 ( 0.028)    Loss 1.5881e+00 (9.4628e-02)    Acc@1  65.62 ( 96.88)   Acc@5  96.88 ( 99.99)
val Loss: 1.2116 Acc: 0.7286

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.109 ( 0.109)    Data  0.078 ( 0.078)    Loss 5.9332e-02 (5.9332e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.047 ( 0.048)    Data  0.024 ( 0.027)    Loss 4.8006e-02 (7.9335e-02)    Acc@1  99.22 ( 97.34)   Acc@5 100.00 ( 99.98)
Epoch: [28][201/313]    Time  0.071 ( 0.053)    Data  0.041 ( 0.030)    Loss 6.3676e-02 (7.6118e-02)    Acc@1  97.66 ( 97.50)   Acc@5 100.00 ( 99.99)
Epoch: [28][301/313]    Time  0.046 ( 0.052)    Data  0.025 ( 0.029)    Loss 1.0384e-01 (7.9281e-02)    Acc@1  95.31 ( 97.37)   Acc@5 100.00 ( 99.99)
train Loss: 0.0803 Acc: 0.9732
Epoch: [28][  1/313]    Time  0.044 ( 0.052)    Data  0.038 ( 0.029)    Loss 1.4109e+00 (8.4505e-02)    Acc@1  71.88 ( 97.24)   Acc@5  98.44 ( 99.99)
val Loss: 1.2426 Acc: 0.7293

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 5.0825e-02 (5.0825e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.048 ( 0.045)    Data  0.025 ( 0.025)    Loss 5.2584e-02 (7.6939e-02)    Acc@1  97.66 ( 97.35)   Acc@5 100.00 (100.00)
Epoch: [29][201/313]    Time  0.043 ( 0.045)    Data  0.023 ( 0.024)    Loss 4.1589e-02 (7.4918e-02)    Acc@1  99.22 ( 97.54)   Acc@5 100.00 (100.00)
Epoch: [29][301/313]    Time  0.045 ( 0.044)    Data  0.025 ( 0.024)    Loss 8.3003e-02 (7.6410e-02)    Acc@1  95.31 ( 97.45)   Acc@5 100.00 (100.00)
train Loss: 0.0767 Acc: 0.9744
Epoch: [29][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 8.4300e-01 (7.9127e-02)    Acc@1  80.47 ( 97.38)   Acc@5  98.44 ( 99.99)
val Loss: 1.2814 Acc: 0.7246

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.106 ( 0.106)    Data  0.074 ( 0.074)    Loss 6.7924e-02 (6.7924e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 3.3586e-02 (6.3615e-02)    Acc@1  98.44 ( 97.92)   Acc@5 100.00 ( 99.99)
Epoch: [30][201/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 9.5019e-02 (6.5697e-02)    Acc@1  97.66 ( 97.92)   Acc@5 100.00 (100.00)
Epoch: [30][301/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 8.6892e-02 (6.6505e-02)    Acc@1  96.88 ( 97.85)   Acc@5 100.00 (100.00)
train Loss: 0.0667 Acc: 0.9784
Epoch: [30][  1/313]    Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 1.0245e+00 (6.9757e-02)    Acc@1  78.91 ( 97.78)   Acc@5  96.88 ( 99.99)
val Loss: 1.2362 Acc: 0.7346

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 5.0576e-02 (5.0576e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.044 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.7535e-02 (4.6172e-02)    Acc@1 100.00 ( 98.55)   Acc@5 100.00 (100.00)
Epoch: [31][201/313]    Time  0.044 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.6016e-02 (4.6328e-02)    Acc@1  99.22 ( 98.53)   Acc@5 100.00 (100.00)
Epoch: [31][301/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.7725e-02 (4.7953e-02)    Acc@1  99.22 ( 98.52)   Acc@5 100.00 (100.00)
train Loss: 0.0476 Acc: 0.9853
Epoch: [31][  1/313]    Time  0.044 ( 0.044)    Data  0.036 ( 0.025)    Loss 1.1287e+00 (5.1054e-02)    Acc@1  76.56 ( 98.46)   Acc@5  99.22 (100.00)
val Loss: 1.1978 Acc: 0.7438

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.104 ( 0.104)    Data  0.074 ( 0.074)    Loss 2.9451e-02 (2.9451e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 2.6023e-02 (3.5272e-02)    Acc@1  99.22 ( 99.04)   Acc@5 100.00 (100.00)
Epoch: [32][201/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 3.8911e-02 (3.3389e-02)    Acc@1  99.22 ( 99.06)   Acc@5 100.00 (100.00)
Epoch: [32][301/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.5513e-02 (3.4403e-02)    Acc@1 100.00 ( 99.01)   Acc@5 100.00 (100.00)
train Loss: 0.0348 Acc: 0.9899
Epoch: [32][  1/313]    Time  0.045 ( 0.044)    Data  0.037 ( 0.025)    Loss 1.6820e+00 (4.0045e-02)    Acc@1  69.53 ( 98.89)   Acc@5  96.88 ( 99.99)
val Loss: 1.2861 Acc: 0.7288

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.105 ( 0.105)    Data  0.073 ( 0.073)    Loss 1.8592e-02 (1.8592e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 2.6148e-02 (3.4291e-02)    Acc@1  99.22 ( 98.92)   Acc@5 100.00 (100.00)
Epoch: [33][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 4.1973e-02 (3.5093e-02)    Acc@1  99.22 ( 98.94)   Acc@5 100.00 (100.00)
Epoch: [33][301/313]    Time  0.044 ( 0.043)    Data  0.025 ( 0.024)    Loss 2.7455e-02 (3.6250e-02)    Acc@1  99.22 ( 98.88)   Acc@5 100.00 (100.00)
train Loss: 0.0362 Acc: 0.9889
Epoch: [33][  1/313]    Time  0.045 ( 0.044)    Data  0.037 ( 0.024)    Loss 1.1745e+00 (3.9824e-02)    Acc@1  78.12 ( 98.82)   Acc@5  96.09 ( 99.99)
val Loss: 1.1877 Acc: 0.7526

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.106 ( 0.106)    Data  0.075 ( 0.075)    Loss 3.2923e-02 (3.2923e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.8742e-02 (2.9369e-02)    Acc@1  99.22 ( 99.16)   Acc@5 100.00 (100.00)
Epoch: [34][201/313]    Time  0.046 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.3623e-02 (3.0276e-02)    Acc@1 100.00 ( 99.08)   Acc@5 100.00 (100.00)
Epoch: [34][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.5708e-02 (3.0993e-02)    Acc@1  98.44 ( 99.06)   Acc@5 100.00 (100.00)
train Loss: 0.0313 Acc: 0.9906
Epoch: [34][  1/313]    Time  0.043 ( 0.044)    Data  0.037 ( 0.024)    Loss 1.2322e+00 (3.5100e-02)    Acc@1  75.78 ( 98.98)   Acc@5  99.22 (100.00)
val Loss: 1.2287 Acc: 0.7459

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.115 ( 0.115)    Data  0.085 ( 0.085)    Loss 9.4252e-03 (9.4252e-03)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.5325e-02 (2.4963e-02)    Acc@1  99.22 ( 99.25)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.0634e-02 (2.6759e-02)    Acc@1 100.00 ( 99.20)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.025)    Loss 2.8899e-02 (2.6782e-02)    Acc@1  98.44 ( 99.20)   Acc@5 100.00 (100.00)
train Loss: 0.0271 Acc: 0.9919
Epoch: [35][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.025)    Loss 1.0380e+00 (3.0304e-02)    Acc@1  79.69 ( 99.13)   Acc@5  98.44 (100.00)
val Loss: 1.2345 Acc: 0.7460

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 1.1985e-02 (1.1985e-02)    Acc@1 100.00 (100.00)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.1367e-02 (2.2180e-02)    Acc@1 100.00 ( 99.47)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.047 ( 0.045)    Data  0.026 ( 0.025)    Loss 2.3099e-02 (2.4179e-02)    Acc@1  99.22 ( 99.32)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 1.6162e-02 (2.5735e-02)    Acc@1 100.00 ( 99.29)   Acc@5 100.00 (100.00)
train Loss: 0.0254 Acc: 0.9931
Epoch: [36][  1/313]    Time  0.047 ( 0.045)    Data  0.040 ( 0.024)    Loss 1.3696e+00 (2.9654e-02)    Acc@1  72.66 ( 99.22)   Acc@5  98.44 (100.00)
val Loss: 1.2410 Acc: 0.7484

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.106 ( 0.106)    Data  0.075 ( 0.075)    Loss 1.0385e-02 (1.0385e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 8.6971e-03 (2.1155e-02)    Acc@1 100.00 ( 99.40)   Acc@5 100.00 (100.00)
Epoch: [37][201/313]    Time  0.045 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.0411e-02 (2.3228e-02)    Acc@1 100.00 ( 99.31)   Acc@5 100.00 (100.00)
Epoch: [37][301/313]    Time  0.046 ( 0.044)    Data  0.026 ( 0.025)    Loss 4.8788e-02 (2.5659e-02)    Acc@1  99.22 ( 99.21)   Acc@5 100.00 (100.00)
train Loss: 0.0263 Acc: 0.9917
Epoch: [37][  1/313]    Time  0.047 ( 0.044)    Data  0.040 ( 0.025)    Loss 1.4530e+00 (3.0890e-02)    Acc@1  71.09 ( 99.08)   Acc@5  96.88 ( 99.99)
val Loss: 1.3013 Acc: 0.7396

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 1.5197e-02 (1.5197e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.042 ( 0.046)    Data  0.023 ( 0.025)    Loss 2.1616e-02 (3.2262e-02)    Acc@1  99.22 ( 98.92)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.024)    Loss 2.4863e-02 (3.5533e-02)    Acc@1  98.44 ( 98.83)   Acc@5 100.00 (100.00)
Epoch: [38][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 1.5077e-02 (3.8116e-02)    Acc@1  99.22 ( 98.72)   Acc@5 100.00 (100.00)
train Loss: 0.0384 Acc: 0.9871
Epoch: [38][  1/313]    Time  0.043 ( 0.044)    Data  0.035 ( 0.024)    Loss 1.3278e+00 (4.2477e-02)    Acc@1  71.88 ( 98.63)   Acc@5  96.88 ( 99.99)
val Loss: 1.2639 Acc: 0.7435

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.106 ( 0.106)    Data  0.073 ( 0.073)    Loss 5.8301e-02 (5.8301e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.044 ( 0.046)    Data  0.025 ( 0.025)    Loss 9.5575e-02 (4.0674e-02)    Acc@1  96.88 ( 98.68)   Acc@5 100.00 ( 99.99)
Epoch: [39][201/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 4.9950e-02 (4.3651e-02)    Acc@1  98.44 ( 98.55)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 9.3029e-02 (4.9877e-02)    Acc@1  96.88 ( 98.32)   Acc@5 100.00 (100.00)
train Loss: 0.0505 Acc: 0.9832
Epoch: [39][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.025)    Loss 1.3045e+00 (5.4540e-02)    Acc@1  73.44 ( 98.24)   Acc@5  96.88 ( 99.99)
val Loss: 1.3467 Acc: 0.7355

Training complete in 11m 42s
Best val Acc: 0.752600
Test Loss: 0.237535

Test Accuracy of airplane: 79% (770/969)
Test Accuracy of automobile: 85% (859/1003)
Test Accuracy of  bird: 63% (662/1038)
Test Accuracy of   cat: 50% (493/971)
Test Accuracy of  deer: 73% (731/1001)
Test Accuracy of   dog: 65% (652/988)
Test Accuracy of  frog: 78% (779/996)
Test Accuracy of horse: 79% (813/1024)
Test Accuracy of  ship: 89% (900/1004)
Test Accuracy of truck: 86% (867/1006)

Test Accuracy (Overall): 75% (7526/10000)