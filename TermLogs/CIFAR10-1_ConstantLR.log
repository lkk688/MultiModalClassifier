PS C:\mygit\MultiModalClassifier> python .\TorchClassifier\myTorchTrainer.py --data_name 'CIFAR10' --data_type 'torchvisiondataset' --data_path "C:\mygit\MultiModalClassifier\data\" --model_name 'squeezenetcustom' --learningratename 'ConstantLR' --optimizer 'SGD'
2.2.1
Torch Version:  2.2.1
Torchvision Version:  0.17.1
Output path: ./outputs/CIFAR10_squeezenetcustom_0910
Num GPUs: 1
0
NVIDIA GeForce RTX 3070 Ti
True
Files already downloaded and verified
Files already downloaded and verified
Number of training examples: 50000
Number of testing examples: 10000
========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
SqueezeNet (SqueezeNet)                  [128, 3, 224, 224]   [128, 490]           --                   True
├─Conv2d (conv1)                         [128, 3, 224, 224]   [128, 96, 224, 224]  2,688                True
├─BatchNorm2d (bn1)                      [128, 96, 224, 224]  [128, 96, 224, 224]  192                  True
├─ReLU (relu)                            [128, 96, 224, 224]  [128, 96, 224, 224]  --                   --
├─MaxPool2d (maxpool1)                   [128, 96, 224, 224]  [128, 96, 112, 112]  --                   --
├─fire (fire2)                           [128, 96, 112, 112]  [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 96, 112, 112]  [128, 16, 112, 112]  1,552                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire3)                           [128, 128, 112, 112] [128, 128, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 16, 112, 112]  2,064                True
│    └─BatchNorm2d (bn1)                 [128, 16, 112, 112]  [128, 16, 112, 112]  32                   True
│    └─ReLU (relu1)                      [128, 16, 112, 112]  [128, 16, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 16, 112, 112]  [128, 64, 112, 112]  1,088                True
│    └─BatchNorm2d (bn2)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─Conv2d (conv3)                    [128, 16, 112, 112]  [128, 64, 112, 112]  9,280                True
│    └─BatchNorm2d (bn3)                 [128, 64, 112, 112]  [128, 64, 112, 112]  128                  True
│    └─ReLU (relu2)                      [128, 128, 112, 112] [128, 128, 112, 112] --                   --
├─fire (fire4)                           [128, 128, 112, 112] [128, 256, 112, 112] --                   True
│    └─Conv2d (conv1)                    [128, 128, 112, 112] [128, 32, 112, 112]  4,128                True
│    └─BatchNorm2d (bn1)                 [128, 32, 112, 112]  [128, 32, 112, 112]  64                   True
│    └─ReLU (relu1)                      [128, 32, 112, 112]  [128, 32, 112, 112]  --                   --
│    └─Conv2d (conv2)                    [128, 32, 112, 112]  [128, 128, 112, 112] 4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─Conv2d (conv3)                    [128, 32, 112, 112]  [128, 128, 112, 112] 36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 112, 112] [128, 128, 112, 112] 256                  True
│    └─ReLU (relu2)                      [128, 256, 112, 112] [128, 256, 112, 112] --                   --
├─MaxPool2d (maxpool2)                   [128, 256, 112, 112] [128, 256, 56, 56]   --                   --
├─fire (fire5)                           [128, 256, 56, 56]   [128, 256, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 32, 56, 56]    8,224                True
│    └─BatchNorm2d (bn1)                 [128, 32, 56, 56]    [128, 32, 56, 56]    64                   True
│    └─ReLU (relu1)                      [128, 32, 56, 56]    [128, 32, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 32, 56, 56]    [128, 128, 56, 56]   4,224                True
│    └─BatchNorm2d (bn2)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─Conv2d (conv3)                    [128, 32, 56, 56]    [128, 128, 56, 56]   36,992               True
│    └─BatchNorm2d (bn3)                 [128, 128, 56, 56]   [128, 128, 56, 56]   256                  True
│    └─ReLU (relu2)                      [128, 256, 56, 56]   [128, 256, 56, 56]   --                   --
├─fire (fire6)                           [128, 256, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 256, 56, 56]   [128, 48, 56, 56]    12,336               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire7)                           [128, 384, 56, 56]   [128, 384, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 48, 56, 56]    18,480               True
│    └─BatchNorm2d (bn1)                 [128, 48, 56, 56]    [128, 48, 56, 56]    96                   True
│    └─ReLU (relu1)                      [128, 48, 56, 56]    [128, 48, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 48, 56, 56]    [128, 192, 56, 56]   9,408                True
│    └─BatchNorm2d (bn2)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─Conv2d (conv3)                    [128, 48, 56, 56]    [128, 192, 56, 56]   83,136               True
│    └─BatchNorm2d (bn3)                 [128, 192, 56, 56]   [128, 192, 56, 56]   384                  True
│    └─ReLU (relu2)                      [128, 384, 56, 56]   [128, 384, 56, 56]   --                   --
├─fire (fire8)                           [128, 384, 56, 56]   [128, 512, 56, 56]   --                   True
│    └─Conv2d (conv1)                    [128, 384, 56, 56]   [128, 64, 56, 56]    24,640               True
│    └─BatchNorm2d (bn1)                 [128, 64, 56, 56]    [128, 64, 56, 56]    128                  True
│    └─ReLU (relu1)                      [128, 64, 56, 56]    [128, 64, 56, 56]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 56, 56]    [128, 256, 56, 56]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 56, 56]    [128, 256, 56, 56]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 56, 56]   [128, 256, 56, 56]   512                  True
│    └─ReLU (relu2)                      [128, 512, 56, 56]   [128, 512, 56, 56]   --                   --
├─MaxPool2d (maxpool3)                   [128, 512, 56, 56]   [128, 512, 28, 28]   --                   --
├─fire (fire9)                           [128, 512, 28, 28]   [128, 512, 28, 28]   --                   True
│    └─Conv2d (conv1)                    [128, 512, 28, 28]   [128, 64, 28, 28]    32,832               True
│    └─BatchNorm2d (bn1)                 [128, 64, 28, 28]    [128, 64, 28, 28]    128                  True
│    └─ReLU (relu1)                      [128, 64, 28, 28]    [128, 64, 28, 28]    --                   --
│    └─Conv2d (conv2)                    [128, 64, 28, 28]    [128, 256, 28, 28]   16,640               True
│    └─BatchNorm2d (bn2)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─Conv2d (conv3)                    [128, 64, 28, 28]    [128, 256, 28, 28]   147,712              True
│    └─BatchNorm2d (bn3)                 [128, 256, 28, 28]   [128, 256, 28, 28]   512                  True
│    └─ReLU (relu2)                      [128, 512, 28, 28]   [128, 512, 28, 28]   --                   --
├─Conv2d (conv2)                         [128, 512, 28, 28]   [128, 10, 28, 28]    5,130                True
├─AvgPool2d (avg_pool)                   [128, 10, 28, 28]    [128, 10, 7, 7]      --                   --
========================================================================================================================
Total params: 734,986
Trainable params: 734,986
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 331.85
========================================================================================================================
Input size (MB): 77.07
Forward/backward pass size (MB): 36693.51
Params size (MB): 2.94
Estimated Total Size (MB): 36773.52
========================================================================================================================
=> no checkpoint found at 'outputs/imagenet_blurred_resnet50_0328/model_best.pth.tar'
Epoch 0/39
----------
Epoch: [0][  1/313]     Time  0.541 ( 0.541)    Data  0.097 ( 0.097)    Loss 2.3183e+00 (2.3183e+00)    Acc@1  11.72 ( 11.72)   Acc@5  53.12 ( 53.12)
STAGE:2024-03-25 21:25:09 28468:18620 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 21:25:09 28468:18620 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 21:25:09 28468:18620 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing       
STAGE:2024-03-25 21:25:11 28468:18620 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:314] Completed Stage: Warm Up
STAGE:2024-03-25 21:25:12 28468:18620 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:320] Completed Stage: Collection
STAGE:2024-03-25 21:25:12 28468:18620 C:\cb\pytorch_1000000000000\work\third_party\kineto\libkineto\src\ActivityProfilerController.cpp:324] Completed Stage: Post Processing       
Epoch: [0][101/313]     Time  0.042 ( 0.100)    Data  0.023 ( 0.073)    Loss 1.5799e+00 (1.8588e+00)    Acc@1  39.06 ( 30.40)   Acc@5  92.97 ( 80.45)
Epoch: [0][201/313]     Time  0.042 ( 0.073)    Data  0.023 ( 0.050)    Loss 1.3379e+00 (1.6738e+00)    Acc@1  55.47 ( 37.65)   Acc@5  94.53 ( 86.17)
Epoch: [0][301/313]     Time  0.047 ( 0.065)    Data  0.027 ( 0.042)    Loss 1.1041e+00 (1.5457e+00)    Acc@1  63.28 ( 42.68)   Acc@5  96.09 ( 88.95)
train Loss: 1.5345 Acc: 0.4310
Epoch: [0][  1/313]     Time  0.040 ( 0.064)    Data  0.034 ( 0.041)    Loss 1.3439e+00 (1.5339e+00)    Acc@1  52.34 ( 43.13)   Acc@5  95.31 ( 89.17)
val Loss: 1.4720 Acc: 0.4770

Epoch 1/39
----------
Epoch: [1][  1/313]     Time  0.116 ( 0.116)    Data  0.083 ( 0.083)    Loss 1.0788e+00 (1.0788e+00)    Acc@1  64.06 ( 64.06)   Acc@5  96.09 ( 96.09)
Epoch: [1][101/313]     Time  0.045 ( 0.047)    Data  0.025 ( 0.025)    Loss 1.0886e+00 (1.1588e+00)    Acc@1  60.16 ( 58.33)   Acc@5  96.09 ( 95.37)
Epoch: [1][201/313]     Time  0.043 ( 0.046)    Data  0.025 ( 0.025)    Loss 1.0026e+00 (1.1262e+00)    Acc@1  63.28 ( 59.29)   Acc@5  96.88 ( 95.68)
Epoch: [1][301/313]     Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.0529e+00 (1.0927e+00)    Acc@1  60.16 ( 60.58)   Acc@5  98.44 ( 95.93)
train Loss: 1.0914 Acc: 0.6062
Epoch: [1][  1/313]     Time  0.048 ( 0.045)    Data  0.038 ( 0.025)    Loss 1.2724e+00 (1.0920e+00)    Acc@1  53.91 ( 60.60)   Acc@5  96.88 ( 95.92)
val Loss: 1.2199 Acc: 0.5729

Epoch 2/39
----------
Epoch: [2][  1/313]     Time  0.114 ( 0.114)    Data  0.077 ( 0.077)    Loss 9.0647e-01 (9.0647e-01)    Acc@1  64.06 ( 64.06)   Acc@5 100.00 (100.00)
Epoch: [2][101/313]     Time  0.043 ( 0.047)    Data  0.024 ( 0.026)    Loss 9.3923e-01 (9.1451e-01)    Acc@1  64.06 ( 67.57)   Acc@5  97.66 ( 97.28)
Epoch: [2][201/313]     Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 8.9362e-01 (9.1039e-01)    Acc@1  71.88 ( 67.61)   Acc@5  97.66 ( 97.24)
Epoch: [2][301/313]     Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 9.3387e-01 (8.9361e-01)    Acc@1  67.97 ( 68.06)   Acc@5  96.88 ( 97.41)
train Loss: 0.8944 Acc: 0.6807
Epoch: [2][  1/313]     Time  0.042 ( 0.045)    Data  0.036 ( 0.025)    Loss 9.5205e-01 (8.9455e-01)    Acc@1  67.19 ( 68.07)   Acc@5  96.09 ( 97.41)
val Loss: 1.0038 Acc: 0.6441

Epoch 3/39
----------
Epoch: [3][  1/313]     Time  0.108 ( 0.108)    Data  0.075 ( 0.075)    Loss 7.6869e-01 (7.6869e-01)    Acc@1  72.66 ( 72.66)   Acc@5  99.22 ( 99.22)
Epoch: [3][101/313]     Time  0.048 ( 0.046)    Data  0.025 ( 0.025)    Loss 7.4132e-01 (7.8561e-01)    Acc@1  75.00 ( 71.80)   Acc@5  99.22 ( 98.11)
Epoch: [3][201/313]     Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 9.0111e-01 (7.7465e-01)    Acc@1  71.09 ( 72.51)   Acc@5  96.88 ( 98.16)
Epoch: [3][301/313]     Time  0.045 ( 0.045)    Data  0.024 ( 0.025)    Loss 7.1596e-01 (7.6481e-01)    Acc@1  72.66 ( 72.98)   Acc@5  97.66 ( 98.22)
train Loss: 0.7641 Acc: 0.7296
Epoch: [3][  1/313]     Time  0.043 ( 0.045)    Data  0.036 ( 0.025)    Loss 9.0652e-01 (7.6460e-01)    Acc@1  66.41 ( 72.94)   Acc@5  97.66 ( 98.21)
val Loss: 0.9705 Acc: 0.6645

Epoch 4/39
----------
Epoch: [4][  1/313]     Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 6.5815e-01 (6.5815e-01)    Acc@1  78.12 ( 78.12)   Acc@5 100.00 (100.00)
Epoch: [4][101/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 7.6985e-01 (8.3441e-01)    Acc@1  75.00 ( 70.68)   Acc@5  97.66 ( 97.90)
Epoch: [4][201/313]     Time  0.044 ( 0.045)    Data  0.025 ( 0.024)    Loss 7.8186e-01 (8.2101e-01)    Acc@1  70.31 ( 71.06)   Acc@5  97.66 ( 97.94)
Epoch: [4][301/313]     Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 5.9817e-01 (7.9667e-01)    Acc@1  75.00 ( 71.97)   Acc@5  99.22 ( 98.02)
train Loss: 0.7952 Acc: 0.7207
Epoch: [4][  1/313]     Time  0.043 ( 0.044)    Data  0.037 ( 0.024)    Loss 8.0916e-01 (7.9522e-01)    Acc@1  70.31 ( 72.07)   Acc@5 100.00 ( 98.00)
val Loss: 0.9481 Acc: 0.6802

Epoch 5/39
----------
Epoch: [5][  1/313]     Time  0.105 ( 0.105)    Data  0.075 ( 0.075)    Loss 6.4943e-01 (6.4943e-01)    Acc@1  77.34 ( 77.34)   Acc@5 100.00 (100.00)
Epoch: [5][101/313]     Time  0.045 ( 0.046)    Data  0.025 ( 0.025)    Loss 8.3317e-01 (6.6926e-01)    Acc@1  68.75 ( 76.54)   Acc@5  98.44 ( 98.82)
Epoch: [5][201/313]     Time  0.045 ( 0.045)    Data  0.026 ( 0.025)    Loss 7.3154e-01 (6.6098e-01)    Acc@1  75.00 ( 76.84)   Acc@5  97.66 ( 98.78)
Epoch: [5][301/313]     Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 6.7340e-01 (6.6137e-01)    Acc@1  75.00 ( 77.00)   Acc@5  99.22 ( 98.75)
train Loss: 0.6605 Acc: 0.7703
Epoch: [5][  1/313]     Time  0.044 ( 0.045)    Data  0.038 ( 0.025)    Loss 9.0404e-01 (6.6130e-01)    Acc@1  67.19 ( 77.00)   Acc@5  99.22 ( 98.74)
val Loss: 0.9034 Acc: 0.6974

Epoch 6/39
----------
Epoch: [6][  1/313]     Time  0.104 ( 0.104)    Data  0.074 ( 0.074)    Loss 6.0382e-01 (6.0382e-01)    Acc@1  79.69 ( 79.69)   Acc@5  99.22 ( 99.22)
Epoch: [6][101/313]     Time  0.044 ( 0.046)    Data  0.025 ( 0.025)    Loss 4.6499e-01 (5.6171e-01)    Acc@1  83.59 ( 80.44)   Acc@5  98.44 ( 99.03)
Epoch: [6][201/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.024)    Loss 5.2027e-01 (5.6869e-01)    Acc@1  85.16 ( 80.18)   Acc@5  98.44 ( 98.97)
Epoch: [6][301/313]     Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.3906e-01 (5.7372e-01)    Acc@1  82.81 ( 79.98)   Acc@5 100.00 ( 99.01)
train Loss: 0.5741 Acc: 0.7994
Epoch: [6][  1/313]     Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 7.5531e-01 (5.7467e-01)    Acc@1  75.78 ( 79.93)   Acc@5  96.09 ( 99.00)
val Loss: 0.7859 Acc: 0.7336

Epoch 7/39
----------
Epoch: [7][  1/313]     Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 5.0460e-01 (5.0460e-01)    Acc@1  83.59 ( 83.59)   Acc@5 100.00 (100.00)
Epoch: [7][101/313]     Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 6.2172e-01 (4.9596e-01)    Acc@1  82.81 ( 82.85)   Acc@5  97.66 ( 99.26)
Epoch: [7][201/313]     Time  0.046 ( 0.045)    Data  0.024 ( 0.025)    Loss 4.6109e-01 (5.1417e-01)    Acc@1  82.03 ( 82.18)   Acc@5 100.00 ( 99.27)
Epoch: [7][301/313]     Time  0.044 ( 0.044)    Data  0.025 ( 0.025)    Loss 5.2925e-01 (5.1683e-01)    Acc@1  83.59 ( 81.97)   Acc@5 100.00 ( 99.24)
train Loss: 0.5167 Acc: 0.8199
Epoch: [7][  1/313]     Time  0.044 ( 0.044)    Data  0.037 ( 0.025)    Loss 7.0049e-01 (5.1727e-01)    Acc@1  79.69 ( 81.98)   Acc@5  99.22 ( 99.24)
val Loss: 0.7211 Acc: 0.7604

Epoch 8/39
----------
Epoch: [8][  1/313]     Time  0.104 ( 0.104)    Data  0.075 ( 0.075)    Loss 5.2708e-01 (5.2708e-01)    Acc@1  83.59 ( 83.59)   Acc@5  98.44 ( 98.44)
Epoch: [8][101/313]     Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 4.4618e-01 (4.4007e-01)    Acc@1  86.72 ( 84.65)   Acc@5  99.22 ( 99.46)
Epoch: [8][201/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.3957e-01 (4.5381e-01)    Acc@1  90.62 ( 84.10)   Acc@5 100.00 ( 99.42)
Epoch: [8][301/313]     Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 4.5904e-01 (4.6095e-01)    Acc@1  82.81 ( 83.77)   Acc@5 100.00 ( 99.42)
train Loss: 0.4615 Acc: 0.8377
Epoch: [8][  1/313]     Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 1.2814e+00 (4.6413e-01)    Acc@1  57.81 ( 83.68)   Acc@5  96.88 ( 99.40)
val Loss: 0.7936 Acc: 0.7447

Epoch 9/39
----------
Epoch: [9][  1/313]     Time  0.114 ( 0.114)    Data  0.083 ( 0.083)    Loss 4.1925e-01 (4.1925e-01)    Acc@1  85.94 ( 85.94)   Acc@5  99.22 ( 99.22)
Epoch: [9][101/313]     Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 4.9106e-01 (3.8742e-01)    Acc@1  80.47 ( 86.53)   Acc@5  99.22 ( 99.57)
Epoch: [9][201/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.025)    Loss 4.6411e-01 (4.0367e-01)    Acc@1  83.59 ( 85.88)   Acc@5  99.22 ( 99.56)
Epoch: [9][301/313]     Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.0104e-01 (4.1182e-01)    Acc@1  88.28 ( 85.58)   Acc@5 100.00 ( 99.52)
train Loss: 0.4123 Acc: 0.8557
Epoch: [9][  1/313]     Time  0.045 ( 0.044)    Data  0.038 ( 0.024)    Loss 7.0236e-01 (4.1324e-01)    Acc@1  72.66 ( 85.52)   Acc@5 100.00 ( 99.52)
val Loss: 0.8578 Acc: 0.7299

Epoch 10/39
----------
Epoch: [10][  1/313]    Time  0.106 ( 0.106)    Data  0.077 ( 0.077)    Loss 3.7604e-01 (3.7604e-01)    Acc@1  85.16 ( 85.16)   Acc@5  99.22 ( 99.22)
Epoch: [10][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.5936e-01 (3.3192e-01)    Acc@1  91.41 ( 88.22)   Acc@5 100.00 ( 99.68)
Epoch: [10][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.025)    Loss 3.0875e-01 (3.5808e-01)    Acc@1  90.62 ( 87.32)   Acc@5 100.00 ( 99.68)
Epoch: [10][301/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.025)    Loss 3.1391e-01 (3.7070e-01)    Acc@1  85.94 ( 86.88)   Acc@5  99.22 ( 99.65)
train Loss: 0.3726 Acc: 0.8682
Epoch: [10][  1/313]    Time  0.042 ( 0.044)    Data  0.035 ( 0.025)    Loss 8.1162e-01 (3.7403e-01)    Acc@1  75.78 ( 86.78)   Acc@5  96.88 ( 99.64)
val Loss: 0.7811 Acc: 0.7507

Epoch 11/39
----------
Epoch: [11][  1/313]    Time  0.106 ( 0.106)    Data  0.073 ( 0.073)    Loss 3.1336e-01 (3.1336e-01)    Acc@1  89.06 ( 89.06)   Acc@5 100.00 (100.00)
Epoch: [11][101/313]    Time  0.041 ( 0.045)    Data  0.022 ( 0.025)    Loss 3.3839e-01 (2.9860e-01)    Acc@1  88.28 ( 89.86)   Acc@5 100.00 ( 99.83)
Epoch: [11][201/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 3.6482e-01 (3.1837e-01)    Acc@1  86.72 ( 88.92)   Acc@5 100.00 ( 99.76)
Epoch: [11][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 4.0732e-01 (3.3391e-01)    Acc@1  86.72 ( 88.28)   Acc@5 100.00 ( 99.74)
train Loss: 0.3353 Acc: 0.8822
Epoch: [11][  1/313]    Time  0.043 ( 0.044)    Data  0.036 ( 0.024)    Loss 1.2432e+00 (3.3820e-01)    Acc@1  63.28 ( 88.14)   Acc@5  93.75 ( 99.71)
val Loss: 0.9727 Acc: 0.7113

Epoch 12/39
----------
Epoch: [12][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 3.2317e-01 (3.2317e-01)    Acc@1  88.28 ( 88.28)   Acc@5  99.22 ( 99.22)
Epoch: [12][101/313]    Time  0.045 ( 0.046)    Data  0.024 ( 0.025)    Loss 2.2832e-01 (2.6058e-01)    Acc@1  91.41 ( 90.77)   Acc@5 100.00 ( 99.83)
Epoch: [12][201/313]    Time  0.082 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.7984e-01 (2.9382e-01)    Acc@1  89.84 ( 89.76)   Acc@5 100.00 ( 99.79)
Epoch: [12][301/313]    Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.7140e-01 (3.1189e-01)    Acc@1  89.06 ( 89.01)   Acc@5 100.00 ( 99.76)
train Loss: 0.3141 Acc: 0.8898
Epoch: [12][  1/313]    Time  0.044 ( 0.045)    Data  0.037 ( 0.025)    Loss 7.8583e-01 (3.1557e-01)    Acc@1  76.56 ( 88.94)   Acc@5  99.22 ( 99.75)
val Loss: 0.7759 Acc: 0.7593

Epoch 13/39
----------
Epoch: [13][  1/313]    Time  0.104 ( 0.104)    Data  0.074 ( 0.074)    Loss 2.3218e-01 (2.3218e-01)    Acc@1  92.19 ( 92.19)   Acc@5 100.00 (100.00)
Epoch: [13][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.1907e-01 (2.4321e-01)    Acc@1  93.75 ( 91.54)   Acc@5 100.00 ( 99.87)
Epoch: [13][201/313]    Time  0.044 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.3126e-01 (2.7486e-01)    Acc@1  92.19 ( 90.37)   Acc@5 100.00 ( 99.82)
Epoch: [13][301/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.0603e-01 (2.8697e-01)    Acc@1  89.06 ( 89.82)   Acc@5  98.44 ( 99.82)
train Loss: 0.2876 Acc: 0.8978
Epoch: [13][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 1.5956e+00 (2.9177e-01)    Acc@1  60.94 ( 89.68)   Acc@5  97.66 ( 99.81)
val Loss: 1.4593 Acc: 0.6346

Epoch 14/39
----------
Epoch: [14][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 1.9299e-01 (1.9299e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [14][101/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.9468e-01 (2.1854e-01)    Acc@1  95.31 ( 92.33)   Acc@5 100.00 ( 99.89)
Epoch: [14][201/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.4321e-01 (2.4203e-01)    Acc@1  96.88 ( 91.47)   Acc@5 100.00 ( 99.88)
Epoch: [14][301/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 4.2246e-01 (2.5872e-01)    Acc@1  86.72 ( 90.89)   Acc@5  97.66 ( 99.87)
train Loss: 0.2615 Acc: 0.9079
Epoch: [14][  1/313]    Time  0.043 ( 0.045)    Data  0.037 ( 0.025)    Loss 8.3626e-01 (2.6332e-01)    Acc@1  74.22 ( 90.73)   Acc@5  98.44 ( 99.86)
val Loss: 0.9620 Acc: 0.7265

Epoch 15/39
----------
Epoch: [15][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 2.9524e-01 (2.9524e-01)    Acc@1  89.06 ( 89.06)   Acc@5 100.00 (100.00)
Epoch: [15][101/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.9006e-01 (2.0664e-01)    Acc@1  90.62 ( 92.69)   Acc@5 100.00 ( 99.94)
Epoch: [15][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 3.5696e-01 (2.2771e-01)    Acc@1  88.28 ( 92.05)   Acc@5  99.22 ( 99.89)
Epoch: [15][301/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.2251e-01 (2.3939e-01)    Acc@1  92.97 ( 91.52)   Acc@5  99.22 ( 99.87)
train Loss: 0.2416 Acc: 0.9144
Epoch: [15][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 5.6926e-01 (2.4261e-01)    Acc@1  80.47 ( 91.41)   Acc@5 100.00 ( 99.87)
val Loss: 1.0842 Acc: 0.7148

Epoch 16/39
----------
Epoch: [16][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 2.0370e-01 (2.0370e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [16][101/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.1210e-01 (1.8950e-01)    Acc@1  95.31 ( 93.50)   Acc@5 100.00 ( 99.90)
Epoch: [16][201/313]    Time  0.043 ( 0.045)    Data  0.023 ( 0.025)    Loss 2.7983e-01 (2.1215e-01)    Acc@1  90.62 ( 92.67)   Acc@5 100.00 ( 99.91)
Epoch: [16][301/313]    Time  0.046 ( 0.044)    Data  0.026 ( 0.024)    Loss 2.1665e-01 (2.2749e-01)    Acc@1  91.41 ( 92.00)   Acc@5 100.00 ( 99.90)
train Loss: 0.2286 Acc: 0.9199
Epoch: [16][  1/313]    Time  0.042 ( 0.044)    Data  0.035 ( 0.024)    Loss 1.0125e+00 (2.3109e-01)    Acc@1  67.97 ( 91.92)   Acc@5  98.44 ( 99.90)
val Loss: 1.1177 Acc: 0.7197

Epoch 17/39
----------
Epoch: [17][  1/313]    Time  0.105 ( 0.105)    Data  0.073 ( 0.073)    Loss 1.9229e-01 (1.9229e-01)    Acc@1  92.97 ( 92.97)   Acc@5 100.00 (100.00)
Epoch: [17][101/313]    Time  0.045 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.1980e-01 (1.8169e-01)    Acc@1  94.53 ( 93.43)   Acc@5 100.00 ( 99.95)
Epoch: [17][201/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.024)    Loss 2.5096e-01 (1.9134e-01)    Acc@1  88.28 ( 93.17)   Acc@5 100.00 ( 99.95)
Epoch: [17][301/313]    Time  0.046 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.8205e-01 (2.0887e-01)    Acc@1  89.84 ( 92.60)   Acc@5 100.00 ( 99.94)
train Loss: 0.2106 Acc: 0.9253
Epoch: [17][  1/313]    Time  0.045 ( 0.044)    Data  0.038 ( 0.024)    Loss 9.5746e-01 (2.1301e-01)    Acc@1  75.00 ( 92.48)   Acc@5  97.66 ( 99.92)
val Loss: 0.9795 Acc: 0.7476

Epoch 18/39
----------
Epoch: [18][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 2.9098e-01 (2.9098e-01)    Acc@1  90.62 ( 90.62)   Acc@5  99.22 ( 99.22)
Epoch: [18][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.2947e-01 (1.6938e-01)    Acc@1  94.53 ( 94.20)   Acc@5 100.00 ( 99.98)
Epoch: [18][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.025)    Loss 1.8349e-01 (1.9711e-01)    Acc@1  94.53 ( 93.05)   Acc@5 100.00 ( 99.94)
Epoch: [18][301/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.6903e-01 (2.0653e-01)    Acc@1  91.41 ( 92.69)   Acc@5 100.00 ( 99.94)
train Loss: 0.2086 Acc: 0.9259
Epoch: [18][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 1.0922e+00 (2.1144e-01)    Acc@1  72.66 ( 92.52)   Acc@5  96.88 ( 99.93)
val Loss: 0.8424 Acc: 0.7669

Epoch 19/39
----------
Epoch: [19][  1/313]    Time  0.106 ( 0.106)    Data  0.076 ( 0.076)    Loss 1.2325e-01 (1.2325e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [19][101/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.5257e-01 (1.5243e-01)    Acc@1  93.75 ( 94.77)   Acc@5 100.00 ( 99.95)
Epoch: [19][201/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.5417e-01 (1.5963e-01)    Acc@1  91.41 ( 94.55)   Acc@5 100.00 ( 99.95)
Epoch: [19][301/313]    Time  0.046 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.5963e-01 (1.7953e-01)    Acc@1  89.84 ( 93.73)   Acc@5 100.00 ( 99.94)
train Loss: 0.1829 Acc: 0.9363
Epoch: [19][  1/313]    Time  0.047 ( 0.044)    Data  0.037 ( 0.024)    Loss 9.4362e-01 (1.8531e-01)    Acc@1  75.78 ( 93.57)   Acc@5  98.44 ( 99.93)
val Loss: 0.9971 Acc: 0.7525

Epoch 20/39
----------
Epoch: [20][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 2.4864e-01 (2.4864e-01)    Acc@1  89.84 ( 89.84)   Acc@5 100.00 (100.00)
Epoch: [20][101/313]    Time  0.044 ( 0.045)    Data  0.024 ( 0.025)    Loss 7.9081e-02 (1.5616e-01)    Acc@1  98.44 ( 94.35)   Acc@5 100.00 ( 99.99)
Epoch: [20][201/313]    Time  0.046 ( 0.044)    Data  0.025 ( 0.024)    Loss 1.1398e-01 (1.6292e-01)    Acc@1  95.31 ( 94.10)   Acc@5 100.00 ( 99.98)
Epoch: [20][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.8186e-01 (1.8283e-01)    Acc@1  93.75 ( 93.49)   Acc@5 100.00 ( 99.96)
train Loss: 0.1845 Acc: 0.9343
Epoch: [20][  1/313]    Time  0.045 ( 0.044)    Data  0.038 ( 0.024)    Loss 1.0467e+00 (1.8724e-01)    Acc@1  75.00 ( 93.37)   Acc@5  98.44 ( 99.96)
val Loss: 1.0295 Acc: 0.7326

Epoch 21/39
----------
Epoch: [21][  1/313]    Time  0.103 ( 0.103)    Data  0.074 ( 0.074)    Loss 1.2457e-01 (1.2457e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [21][101/313]    Time  0.044 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.0401e-01 (1.5046e-01)    Acc@1  94.53 ( 94.61)   Acc@5 100.00 ( 99.97)
Epoch: [21][201/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.5994e-01 (1.6047e-01)    Acc@1  94.53 ( 94.26)   Acc@5 100.00 ( 99.97)
Epoch: [21][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.025)    Loss 1.6503e-01 (1.7276e-01)    Acc@1  92.19 ( 93.78)   Acc@5 100.00 ( 99.97)
train Loss: 0.1734 Acc: 0.9375
Epoch: [21][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.025)    Loss 9.2119e-01 (1.7583e-01)    Acc@1  75.78 ( 93.69)   Acc@5  99.22 ( 99.97)
val Loss: 0.7921 Acc: 0.7856

Epoch 22/39
----------
Epoch: [22][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 1.1859e-01 (1.1859e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [22][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 7.4226e-02 (1.2098e-01)    Acc@1  97.66 ( 95.93)   Acc@5 100.00 ( 99.99)
Epoch: [22][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.3978e-01 (1.4282e-01)    Acc@1  96.09 ( 95.02)   Acc@5 100.00 ( 99.98)
Epoch: [22][301/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 2.7976e-01 (1.6008e-01)    Acc@1  90.62 ( 94.30)   Acc@5 100.00 ( 99.97)
train Loss: 0.1617 Acc: 0.9422
Epoch: [22][  1/313]    Time  0.048 ( 0.044)    Data  0.040 ( 0.024)    Loss 1.0980e+00 (1.6472e-01)    Acc@1  74.22 ( 94.16)   Acc@5  96.88 ( 99.96)
val Loss: 1.1853 Acc: 0.7127

Epoch 23/39
----------
Epoch: [23][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 1.5753e-01 (1.5753e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [23][101/313]    Time  0.045 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.1791e-01 (1.4943e-01)    Acc@1  96.09 ( 94.83)   Acc@5 100.00 ( 99.98)
Epoch: [23][201/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.8998e-01 (1.4453e-01)    Acc@1  93.75 ( 94.93)   Acc@5 100.00 ( 99.98)
Epoch: [23][301/313]    Time  0.045 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.5946e-01 (1.6384e-01)    Acc@1  93.75 ( 94.16)   Acc@5 100.00 ( 99.98)
train Loss: 0.1649 Acc: 0.9411
Epoch: [23][  1/313]    Time  0.048 ( 0.044)    Data  0.041 ( 0.025)    Loss 9.2553e-01 (1.6736e-01)    Acc@1  73.44 ( 94.04)   Acc@5  98.44 ( 99.97)
val Loss: 0.8919 Acc: 0.7752

Epoch 24/39
----------
Epoch: [24][  1/313]    Time  0.128 ( 0.128)    Data  0.098 ( 0.098)    Loss 1.5092e-01 (1.5092e-01)    Acc@1  94.53 ( 94.53)   Acc@5 100.00 (100.00)
Epoch: [24][101/313]    Time  0.042 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.1888e-01 (1.3427e-01)    Acc@1  95.31 ( 95.17)   Acc@5 100.00 ( 99.98)
Epoch: [24][201/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.1087e-01 (1.4219e-01)    Acc@1  92.19 ( 94.87)   Acc@5 100.00 ( 99.98)
Epoch: [24][301/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.2621e-01 (1.5857e-01)    Acc@1  92.19 ( 94.36)   Acc@5 100.00 ( 99.96)
train Loss: 0.1604 Acc: 0.9428
Epoch: [24][  1/313]    Time  0.043 ( 0.045)    Data  0.036 ( 0.025)    Loss 1.2386e+00 (1.6382e-01)    Acc@1  70.31 ( 94.21)   Acc@5  97.66 ( 99.95)
val Loss: 0.9854 Acc: 0.7498

Epoch 25/39
----------
Epoch: [25][  1/313]    Time  0.102 ( 0.102)    Data  0.072 ( 0.072)    Loss 8.6376e-02 (8.6376e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [25][101/313]    Time  0.097 ( 0.046)    Data  0.033 ( 0.025)    Loss 1.2977e-01 (1.1882e-01)    Acc@1  94.53 ( 96.06)   Acc@5 100.00 ( 99.96)
Epoch: [25][201/313]    Time  0.041 ( 0.045)    Data  0.023 ( 0.024)    Loss 7.8286e-02 (1.2396e-01)    Acc@1  97.66 ( 95.66)   Acc@5 100.00 ( 99.97)
Epoch: [25][301/313]    Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.2213e-01 (1.3539e-01)    Acc@1  90.62 ( 95.34)   Acc@5 100.00 ( 99.97)
train Loss: 0.1386 Acc: 0.9521
Epoch: [25][  1/313]    Time  0.043 ( 0.045)    Data  0.036 ( 0.025)    Loss 7.8207e-01 (1.4069e-01)    Acc@1  78.12 ( 95.15)   Acc@5  98.44 ( 99.97)
val Loss: 0.7874 Acc: 0.7912

Epoch 26/39
----------
Epoch: [26][  1/313]    Time  0.104 ( 0.104)    Data  0.075 ( 0.075)    Loss 1.0321e-01 (1.0321e-01)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [26][101/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.026)    Loss 5.9673e-02 (1.1754e-01)    Acc@1  99.22 ( 96.04)   Acc@5 100.00 ( 99.97)
Epoch: [26][201/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.026)    Loss 1.5115e-01 (1.2307e-01)    Acc@1  95.31 ( 95.73)   Acc@5 100.00 ( 99.97)
Epoch: [26][301/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 1.5792e-01 (1.3909e-01)    Acc@1  95.31 ( 95.10)   Acc@5  99.22 ( 99.97)
train Loss: 0.1393 Acc: 0.9510
Epoch: [26][  1/313]    Time  0.054 ( 0.045)    Data  0.046 ( 0.025)    Loss 8.8240e-01 (1.4168e-01)    Acc@1  79.69 ( 95.05)   Acc@5  96.88 ( 99.96)
val Loss: 0.8337 Acc: 0.7790

Epoch 27/39
----------
Epoch: [27][  1/313]    Time  0.105 ( 0.105)    Data  0.074 ( 0.074)    Loss 1.0649e-01 (1.0649e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [27][101/313]    Time  0.048 ( 0.047)    Data  0.029 ( 0.026)    Loss 9.3977e-02 (1.0087e-01)    Acc@1  96.09 ( 96.57)   Acc@5 100.00 ( 99.99)
Epoch: [27][201/313]    Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.1099e-01 (1.0828e-01)    Acc@1  96.09 ( 96.24)   Acc@5 100.00 ( 99.98)
Epoch: [27][301/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.5296e-01 (1.2366e-01)    Acc@1  92.97 ( 95.66)   Acc@5 100.00 ( 99.97)
train Loss: 0.1255 Acc: 0.9559
Epoch: [27][  1/313]    Time  0.047 ( 0.045)    Data  0.041 ( 0.025)    Loss 5.9522e-01 (1.2702e-01)    Acc@1  81.25 ( 95.54)   Acc@5  99.22 ( 99.97)
val Loss: 0.9304 Acc: 0.7750

Epoch 28/39
----------
Epoch: [28][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 1.4001e-01 (1.4001e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [28][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.2830e-01 (1.1924e-01)    Acc@1  95.31 ( 95.65)   Acc@5 100.00 (100.00)
Epoch: [28][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.1408e-01 (1.1731e-01)    Acc@1  96.09 ( 95.81)   Acc@5 100.00 (100.00)
Epoch: [28][301/313]    Time  0.043 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.3615e-01 (1.2700e-01)    Acc@1  92.97 ( 95.46)   Acc@5 100.00 ( 99.99)
train Loss: 0.1291 Acc: 0.9538
Epoch: [28][  1/313]    Time  0.044 ( 0.044)    Data  0.038 ( 0.024)    Loss 7.8405e-01 (1.3115e-01)    Acc@1  77.34 ( 95.32)   Acc@5  99.22 ( 99.99)
val Loss: 1.0443 Acc: 0.7440

Epoch 29/39
----------
Epoch: [29][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 1.1161e-01 (1.1161e-01)    Acc@1  96.09 ( 96.09)   Acc@5 100.00 (100.00)
Epoch: [29][101/313]    Time  0.045 ( 0.045)    Data  0.024 ( 0.025)    Loss 9.4858e-02 (1.3277e-01)    Acc@1  96.09 ( 95.24)   Acc@5 100.00 ( 99.99)
Epoch: [29][201/313]    Time  0.043 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.6811e-01 (1.2765e-01)    Acc@1  95.31 ( 95.49)   Acc@5 100.00 ( 99.99)
Epoch: [29][301/313]    Time  0.045 ( 0.045)    Data  0.026 ( 0.025)    Loss 2.1437e-01 (1.3727e-01)    Acc@1  92.97 ( 95.06)   Acc@5 100.00 ( 99.99)
train Loss: 0.1385 Acc: 0.9502
Epoch: [29][  1/313]    Time  0.049 ( 0.045)    Data  0.038 ( 0.025)    Loss 9.5722e-01 (1.4115e-01)    Acc@1  76.56 ( 94.96)   Acc@5  97.66 ( 99.98)
val Loss: 1.1533 Acc: 0.7426

Epoch 30/39
----------
Epoch: [30][  1/313]    Time  0.125 ( 0.125)    Data  0.095 ( 0.095)    Loss 1.2907e-01 (1.2907e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [30][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.2672e-01 (1.1508e-01)    Acc@1  89.84 ( 95.90)   Acc@5 100.00 ( 99.98)
Epoch: [30][201/313]    Time  0.044 ( 0.044)    Data  0.024 ( 0.025)    Loss 1.8540e-01 (1.2660e-01)    Acc@1  91.41 ( 95.43)   Acc@5 100.00 ( 99.98)
Epoch: [30][301/313]    Time  0.044 ( 0.044)    Data  0.025 ( 0.024)    Loss 1.5883e-01 (1.3569e-01)    Acc@1  96.09 ( 95.09)   Acc@5 100.00 ( 99.98)
train Loss: 0.1361 Acc: 0.9508
Epoch: [30][  1/313]    Time  0.045 ( 0.044)    Data  0.037 ( 0.024)    Loss 6.1408e-01 (1.3758e-01)    Acc@1  80.47 ( 95.03)   Acc@5 100.00 ( 99.99)
val Loss: 0.8180 Acc: 0.7852

Epoch 31/39
----------
Epoch: [31][  1/313]    Time  0.104 ( 0.104)    Data  0.072 ( 0.072)    Loss 9.5352e-02 (9.5352e-02)    Acc@1  96.88 ( 96.88)   Acc@5 100.00 (100.00)
Epoch: [31][101/313]    Time  0.043 ( 0.045)    Data  0.025 ( 0.025)    Loss 8.0038e-02 (1.1065e-01)    Acc@1  96.88 ( 96.09)   Acc@5 100.00 ( 99.98)
Epoch: [31][201/313]    Time  0.044 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.1808e-01 (1.1364e-01)    Acc@1  95.31 ( 95.94)   Acc@5 100.00 ( 99.98)
Epoch: [31][301/313]    Time  0.047 ( 0.044)    Data  0.024 ( 0.024)    Loss 2.4816e-01 (1.3040e-01)    Acc@1  90.62 ( 95.43)   Acc@5 100.00 ( 99.98)
train Loss: 0.1321 Acc: 0.9537
Epoch: [31][  1/313]    Time  0.042 ( 0.044)    Data  0.036 ( 0.024)    Loss 7.7227e-01 (1.3416e-01)    Acc@1  78.12 ( 95.31)   Acc@5  97.66 ( 99.97)
val Loss: 0.8501 Acc: 0.7815

Epoch 32/39
----------
Epoch: [32][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 1.0864e-01 (1.0864e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [32][101/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 4.4529e-02 (9.5325e-02)    Acc@1  98.44 ( 96.65)   Acc@5 100.00 ( 99.98)
Epoch: [32][201/313]    Time  0.047 ( 0.045)    Data  0.026 ( 0.025)    Loss 6.4851e-02 (1.0178e-01)    Acc@1  97.66 ( 96.47)   Acc@5 100.00 ( 99.98)
Epoch: [32][301/313]    Time  0.047 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.5448e-01 (1.1161e-01)    Acc@1  95.31 ( 96.09)   Acc@5 100.00 ( 99.98)
train Loss: 0.1126 Acc: 0.9606
Epoch: [32][  1/313]    Time  0.050 ( 0.045)    Data  0.043 ( 0.025)    Loss 1.0163e+00 (1.1548e-01)    Acc@1  75.00 ( 95.99)   Acc@5  97.66 ( 99.97)
val Loss: 1.0047 Acc: 0.7668

Epoch 33/39
----------
Epoch: [33][  1/313]    Time  0.103 ( 0.103)    Data  0.072 ( 0.072)    Loss 1.0470e-01 (1.0470e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [33][101/313]    Time  0.043 ( 0.046)    Data  0.024 ( 0.025)    Loss 1.4706e-01 (9.2292e-02)    Acc@1  96.09 ( 96.81)   Acc@5 100.00 (100.00)
Epoch: [33][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.5928e-01 (1.1157e-01)    Acc@1  93.75 ( 96.07)   Acc@5 100.00 ( 99.99)
Epoch: [33][301/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.9619e-01 (1.2436e-01)    Acc@1  92.19 ( 95.58)   Acc@5 100.00 ( 99.99)
train Loss: 0.1260 Acc: 0.9553
Epoch: [33][  1/313]    Time  0.043 ( 0.045)    Data  0.036 ( 0.025)    Loss 1.1433e+00 (1.2927e-01)    Acc@1  72.66 ( 95.45)   Acc@5  97.66 ( 99.98)
val Loss: 0.9448 Acc: 0.7649

Epoch 34/39
----------
Epoch: [34][  1/313]    Time  0.104 ( 0.104)    Data  0.073 ( 0.073)    Loss 1.5078e-01 (1.5078e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [34][101/313]    Time  0.042 ( 0.045)    Data  0.024 ( 0.025)    Loss 2.5783e-01 (1.1800e-01)    Acc@1  91.41 ( 95.70)   Acc@5 100.00 ( 99.99)
Epoch: [34][201/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.7800e-01 (1.1903e-01)    Acc@1  95.31 ( 95.76)   Acc@5 100.00 ( 99.99)
Epoch: [34][301/313]    Time  0.043 ( 0.044)    Data  0.025 ( 0.024)    Loss 2.7789e-01 (1.3232e-01)    Acc@1  92.19 ( 95.26)   Acc@5 100.00 ( 99.99)
train Loss: 0.1325 Acc: 0.9525
Epoch: [34][  1/313]    Time  0.045 ( 0.044)    Data  0.038 ( 0.025)    Loss 1.0644e+00 (1.3550e-01)    Acc@1  74.22 ( 95.18)   Acc@5  96.88 ( 99.98)
val Loss: 0.8631 Acc: 0.7763

Epoch 35/39
----------
Epoch: [35][  1/313]    Time  0.107 ( 0.107)    Data  0.075 ( 0.075)    Loss 8.9195e-02 (8.9195e-02)    Acc@1  97.66 ( 97.66)   Acc@5 100.00 (100.00)
Epoch: [35][101/313]    Time  0.043 ( 0.046)    Data  0.025 ( 0.025)    Loss 8.7192e-02 (9.5261e-02)    Acc@1  97.66 ( 96.83)   Acc@5 100.00 (100.00)
Epoch: [35][201/313]    Time  0.043 ( 0.045)    Data  0.023 ( 0.025)    Loss 6.7610e-02 (1.0264e-01)    Acc@1  97.66 ( 96.46)   Acc@5 100.00 (100.00)
Epoch: [35][301/313]    Time  0.045 ( 0.045)    Data  0.025 ( 0.025)    Loss 2.5388e-01 (1.1365e-01)    Acc@1  89.06 ( 96.03)   Acc@5 100.00 ( 99.99)
train Loss: 0.1143 Acc: 0.9601
Epoch: [35][  1/313]    Time  0.044 ( 0.045)    Data  0.036 ( 0.025)    Loss 8.9954e-01 (1.1684e-01)    Acc@1  78.91 ( 95.95)   Acc@5  98.44 ( 99.99)
val Loss: 0.9225 Acc: 0.7805

Epoch 36/39
----------
Epoch: [36][  1/313]    Time  0.105 ( 0.105)    Data  0.075 ( 0.075)    Loss 1.7824e-01 (1.7824e-01)    Acc@1  93.75 ( 93.75)   Acc@5 100.00 (100.00)
Epoch: [36][101/313]    Time  0.046 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.4104e-01 (9.7947e-02)    Acc@1  94.53 ( 96.49)   Acc@5 100.00 (100.00)
Epoch: [36][201/313]    Time  0.049 ( 0.045)    Data  0.030 ( 0.025)    Loss 1.1906e-01 (1.0734e-01)    Acc@1  96.09 ( 96.18)   Acc@5 100.00 (100.00)
Epoch: [36][301/313]    Time  0.045 ( 0.045)    Data  0.025 ( 0.025)    Loss 9.5289e-02 (1.2223e-01)    Acc@1  95.31 ( 95.67)   Acc@5 100.00 ( 99.98)
train Loss: 0.1238 Acc: 0.9561
Epoch: [36][  1/313]    Time  0.048 ( 0.045)    Data  0.041 ( 0.025)    Loss 8.0390e-01 (1.2600e-01)    Acc@1  77.34 ( 95.55)   Acc@5  97.66 ( 99.98)
val Loss: 0.9708 Acc: 0.7631

Epoch 37/39
----------
Epoch: [37][  1/313]    Time  0.103 ( 0.103)    Data  0.073 ( 0.073)    Loss 5.4638e-02 (5.4638e-02)    Acc@1  98.44 ( 98.44)   Acc@5 100.00 (100.00)
Epoch: [37][101/313]    Time  0.044 ( 0.046)    Data  0.024 ( 0.025)    Loss 7.2214e-02 (9.8628e-02)    Acc@1  97.66 ( 96.59)   Acc@5 100.00 ( 99.99)
Epoch: [37][201/313]    Time  0.045 ( 0.045)    Data  0.026 ( 0.025)    Loss 2.0302e-01 (1.0127e-01)    Acc@1  95.31 ( 96.43)   Acc@5 100.00 ( 99.99)
Epoch: [37][301/313]    Time  0.044 ( 0.045)    Data  0.025 ( 0.025)    Loss 1.1145e-01 (1.1418e-01)    Acc@1  96.09 ( 95.99)   Acc@5 100.00 ( 99.99)
train Loss: 0.1136 Acc: 0.9600
Epoch: [37][  1/313]    Time  0.042 ( 0.044)    Data  0.035 ( 0.025)    Loss 1.0689e+00 (1.1662e-01)    Acc@1  80.47 ( 95.95)   Acc@5 100.00 (100.00)
val Loss: 0.8655 Acc: 0.7863

Epoch 38/39
----------
Epoch: [38][  1/313]    Time  0.120 ( 0.120)    Data  0.089 ( 0.089)    Loss 3.8295e-02 (3.8295e-02)    Acc@1  99.22 ( 99.22)   Acc@5 100.00 (100.00)
Epoch: [38][101/313]    Time  0.043 ( 0.045)    Data  0.024 ( 0.025)    Loss 1.2851e-01 (7.8740e-02)    Acc@1  95.31 ( 97.40)   Acc@5 100.00 (100.00)
Epoch: [38][201/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.2959e-01 (8.5654e-02)    Acc@1  96.88 ( 97.10)   Acc@5 100.00 ( 99.99)
Epoch: [38][301/313]    Time  0.042 ( 0.044)    Data  0.024 ( 0.024)    Loss 1.0039e-01 (1.0788e-01)    Acc@1  95.31 ( 96.27)   Acc@5 100.00 ( 99.98)
train Loss: 0.1098 Acc: 0.9621
Epoch: [38][  1/313]    Time  0.042 ( 0.044)    Data  0.036 ( 0.024)    Loss 1.0595e+00 (1.1280e-01)    Acc@1  75.00 ( 96.14)   Acc@5  98.44 ( 99.98)
val Loss: 0.8515 Acc: 0.7869

Epoch 39/39
----------
Epoch: [39][  1/313]    Time  0.111 ( 0.111)    Data  0.082 ( 0.082)    Loss 1.3541e-01 (1.3541e-01)    Acc@1  95.31 ( 95.31)   Acc@5 100.00 (100.00)
Epoch: [39][101/313]    Time  0.042 ( 0.045)    Data  0.023 ( 0.025)    Loss 3.4046e-02 (9.3073e-02)    Acc@1 100.00 ( 96.84)   Acc@5 100.00 (100.00)
Epoch: [39][201/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.5498e-01 (9.9714e-02)    Acc@1  95.31 ( 96.51)   Acc@5 100.00 (100.00)
Epoch: [39][301/313]    Time  0.042 ( 0.044)    Data  0.023 ( 0.024)    Loss 1.9247e-01 (1.1406e-01)    Acc@1  92.97 ( 96.07)   Acc@5 100.00 ( 99.99)
train Loss: 0.1147 Acc: 0.9602
Epoch: [39][  1/313]    Time  0.044 ( 0.044)    Data  0.037 ( 0.024)    Loss 9.9407e-01 (1.1755e-01)    Acc@1  78.12 ( 95.96)   Acc@5  98.44 ( 99.99)
val Loss: 1.0036 Acc: 0.7628

Training complete in 11m 8s
Best val Acc: 0.791200
Test Loss: 0.157487

Test Accuracy of airplane: 83% (797/954)
Test Accuracy of automobile: 90% (879/974)
Test Accuracy of  bird: 74% (767/1035)
Test Accuracy of   cat: 58% (590/1014)
Test Accuracy of  deer: 75% (780/1033)
Test Accuracy of   dog: 75% (775/1032)
Test Accuracy of  frog: 84% (880/1045)
Test Accuracy of horse: 77% (767/989)
Test Accuracy of  ship: 90% (900/998)
Test Accuracy of truck: 83% (777/926)

Test Accuracy (Overall): 79% (7912/10000)